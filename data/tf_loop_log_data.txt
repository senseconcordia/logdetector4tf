VLOG(1) << status;
VLOG_IS_ON(2))
VLOG(1) << "Skipping cancelled enqueue attempt";
VLOG(1) << "Skipping cancelled dequeue attempt";
VLOG_IS_ON(2))
VLOG(2) << device->name();
VLOG(2) << "rank " << rank << " output " << output << " buf " << DMAHelper::base(output);
VLOG(3) << "rank " << rank << " got output tensor " << actual.DebugString(output_length);
VLOG(1) << "_ScopedAllocatorSplitOp assigning input " << i << " to output " << i - 1 << " buf addr " << DMAHelper::base(&context->input(i));
VLOG(2) << "Border Input(" << i << "): " << border_inputs.at(i);
VLOG(2) << "Border Output(" << i << "): " << border_outputs.at(i);
VLOG(2) << "Graph input: " << name;
VLOG(2) << "Graph output: " << name;
VLOG(2) << "Fused node: " << name;
VLOG(2) << "Border input: " << name;
VLOG(2) << "Border output: " << name;
VLOG(2) << "Input matched!";
VLOG(1) << DumpCluster(ci);
VLOG(1) << "(" << i << ") " << expected_val << ", " << resized_image_val;
VLOG(1) << "Skipping cancelled TakeGrad attempt";
VLOG(1) << "Profile Cudnn RNN algorithm (algo, tensor_op_enabled) = (" << algo.algo_id() << ", " << algo.tensor_ops_enabled() << ").";
VLOG(1) << "Cudnn RNN algorithm (algo, tensor_op_enabled) = (" << algo.algo_id() << ", " << algo.tensor_ops_enabled() << ")" << " run time: " << total_time << " ms.";
VLOG(3) << "Running function.";
VLOG(3) << "Reducing interval_micros from " << interval_micros_ << " to " << (deadline - end);
VLOG(3) << "Function took longer than interval_micros, so not sleeping";
VLOG(1) << "<Node> " << node->name();
VLOG(1) << "Add dependency: " << name << " -> " << node->name();
VLOG(1) << "input_node " << input_node->name() << " of " << node.name() << " is not cached yet.";
VLOG(1) << "QuantileStream has already been finalized for feature" << feature_idx << ".";
VLOG(2) << "Breaking ties on feature ids and buckets";
VLOG(2) << "Breaking ties on feature ids and buckets";
VLOG(3) << "Failed to hash tensor: " << s.ToString();
VLOG(3) << "Blocked waiting for element " << current_elements_[cycle_index_]->id;
VLOG(3) << "Future worker created element " << element->id;
VLOG(2) << "Waiting for " << wait_ms << " ms.";
VLOG(2) << "Sleeping for: " << slack_us_ * kSleepFactor;
VLOG(2) << "Reading file: " << filenames_tensor->flat<tstring>()(i);
VLOG(1) << "Starting to fill up shuffle buffer of size: " << this->dataset()->buffer_size_;
VLOG(2) << "Branch " << i << ": " << percentile;
VLOG(2) << "Starting to read: " << filename;
VLOG(2) << "Finished reading: " << filename;
VLOG(2) << "RunCollective rank " << rank << " global_rank " << p->global_rank << " root_rank " << collective->root_rank;
VLOG(3) << "Locking mutex nccl_stream " << nccl_stream;
VLOG(2) << "call NcclAllReduce collective_key " << collective->collective_key << " participant " << p_idx << " sendbuff " << sendbuff << " recvbuff " << recvbuff << " nccl_comm " << nccl_comm << " comm_stream " << comm_stream << " cuda_stream " << cu_stream;
VLOG(2) << "call NcclBroadcast collective_key " << collective->collective_key << " participant " << p_idx << " sendbuff " << sendbuff << " recvbuff " << recvbuff << " nccl_comm " << nccl_comm << " comm_stream " << comm_stream << " cuda_stream " << cu_stream;
VLOG(2) << "call NcclAllGather collective_key " << collective->collective_key << " participant " << p_idx << " sendbuff " << sendbuff << " sendcount " << p->input->NumElements() << " recvbuff " << recvbuff << " recvcount " << p->output->NumElements() << " nccl_comm " << nccl_comm << " comm_stream " << comm_stream << " cuda_stream " << cu_stream;
VLOG(2) << "done Nccl kernel collective_key " << collective->collective_key << " participant " << p_idx << " ncclResult " << nccl_result;
VLOG(1) << "Verifying rank " << global_rank << " expected shape " << test_case->expected.shape() << " out shape " << out_cpu.shape();
VLOG(1) << "AddToAllReduce node " << node << " global_rank " << global_rank;
VLOG(2) << "rank " << rank << " device " << device->name();
VLOG(2) << "rank " << rank << " device " << device->name();
VLOG(1) << "Renaming " << p.first << " to " << DataFilename(merged_prefix, p.second, merge.shard_ids.size());
VLOG(1) << "Optimized for common case: directly copying into " "pre-allocated buffer; spec: " << slice_spec.DebugString();
VLOG(1) << "Group: " << absl::StrJoin(g.group(), ",");
VLOG(1) << "Indices: " << g.indices();
VLOG(1) << "Values: " << g.values<int32>();
VLOG(5) << "|| " << line;
VLOG(2) << "Setting tunable parameter " << pair.first << " to " << parameter->value;
VLOG(2) << "Setting tunable parameter " << pair.first << " to " << parameter->value;
VLOG(2) << "Failed to find a tunable parameter that would decrease the " "output time. This means that the autotuning optimization got " "stuck in a local maximum. The optimization attempt will be " "aborted.";
VLOG(1) << "Ignoring encountered unknown operation " << SummarizeNodeDef(*node) << " when stripping default attributes. It is likely a function, " "in which case ignoring it is fine";
VLOG(4) << "source id " << i << " " << (*thread_work_sources)[i]->ToString();
VLOG(2) << "Set work for tid=" << i << " with start_request_idx=" << request_idx_list[i];
VLOG(2) << "Set work for tid=" << (i + num_blocking_threads) << " with start_request_idx=" << request_idx_list[i];
VLOG(2) << "Running " << (task_from_blocking_queue ? "inter" : "intra") << " work from " << tws->GetTracemeId();
VLOG_IS_ON(4))
VLOG(4) << "source id " << i << " " << (*thread_work_sources)[i]->ToString();
VLOG(1) << "Testing VLOG(1)!";
VLOG(1) << "Annotated DNS mapping: " << name << " --> " << chosen_address;
VLOG(1) << "... address: " << buf;
VLOG(1) << "The operation failed and will be automatically retried in " << (delay_micros / 1000000.0) << " seconds (attempt " << (retries + 1) << " out of " << config.max_retries << "), caused by: " << status.ToString();
VLOG(2) << "Found fetch node for " << t;
VLOG(1) << "CSE: equivalent: " << (*candidate)->name() << " and " << n->name();
VLOG(1) << n->id();
VLOG(1) << e->id();
VLOG(1) << "Adding control dependency from node " << src_node->name() << " instance " << instance_keys[src_idx] << " to node " << dst_node->name() << " instance " << instance_keys[dst_idx];
VLOG(1) << "MklLayoutRewritePass: unusual case of same filter" << " feeding multiple Conv2D nodes: " << filter_node->DebugString();
VLOG(1) << "MklLayoutRewritePass: workspace_enabled for " << orig_node->type_string();
VLOG(1) << "MklLayoutRewritePass: workspace_enabled for " << orig_node->type_string();
VLOG(1) << "MklLayoutRewritePass: dummy workspace_enabled for " << orig_node->type_string();
VLOG(1) << "MklLayoutRewritePass: Scheduled nodes " << n1_name << " and " << n2_name << " for merging";
VLOG(1) << "MklLayoutRewritePass: Merged nodes " << n1_name << " and " << n2_name;
VLOG(1) << "MklLayoutRewritePass: Scheduled node " << node_name << " with op " << op_name << " for rewrite using" << " layout optimization.";
VLOG(1) << "MklLayoutRewritePass: rewrote node " << node_name << " with op " << op_name << " for Mkl layout optimization.";
VLOG(1) << "MklLayoutRewritePass: fixed metadata edges for node " << node_name << " with op " << op_name;
VLOG(1) << "Send/Recv control: " << src->assigned_device_name() << "[" << src->name() << "] -> " << dst->assigned_device_name() << "[" << dst->name() << "]";
VLOG(2) << "Add back edge: " << src_node->name() << " -> " << e.dst_node->name();
VLOG(1) << "MklToTfConversionPass: InsertConversionNodes: " << src->type_string() << " and " << dst->type_string();
VLOG(1) << "MklToTfConversionPass: Scheduled nodes " << src->name() << " and " << dst->name() << " for inserting conversion nodes";
VLOG(1) << "MklToTfConversionPass: Inserted conversion " << "node on edge between " << src_name << " and " << dst_name;
VLOG(1) << "MklToTfConversionPass: InputConversion: Scheduled node " << n->name() << " for inserting input conversion node";
VLOG(1) << "MklToTfConversionPass: Inserted conversion " << "on node " << n->name();
VLOG(2) << "Reverse reach : " << n->name() << " from " << in->name();
VLOG(2) << "Reverse reach : " << n->name() << " from " << in->name();
VLOG(2) << "collective edge " << edge->src()->name() << " -> " << edge->dst()->name();
VLOG(1) << "Collective " << col_params.ToString() << " blocked by instance " << instance;
VLOG(2) << "subdiv_i=" << subdiv_i << " i=" << i << " perm_i_base=" << perm_i_base << " subdiv_perms.size=" << col_params_.instance.impl_details.subdiv_permutations.size();
VLOG_IS_ON(2))
VLOG(2) << "subdiv_i=" << subdiv_i << " perm=" << sp_buf;
VLOG(2) << "dev=" << dev_name;
VLOG_IS_ON(1))
VLOG(1) << "Running Broadcast tree device=" << col_ctx_->device_name << " subdiv=" << si << " perm=" << subdiv_buf << " my_rank=" << my_rank << " source_rank=" << source_rank;
VLOG(2) << "copying input to output for device=" << col_ctx_->device_name << " subdiv=" << si;
VLOG(1) << "Creating bin of max chunk size " << strings::HumanReadableNumBytes(bin_size);
VLOG(5) << "|| " << line;
VLOG(2) << "Remove Identity: " << n->DebugString();
VLOG(4) << " [index " << i << "]" << " device: " << input_devices[i] << " (input: " << input_tensors[i] << ")";
VLOG(4) << " [index " << i << "] " << fbody->fdef.signature().input_arg(i).name() << " as " << n->name() << " (input: " << inputs[i].name() << ", requested_device: " << n->requested_device() << ")";
VLOG(4) << " [index " << i << "] " << fbody->fdef.signature().output_arg(i).name() << " as " << n->name() << " (ret: " << data.node->name() << ":" << data.index << ", requested_device: " << n->requested_device() << ")";
VLOG(4) << " [data output] add control edge from: " << n->name();
VLOG(4) << " [control output] add control edge from: " << n->name();
VLOG(3) << "noinline: " << SummarizeNode(*node);
VLOG(1) << "Failed to inline function call: node=" << p.first->name() << " error=" << inlined.error_message();
VLOG(2) << "Failed to find expected ScopedAllocator attr on " << use_node->name();
VLOG(1) << "Process node: " << id << " step " << params.step_id << " " << SummarizeNodeDef(item.kernel->def()) << (tagged_node.is_dead ? " is dead" : "") << " device: " << device->name();
VLOG(2) << "Async kernel done: " << state->item->node_id << " step " << step_id_ << " " << SummarizeNodeDef(state->item->kernel->def()) << (state->tagged_node.is_dead ? " is dead" : "") << " device: " << device->name();
VLOG(2) << "Synchronous kernel done: " << id << " step " << params.step_id << " " << SummarizeNodeDef(item.kernel->def()) << (tagged_node.is_dead ? " is dead: " : "") << " device: " << device->name();
VLOG(2) << "SessionFactory type " << session_factory.first << " accepts target: " << options.target;
VLOG(2) << "SessionFactory type " << session_factory.first << " does not accept target: " << options.target;
VLOG(2) << "num_subdivs " << num_subdivs << " num_chunks " << num_chunks << " chunk_size " << chunk_size;
VLOG(2) << "Saving " << n->DebugString();
VLOG(2) << "Restored " << n->DebugString();
VLOG(2) << "Mapping " << n->name() << " to " << n->cost_id();
VLOG(3) << added_device.error_message();
VLOG(3) << "Replace function: name=" << func_name;
VLOG(3) << "Add new function: name=" << func_name;
VLOG(1) << "Running optimization phase " << phase.first;
VLOG(1) << "Running optimization pass: " << pass->name();
VLOG_IS_ON(1))
VLOG(vlog_level) << "Registered optimization pass grouping " << grouping << " phase " << phase.first << ": " << pass->name();
VLOG(1) << "actual " << actual.SummarizeValue(100);
VLOG(2) << "Computed IOColocationGroups for node " << node->name() << ":\\" << groups.DebugString();
VLOG(2) << "\[" << absl::StrJoin(NodeAndBoolToString(nodes), "\\") << "]";
VLOG(2) << "Colocating \\"" << nodes[0].first->name() << "\\" and \\"" << nodes[i].first->name() << "\\"";
VLOG(2) << "Using BFCAllocator with memory limit of " << cpu_mem_limit_in_mb << " MB for ProcessState CPU allocator";
VLOG(2) << "Using PoolAllocator for ProcessState CPU allocator " << "numa_enabled_=" << numa_enabled_ << " numa_node=" << numa_node;
VLOG(2) << "Created " << DebugString(graph->get()) << " for " << partition_name;
VLOG(5) << " " << node->name() << ": requested: \'" << node->requested_device() << "\' assigned: \'" << node->assigned_device_name() << "\'";
VLOG(1) << "Node " << n->name() << " has input shape dimension " << i << " of " << shape.dim_size(i) << " but type INT32 " << " so not replacing as constant: this will trigger a " "runtime error later.";
VLOG(3) << "Trying to determine device for node " << node->name() << "[T=" << DataTypeString(dtype) << "]";
VLOG(3) << "Considering src: " << src_node->name() << " src_device: " << *src_device << " colo group: " << colocation_group;
VLOG(3) << "Considering src: " << src_node->name() << " src_device: " << *src_device << " colo group: " << colocation_group;
VLOG(3) << "Considering src: " << src_node->name() << " colo group: " << colocation_group;
VLOG(3) << "Setting output device to " << matching_devices[0]->name() << " for node " << SummarizeNode(*node);
VLOG(3) << "Did not set device for a resource output node " << SummarizeNode(*node);
VLOG(3) << "Setting output device to " << output_devices[index] << " for return at index " << index;
VLOG(3) << " [input " << index++ << "] " << device;
VLOG(3) << " [output " << index++ << "] " << device;
VLOG_IS_ON(1))
VLOG(1) << "Start instantiating component function " << unique_name << " on device " << target;
VLOG(4) << DebugString(shard);
VLOG(1) << "Finished instantiating component function " << unique_name << " with handle " << *component_handle << " status: " << s;
VLOG(2) << "Failed to get component function arguments: " << s;
VLOG(1) << "Running component function on device " << target << " with handle " << handle;
VLOG(4) << " with " << opts_copy.DebugString();
VLOG(2) << "Component function execution failed: " << status;
VLOG(3) << "Considering src: " << src_node->name() << " src_device: " << *src_device << " colo group: " << colocation_group;
VLOG(6) << "Adding identity into " << node->name() << ":" << src_output << " -> " << dst->name() << ":" << dst_input << " \" << identity_node->DebugString();
VLOG(6) << "Added identity into " << node->name() << ":" << output_idx << " -> <no consumer>: \" << identity_node->DebugString();
VLOG(2) << "Adding instance with for " << mgr_->device_name() << " scope_id=" << f.scope_id;
VLOG(1) << "field=" << i << " scope_id=" << field->scope_id << " bytes_requested=" << field->bytes_requested << " offset=" << field->offset << " bytes_allocated=" << field->bytes_allocated;
VLOG(4) << FieldState();
VLOG(2) << "inp mvec " << n->id() << " " << i << " " << inp_mvec[i];
VLOG(2) << "out mvec " << n->id() << " " << i << " " << out_mvec[i];
VLOG(1) << e->src()->id() << ":" << e->src_output() << " -> " << e->dst()->id() << ":" << e->dst_input() << ": " << sm << " -> " << dm;
VLOG(4) << FieldState();
VLOG(1) << "Get " << V(out);
VLOG(2) << "fields_shapes[" << i << "]=" << fields_shapes_[i].DebugString();
VLOG(1) << "actual " << actual.SummarizeValue(100);
VLOG(2) << "Node " << node_id << " " << n->type_string() << " " << n->name() << " " << n->in_edges().size() << " inputs";
VLOG(2) << " Edge from " << e->src()->id() << " " << e->src()->name() << " fanout " << e->src()->out_edges().size();
VLOG(3) << "Inspecting node " << n->DebugString();
VLOG(2) << "Created device_to_device_stream[" << stream_group_within_gpu << "] = " << group->device_to_device.back();
VLOG(1) << "GPUDevice PlatformGpuId " << platform_gpu_id << " TfGpuId " << tf_gpu_id << " on bus " << dev_locality.bus_id() << " numa: " << numa_node << " pci: " << desc->pci_bus_id() << " DeviceLocality: " << dev_locality.DebugString();
VLOG(1) << "Failed to run item: " << status;
VLOG(1) << " " << filter;
DVLOG(2) << "for op " << op->Name() << " input " << i << " " << DataTypeString(tensor_handle->dtype) << " input device = " << resource_device->name() << ", op device = " << op_device->name();
DVLOG(1) << (resource_device != op_device ? "Changing " : "Setting ") << "device of operation " << op->Name() << " to " << resource_device->name() << " because input #" << i << " is a resource in this device.";
DVLOG(2) << "for op " << op->Name() << " input " << i << " " << DataTypeString(tensor_handle->dtype) << " input device = " << input_device->name() << ", op device = " << op_device->name();
VLOG(1) << "Input ptr: " << input;
VLOG(3) << "Add new function definition: " << fdef.signature().name();
VLOG(0) << "Skip error: " << error_message;
VLOG(0) << "Skip error: " << error_message;
VLOG(3) << "Skip error: " << error_message;
VLOG(1) << "Add fetch " << f;
VLOG(1) << "Add feed " << f.first;
VLOG(1) << "Will use feed node " << feed.first;
VLOG(1) << "Will use fetch node " << fetch;
VLOG(4) << op_context.name << " has " << node_costs.num_ops_with_unknown_shapes << " unknown shapes";
VLOG(1) << "input: " << input << " not found for non-Merge node: " << op_name;
VLOG(4) << "Node with inaccurate time estimation: " << node;
VLOG(3) << "Added ready node: " << curr_node->name();
VLOG(3) << " Add output: " << output_node->name();
VLOG(1) << absl::StrFormat(" + %30s : %c %10d / %10d / %10d / %10d", op, (is_op_cost_accurate ? \' \' : \'~\'), cost, compute_cost, memory_cost, intermediate_memory_cost);
VLOG(1) << "Device = " << name << ", num_nodes = " << state.nodes_executed.size() << ", wall_time_ns = " << wall_time_ns.count() << ", memory usage: " << "persistent = " << HumanReadableNumBytes(persistent_memory_usage) << ", peak = " << HumanReadableNumBytes(state.max_memory_usage) << ", total = " << HumanReadableNumBytes(max_memory_usage) << ", at the end: " << HumanReadableNumBytes(state.memory_usage);
VLOG(1) << state.device_costs.num_ops_total << " ops processed in total, with " << state.device_costs.num_ops_with_unknown_shapes << " having unknown shapes";
VLOG(1) << device_annotation_stats.num_ops_annotated << " ops with shape annotation, with " << device_annotation_stats.num_ops_executed_more_than_once << " executed more than once, " << device_annotation_stats.num_ops_with_dynamic_shapes << " with dynamic shapes, " << device_annotation_stats.num_ops_with_incompatible_shapes << " with incompatible shapes, " << device_annotation_stats.num_ops_executed << " ops executed in total.";
VLOG(1) << "Per-op execution time / compute time / memory time " << " / intermediate memory time" << " (and memory usage at peak memory usage):";
VLOG(1) << absl::StrFormat( " + %30s : %c %10d / %10d / %10d / %10d", op.c_str(), (is_op_cost_accurate ? \' \' : \'~\'), cost, compute_cost, memory_cost, intermediate_memory_cost) << " (" << HumanReadableNumBytes(op_mem_usage) << " [" << mem_usage_percent << "%] " << (persisent_ops.count(op) > 0 ? ": persistent op)" : ")");
VLOG(1) << "Device = " << name << ", total_compute_time_ns = " << (is_total_cost_accurate ? "" : "~") << total_compute_time_ns.count() << ", utilization = " << utilization << "%";
VLOG(2) << "Node: " << item.first << ", Count: " << item.second << ", Individual Cost: " << (is_cost_accurate ? "" : "~") << cost << " us";
VLOG(1) << "At time " << event.timestamp << " allocated " << event.tensor->memory_used << " for tensor " << event.tensor->node << ":" << event.tensor->output_id;
VLOG(1) << "At time " << event.timestamp << " deallocated " << event.tensor->memory_used << " for tensor " << event.tensor->node << ":" << event.tensor->output_id;
VLOG(2) << "Use minimum dim size 1 because the shape is unknown.";
VLOG(1) << "Key:" << item.first << " Value:" << SummarizeAttrValue(item.second);
VLOG(1) << "Input Size: " << input_size << " Total Input Size:" << total_input_size;
VLOG(1) << "Input Count: " << input_count << " Largest Input Count:" << largest_input_count;
VLOG(1) << "Output Size: " << output_size << " Total Output Size:" << total_output_size;
VLOG(1) << "Missing accurate estimator for op: " << op_info.op();
VLOG(1) << "Missing accurate estimator for op: " << op_info.op();
VLOG(2) << "CalculateTensorSize() -- unknown dim: " << i;
VLOG(2) << "Node: " << node.name() << ", Op: " << node.op() << ", " << inputs << ", " << outputs;
VLOG(2) << p.first << " (" << p.second << ")";
VLOG(3) << "UpdateOutputShapesUsingAnnotatedInformation() -- node: " << node.name() << ", inferred output shape " << i << ": " << "ic->output(i): " << ic->DebugString(ic->output(i)) << ", annotated output shape: " << ic->DebugString(output_shape) << " -- " << node.ShortDebugString();
VLOG(2) << "Skipping feed node shape: " << node.name();
VLOG(3) << "Filling in graph properties for node: " << node.name();
VLOG(2) << "ComputeTransitiveFanin: problem with root node: " << root;
VLOG(2) << "ComputeTransitiveFanin: problem with node: " << input;
VLOG(2) << "ComputeTransitiveFanin: problem with node: " << input;
VLOG(3) << "Remove functions output: name=" << output.node_name << "(index = " << i << ")";
VLOG(1) << "Node not ready: " << graph.node(i).DebugString();
VLOG(0) << "extracted: " << i;
VLOG(2) << " Recomputation trigger " << current_trigger_node->name() << " depends on " << (*target_input_iterator)->name();
VLOG(2) << " " << original_node->name();
VLOG(1) << "Available memory unknown for device " << name;
VLOG(1) << "Missing properties for " << node->name();
VLOG(1) << "Shape not fully known for " << node->name();
VLOG(1) << "Unsupported dtype for " << node->name();
VLOG(1) << "Peak memory usage unknown for device " << name;
VLOG(1) << "Not enough time to swap: skipping " << live_tensor.node;
VLOG(1) << "Will swap fanout " << fanout_to_swap.node->name() << ":" << fanout_to_swap.port_id << " of tensor " << mem_info.port.node->name() << ":" << mem_info.port.port_id << " of size " << mem_info.memory_used;
VLOG(0) << "Cancel Transpose nodes around Pad:" << " transpose_before=" << transpose_before->node()->name() << " pad=" << pad->node()->name() << " transpose_after=" << absl::StrJoin(pad_fanout_transposes, ",", MutableNodeViewFormatter());
VLOG(2) << "Registered default graph optimizer: " << optimizer_name;
VLOG(2) << "Registered custom graph optimizer: " << optimizer_name;
VLOG(2) << "Can\'t register an optimizer by name: " << optimizer_name;
VLOG(2) << "Registered custom configurable graph optimizer: " << optimizer_config.name();
VLOG(2) << "Registered default graph optimizer: " << optimizer_config.name();
VLOG(2) << "Can\'t register an optimizer by name: " << optimizer_config.name();
VLOG(3) << "Stopping after iteration " << iteration << ", graph is tiny (#nodes = " << optimized_graph->node_size() << " < " << min_graph_nodes << ")";
VLOG(4) << "Starting optimization iteration " << iteration;
VLOG_IS_ON(4))
VLOG_IS_ON(4))
VLOG_IS_ON(4))
VLOG(3) << "Optimize function: function=" << func_name << " [" << function_idx++ << " of " << optimized_graph->library().function_size() << "]";
VLOG(3) << added_device.error_message();
VLOG(3) << "Optimize function: function=" << func_name << " [" << function_idx++ << " of " << optimized_graph->library().function_size() << "]";
VLOG(3) << "Got " << func_info->function_type() << " function: " << function_name << " with interface: " << interface_name;
VLOG(2) << "consumer before:\" << consumer->DebugString();
VLOG(2) << "consumer after:\" << consumer->DebugString();
VLOG(2) << "Cross-device " << node->name() << " " << input->device() << " -> " << node->device();
VLOG(2) << "Duplicate input device from " << node->name();
VLOG(1) << "GroupCrossDeviceControlEdges: Added " << SummarizeNodeDef(*noop);
VLOG(2) << "Rewriting input from " << input_name;
VLOG(2) << "Rewriting input from " << input_name;
VLOG(3) << "Updated DTYPE for Identity node: " << fanout.node_view()->node()->DebugString();
VLOG(2) << "Swapping: " << function_name << " TO: " << func_name;
VLOG(3) << "Generated constant node: " << SummarizeNodeDef(*const_node);
VLOG(3) << "Preserving edge from " << node->name() << ":" << port << "[" << node->op() << "] to " << output->name() << ":" << i << "[" << output->op() << "]";
VLOG(2) << "foldable(" << graph_->node(i).name() << ") = " << foldable;
VLOG(1) << "Failed to fold node " << node->DebugString() << "\Error message: " << s;
VLOG(0) << "Skip error: " << error_message;
VLOG(0) << "Skip error: " << error_message;
VLOG(3) << "Skip error: " << error_message;
VLOG(2) << "Changing op of " << node->op() << " node " << node->name() << " to FusedBatchNormV2";
VLOG(2) << "Changing op of " << node->op() << " node " << node->name() << " to FusedBatchNormGradV2";
VLOG_IS_ON(2) && inserted)
VLOG(2) << "Painting type " << root.type_attr.DebugString() << " of node " << root.node->name() << " WHITE because its op " << root.node->op() << " is on the whitelist";
VLOG_IS_ON(2) && inserted)
VLOG(2) << "Painting type " << item.type_attr.DebugString() << " of " << item.node->op() << " node " << item.node->name() << " BLACK";
VLOG_IS_ON(2) && inserted)
VLOG(2) << "Painting type " << item.type_attr.DebugString() << " of " << item.node->op() << " node " << item.node->name() << " WHITE";
VLOG_IS_ON(2) && inserted)
VLOG(2) << "Painting type " << item.type_attr.DebugString() << " of " << item.node->op() << " node " << item.node->name() << " WHITE";
VLOG(2) << "Painting type T of Merge node " << graph_type_view_.GetNode(merge_idx)->node->name() << " BLACK to match the color of its sibling Merge nodes " "with common NextIteration node " << node.name();
VLOG(2) << "Painting type " << node_type.type_attr.DebugString() << " of " << node_type.node->op() << " node " << node_type.node->name() << " " << (any_black ? "BLACK" : "WHITE") << " because at least one of its siblings is " << (any_black ? "BLACK" : "WHITE");
VLOG(1) << "Changing type " << type_attr.DebugString() << " of " << node->op() << " node " << node->name() << " to DT_HALF";
VLOG(1) << "Inserting cast to " << (to_fp16 ? "DT_HALF" : "DT_FLOAT") << " at " << src.node->op() << " " << src.node->name() << ":" << src.port_id;
VLOG(1) << "Fanout " << fanout_idx << " : " << fanout_node.name();
VLOG(1) << "Converting " << push_node_idx << " : " << push_node->DebugString();
VLOG(1) << "After converting: " << push_node->DebugString();
VLOG(3) << "Merge node before cleanup: " << merge_node->DebugString();
VLOG(3) << "Merge node after cleanup: " << merge_node->DebugString();
VLOG(3) << "Push const into function body: input=" << input;
VLOG(3) << "Forward control dependency: input=" << ctrl;
VLOG(4) << "Check that node " << side_effect->name() << " will execute after inlining.";
VLOG(4) << "Found a path to control source: " << side_effect->name() << " ---> " << (*it)->name();
VLOG(4) << "Add dead tensors source. Switch node: " << n->name();
VLOG(4) << "Add dead tensors source. Function call: " << func.name() << " node=" << n->name();
VLOG(4) << "Found a path to output node from dead tensor source: " << dead_tensor_source->name() << " ---> " << (*it)->name();
VLOG(2) << "Lower functional control flow op: " << SummarizeNode(*n);
VLOG(2) << "Ignore error: " << can_inline_function_call.error_message();
VLOG(2) << "Inline function call node: " << n->name();
VLOG(2) << "Failed to inline function call node: " << can_inline_function_call.error_message();
VLOG(2) << "op " << n->name() << " has type " << dtype << " shapes.size() " << shapes->size();
VLOG(2) << "Adding shape " << props.shape().DebugString();
VLOG(2) << " consider edge " << (*inputs)[edge_index];
VLOG(2) << "for node " << n->name();
VLOG(2) << "inode " << inode->DebugString() << " output_index " << output_index;
VLOG(2) << "inode after rewrite " << inode->DebugString() << " output_index " << output_index;
VLOG(2) << "inode " << inode->DebugString() << " output_index " << output_index;
VLOG(log_level) << line;
VLOG(2) << "get attrs for " << nd.from_node_def->name();
VLOG(1) << "Remove control output from " << input_node_name << " via edge " << input_name << " to " << n->name();
VLOG(2) << "TransitiveFanout parent: " << node->name() << " child: " << output->name() << " of type " << output->op();
VLOG(2) << "To input " << i << ": " << nd.from_node_def->name() << " add control input " << "^" << sa_name;
VLOG(2) << "Found node " << inputs_to_first[i].from_node_def->name() << " in the fanout of " << sa_name;
VLOG(2) << "Adding control dependency from " << inputs_to_first[i].from_node_def->name() << " to " << sa_node->name();
VLOG(3) << "old_op " << old_op->name() << " had " << output_nodes.size() << " outputs. Moving them to the ScopedAllocatorSplit node.";
VLOG_IS_ON(2))
VLOG(3) << " output: " << n->name();
VLOG(3) << "really checking old output " << n->name() << " for corresponding input.";
VLOG(3) << "Dropping control output from " << old_op->name() << " to " << n->name();
VLOG(3) << "about to iterate over " << n->input_size() << " inputs";
VLOG(3) << "input " << n->input(i);
VLOG(3) << "match pos=" << position;
VLOG(3) << "breaking on success";
VLOG(3) << "other input " << n->input(i);
VLOG(3) << "before HasOp";
VLOG(3) << "bottom of for output_nodes";
VLOG(3) << "Clearing all inputs of " << old_op->name();
VLOG(3) << "after clear: " << old_op->DebugString();
VLOG(2) << "op_instance_name " << nd->name();
VLOG(1) << "search target " << it;
VLOG(1) << "found " << op_name << " on dev " << node->device();
VLOG(2) << "Processing device " << dt.first;
VLOG(1) << "Processing " << op_name << " set size " << it.second.size();
VLOG(2) << "applied to tree node " << t->edge_ << " at depth " << t->depth_ << " of size " << t->nodes_.size();
VLOG(1) << "Applying Rewriter for " << op_name;
VLOG(2) << "TransitiveFanout parent: " << node->name() << " child: " << output->name() << " of type " << output->op();
VLOG(2) << "Moving node " << node.name() << " to device " << device;
VLOG(2) << "Swapping node " << node->name() << " back to device " << device;
VLOG(1) << "Init node: " << init;
VLOG(1) << "Fetch node: " << fetch;
VLOG(2) << "Variable: " << var->name();
VLOG(2) << "Apply gradients node: " << graph_.node(i).name();
VLOG(2) << "Could not convert the output at node: " << output_node->DebugString() << "\Error: " << s;
VLOG(1) << "Top element " << v[i];
VLOG(1) << "Enabling activity tracing for: " << activity;
VLOG(1) << "Disabling activity tracing for: " << activity;
VLOG(2) << std::endl << "step_id: " << step << ", step_info:" << std::endl << DebugStepInfo( (*per_core_step_info.mutable_step_info_per_core())[0]);
VLOG(2) << "Register " << c->req.graph_def().DebugString();
VLOG(2) << "Selectively listing workers in job: " << job_name;
VLOG(2) << "- " << filter;
VLOG(1) << "x = [" << x_matrix.shuffle(matrix_transpose) << "] y = [" << y_matrix.shuffle(matrix_transpose) << "] lambda = " << lambda
VLOG(3) << "Setting GRPC default for \'" << name_value[0] << "\' to \'" << name_value[1] << "\'";
VLOG(1) << "Erase stale GrpcResponseCache entry " << it->first;
VLOG(4) << "GrpcEagerClientThread got next tag";
VLOG(4) << "GrpcEagerClientThread blocking for next tag";
VLOG(2) << " " << da.name();
VLOG(2) << "Remote worker " << rw;
VLOG(1) << "Changing the parallel_iterations attribute of the " << "Enter/RefEnter node \\"" << node->name() << "\\" on device \\"" << device->name() << "\\" from " << parallel_iterations->i() << " to 1.";
VLOG(1) << "Falling back to slow path for Op \\"" << op_def.name() << "\\", Input \\"" << op_def.input_arg(i).name() << "\\" since we expected a sequence, but got " << item->ob_type->tp_name;
VLOG(1) << "Falling back to slow path for Op \\"" << op_def.name() << "\\", Input \\"" << op_def.input_arg(i).name() << "\\", Index " << j << " since we expected an EagerTensor/ResourceVariable, " "but got " << inner_item->ob_type->tp_name;
VLOG(1) << "Falling back to slow path for Op \\"" << op_def.name() << "\\", Input \\"" << op_def.input_arg(i).name() << "\\" since we expected an EagerTensor/ResourceVariable, but got " << item->ob_type->tp_name;
VLOG(1) << "Falling back to slow path for Op \\"" << op_def->name() << "\\" since we are unable to set the value for attr \\"" << attr.name() << "\\" due to: " << TF_Message(status);
VLOG(log_level) << HelpfulOperatorTypeName(*op) << " :";
VLOG(log_level) << " " << FormatArraysList(model, op->inputs) << " -> " << FormatArraysList(model, op->outputs);
VLOG(log_level) << " (with fused activation function)";
VLOG(1) << "Deduplicating arrays; using " << lhs_array_name << " in place of " << rhs_array_name;
VLOG(log_level) << " " << OperatorTypeName(*it) << ": " << count;
VLOG(log_level) << transformation->Name() << " " << made_a_change_msg << " at op_index=" << op_index << "/" << model->operators.size() - 1;
VLOG(log_level) << transformation->Name() << " " << made_a_change_msg << " at op_index=" << op_index << "/" << model->operators.size() - 1 << ": " << message;
VLOG(log_level) << transformation->Name() << " " << made_a_change_msg << " at op_index=" << op_index << "/" << model->operators.size() - 1;
VLOG(log_level) << transformation->Name() << " " << made_a_change_msg << " at op_index=" << op_index << "/" << model->operators.size() - 1 << ": " << message;
VLOG(1) << "Adding func to graph: " << fdef_to_load.DebugString();
VLOG(1) << " Added worker " << w;
VLOG(1) << " Removed worker " << w;
VLOG(1) << " Replaced worker " << w;
VLOG(1) << "Updating cluster with existing worker " << w;
VLOG(2) << "hit in cache";
VLOG(2) << "hit in cache for device ordinal " << config.ordinal;
VLOG(1) << DebugStreamPointers() << " reusing sub_stream " << sub_stream->DebugStreamPointers();
VLOG(1) << DebugStreamPointers() << " dropped !ok sub_stream " << sub_stream->DebugStreamPointers();
VLOG(1) << DebugStreamPointers() << " returned ok sub_stream " << sub_stream->DebugStreamPointers();
VLOG(1) << DebugStreamPointers() << " returned !ok sub_stream " << sub_stream->DebugStreamPointers();
VLOG(2) << "Looking for ptxas at " << ptxas_path;
VLOG(1) << "could not open \\"" << piece << "\\"";
VLOG(1) << piece << " :: " << entity->d_name;
VLOG(1) << piece << " :: " << entity->d_name;
VLOG(1) << "could not open \\"" << piece << "\\"";
VLOG(1) << piece << " :: " << entity->d_name;
VLOG(1) << piece << " :: " << entity->d_name;
VLOG(2) << "*(arg.address): " << reinterpret_cast<void*>( *static_cast<const uint64_t*>(arg.address));
VLOG(kImmediateModeVlogLevel) << "solution " << i << " (time, mem, id, algo) = " << solution.time << ", " << solution.workspace_size << ", " << solution.solution_id << ", " << ToString(solution.algorithm);
VLOG(kImmediateModeVlogLevel) << "solution " << i << " (time, mem, id, algo) = " << solution.time << ", " << solution.workspace_size << ", " << solution.solution_id << ", " << ToString(solution.algorithm);
VLOG(kImmediateModeVlogLevel) << "solution " << i << " (time, mem, id, algo) = " << solution.time << ", " << solution.workspace_size << ", " << solution.solution_id << ", " << ToString(solution.algorithm);
VLOG(3) << "CompareShapes: lhs and rhs have different dynamic dimensions.";
VLOG(1) << " argv[" << i << "] = " << env_argv->argv[i];
VLOG(3) << "Non-local device: " << device_id;
VLOG(4) << "Argument " << i << " buffer: " << argument_buffers.back().ToString();
VLOG(1) << "Received response for: " << event_id;
VLOG(1) << "Copying: " << it->second.num_bytes << " to position " << it->second.dst;
VLOG(1) << "Sending request: " << EventId::FromInt(e->operation_id());
VLOG(2) << "Sending request: " << e->DebugString();
VLOG(2) << "Received response: " << resp.DebugString();
VLOG(1) << "Received response for: " << event_id;
VLOG(1) << "Copying: " << it->second.num_bytes << " to position " << it->second.dst;
VLOG(2) << "Other devices, device id: " << device->id();
VLOG(3) << "Non-local device: " << device_id;
VLOG(10) << "Looking at instruction: " << instruction->name();
VLOG(10) << "Operand not profitable: " << operand->name();
VLOG(10) << "Operand profitable: " << operand->name();
VLOG(10) << "User: " << user->name();
VLOG(10) << "User is not fusible, or is the instruction itself: " << user->name();
VLOG(10) << "User is connected: " << user->name();
VLOG(10) << "User ID for user: " << user->name() << " is " << user_id << " which is higher than " << instruction_id;
VLOG(10) << "User not legal to fuse: " << user->name();
VLOG(10) << "User added to candidate list: " << user->name();
VLOG(1) << "Considering candidate profit_score=" << candidate.score << "\\\instr1 = " << instr1->ToString() << "\\\instr2 = " << instr2->ToString();
VLOG(1) << "Fuse!";
VLOG(2) << "Before multi_output_fusion:";
VLOG(2) << "instr1: " << instr1->ToString();
VLOG(2) << "\" << instr1->fused_instructions_computation()->ToString( HloPrintOptions().set_indent_amount(1));
VLOG(2) << "instr2: " << instr2->ToString();
VLOG(2) << "\" << instr2->fused_instructions_computation()->ToString( HloPrintOptions().set_indent_amount(1));
VLOG(2) << "After fusion, \ this: " << fusion->name() << "\" << fusion->fused_instructions_computation()->ToString( HloPrintOptions().set_indent_amount(1));
VLOG(10) << "transform permuting/subset of a scalar broadcast into " << "a single broadcast";
VLOG(10) << "Found non-trivial dimension being padded: " << i;
VLOG(10) << "Found window did not cover single unpadded element in " "dimension: " << i;
VLOG(10) << "Found window covers more than one element in non-trivial " "dimension: " << i;
VLOG(1) << "Set element: " << hlo->ToString();
VLOG(1) << "Skipping due to non-trivial reduction function.";
VLOG(1) << "Skipping due to non-trivial reduction function.";
VLOG(1) << "Considering HLO " << instructions.front()->ToString() << " with current set size of " << current_size_in_bytes << " and current operand count of " << current_operand_count;
VLOG(1) << "Skipping due to " << instructions.front()->operands().size() << " operands";
VLOG(1) << "Skipping due to size " << size_in_bytes << " above threshold";
VLOG(1) << "Starting new set due to dependency between " << previous[i]->ToString() << " AND " << instructions[i]->ToString();
VLOG(1) << "The instruction cannot be entered into the set due " "to the combined size being too large.";
VLOG(1) << "Skipping as the instruction is larger than the set.";
VLOG(1) << "Resetting the set as the set is larger than the instruction.";
VLOG(1) << "Adding instruction to set.";
VLOG(1) << "Done constructing sets. Final set size is " << current_size_in_bytes << " bytes and " << current_operand_count << " operands";
VLOG(4) << "Keeping value " << interval.buffer->ToShortString() << " in default mem because it has a tuple-select or " << "add-dependency position.";
VLOG(4) << "Interval " << interval.buffer->ToShortString() << " is reserved in the alternate memory. Total reserved bytes = " << reserved_in_bytes_;
VLOG(3) << "Coloring " << position.ToString();
VLOG(4) << "Not allocating " << interval.buffer->ToShortString() << " because it aliases with another interval and " << " allocate_across_sequential_calls is false.";
VLOG(3) << "Adding an aliased allocation: (" << aliased_allocation->start_time() << ", " << aliased_allocation->end_time() << ") pos: " << aliased_allocation->defining_position() << " mem space: " << (aliased_allocation->memory_space() == MemorySpace::kDefault ? "default" : "alt");
VLOG(3) << "skip use " << use.ToString() << " because it\'s in a different computation.";
VLOG(3) << "Allocation for " << value_and_sequence.value->ToShortString();
VLOG(3) << " " << alloc->start_time() << "-" << alloc->end_time() << addr_str << ", " << alloc->uses().size() << " uses";
VLOG(3) << "Adding required assignment for parameter value = " << value->ToShortString() << " time = " << parameter_instruction_time << " space = " << (memory_space == MemorySpace::kDefault ? "def" : "alt");
VLOG(3) << "Adding required assignment for output value = " << value->ToShortString() << " time = " << root_instruction_time << " space = " << (memory_space == MemorySpace::kDefault ? "def" : "alt");
VLOG(3) << "Committing chunk: " << interval_and_chunk.first.start << "-" << interval_and_chunk.first.end << " : [" << interval_and_chunk.second.chunk.offset << ", " << interval_and_chunk.second.chunk.size << "]";
VLOG(3) << "Try evicting (" << time << ", " << time + 1 << ")";
VLOG(3) << "Eviction successful.";
VLOG(4) << "Old shape = " << subshape.ToString() << ", new shape = " << new_instruction->shape().ToString() << "; inserting a bitcast.";
VLOG(3) << " [" << pair.second.offset << ", " << pair.second.size << "] : " << pair.first.ToString();
VLOG(3) << " space: " << pair.first << ", size: " << pair.second.size;
VLOG(3) << "Coloring " << position.ToString();
VLOG(3) << "Removing instruction from preset assignments.";
VLOG(4) << "Not simplifying " << computation->name() << " because it\'s not in the schedule.";
VLOG(4) << "Running simplify graph loop over " << computation->name();
VLOG(4) << "Instruction removed: " << instruction->ToString();
VLOG(4) << "Replacing uses of " << instruction->ToString() << " with " << forwarded_instruction->ToString();
VLOG(4) << "Delaying CopyStart (" << copy_start_schedule_after << " to " << (copy_start_schedule_after + 1) << ") for " << copy_allocation->copy_start()->ToString() << " because it is not in the correct computation.";
VLOG(4) << "Not scheduling " << computation->name() << " because it\'s not in the schedule.";
VLOG(4) << "Scheduling: " << computation->ToString();
VLOG(4) << "before " << instruction_index << ": " << new_instruction->name();
VLOG(4) << "inst " << instruction_index << ": " << instruction->name();
VLOG(4) << "after " << instruction_index << ": " << new_instruction->name();
VLOG(3) << " value: " << value->ToShortString() << " (" << time_bound.start << ", " << time_bound.end << ")";
VLOG(3) << " buffer: " << buffer.ToString() << ": (" << start_time << ", " << end_time << ") off: " << position_and_chunk.second.offset << ", size: " << position_and_chunk.second.size;
VLOG(3) << "Memory usage: " << memory_usage << " at time: " << time;
VLOG(4) << "Trying alternate memory allocation (" << alternate_mem_interval.start << ", " << alternate_mem_interval.end << ")";
VLOG(4) << "This would violate the outstanding async copy limit.";
VLOG(4) << "This would violate asynchronous copy ordering.";
VLOG(3) << "Move the buffer to alternate memory at " << alternate_mem_interval.start << ". Offset = " << chunk_candidate.chunk.offset << ", size = " << chunk_candidate.chunk.size << ", heap_size = " << chunk_candidate.heap_size << ", prefetch picker = " << options_.prefetch_interval_picker->ToDebugString();
VLOG(4) << "Running simplify graph loop over " << computation->name();
VLOG(4) << "Instruction removed: " << instruction->ToString();
VLOG(4) << "Replacing uses of " << instruction->ToString() << " with " << forwarded_instruction->ToString();
VLOG(4) << "Delaying CopyStart (" << copy_start_schedule_after << " to " << (copy_start_schedule_after + 1) << ") for " << copy_allocation->copy_start()->ToString() << " because it is not in the correct computation.";
VLOG(2) << "Adding " << instruction->ToString(print_no_metadata) << " to unhoisted invariant set.";
VLOG(2) << "Hoisting " << instruction->ToString(print_no_metadata);
VLOG(2) << "shaped_buffer:" << shaped_buffer;
VLOG(2) << " " << pair.first;
VLOG(2) << "Value " << value->ToShortString() << " is read only, but its buffer contains more than one value. " "Copying.";
VLOG(2) << "Output indices " << index.ToString() << " and " << other_index.ToString() << " are both aliased to " << alias->parameter_number << " copying " << other_index;
VLOG(2) << "Index " << index << " of computation " << computation->name() << " (" << root->name() << ") has ambiguous or non-distinct buffer. Copying.";
VLOG(2) << "Root of (" << root->name() << ") of computation(" << computation->name() << ") has constant or parameter value at index " << index << ". Copying.";
VLOG(2) << "Running fixpoint iteration " << num_iterations << " of copy elision";
VLOG(5) << "input value set = " << input->ToString();
VLOG(5) << "current_value_defined_here: " << current_value->ToString();
VLOG(5) << "after input_value_ids.size = " << input_value_ids.size();
VLOG(3) << "Worklist top: " << instruction->name();
VLOG(3) << ToString();
VLOG(4) << "No change.";
VLOG(4) << "New value set for " << instruction->name() << ": " << GetInstructionValueSet(instruction);
VLOG(2) << "Found passthrough domain link:";
VLOG(2) << " " << user->ToString();
VLOG(2) << " " << instruction->ToString();
VLOG(2) << "Found passthrough domain link:";
VLOG(2) << " <root>";
VLOG(2) << " " << instruction->ToString();
VLOG(4) << " " << instruction->name() << " already has sharding " << instruction->sharding();
VLOG(1) << "Starting infeed on device " << device;
VLOG(1) << "Infeed step " << step;
VLOG(1) << "Starting outfeed on device " << device;
VLOG(1) << "Outfeed step " << step;
VLOG(6) << "Propagating constraint to operand " << operand_no << " of " << instruction->ToShortString();
VLOG(6) << "Operand already has a constraint " << constraint->ToString();
VLOG(3) << "Propagating layout through backedge" << buffer_constraint.layout().ToString();
VLOG(5) << "Running " << (i == 0 ? "un" : "") << "constrained pass";
VLOG(5) << "Removing added copy: " << instruction->ToString();
VLOG(2) << "Propagating " << layout_constraint->ToString() << " to its neighbors.";
VLOG(4) << " use is conditional " << use.instruction->name() << " and def is in " << j << "th branch computation";
VLOG(4) << "use of " << a << " (" << use << ") not before " << b << " is defined";
VLOG(4) << a << " is live out of computation and defined before " << b << " which is in same computation";
VLOG(3) << "HandleSort operand " << i << " literal: " << GetEvaluatedLiteralFor(sort->operand(i)).ToString();
VLOG(3) << "HandleReduce arg_literal: " << input_args[i]->ToString();
VLOG(3) << "HandleReduce init_literal: " << init_values[i]->ToString();
VLOG(2) << StrFormat("starts[%d] = %d", dimension, start_index);
VLOG(2) << StrFormat("limits[%d] = %d", dimension, limit_index);
VLOG(2) << StrFormat("slice_sizes[%d] = %d", dim, slice_dim_size);
VLOG(2) << StrFormat("update_sizes[%d] = %d", dim, update_dim_size);
VLOG(1) << "Removing dead root " << dead_root->ToString() << " and it\'s unused operands";
VLOG(1) << "WhileDCE SKIP while: " << xla_while->ToString();
VLOG(1) << "Moved value " << value->ToShortString() << " to while param: " << used->ToString();
VLOG(2) << "Schedule instruction: " << best->ToShortString() << " Bytes freed: " << best_it->first.first;
VLOG(1) << " Invariant checker " << invariant_checker->name();
VLOG(1) << " Invariant checker done " << invariant_checker->name();
VLOG(2) << "Failed invariant check:";
XLA_VLOG_LINES(2, hlo->ToString());
VLOG(1) << " HLO pass " << pass_name;
VLOG(1) << "Time step: " << i;
VLOG(1) << "Start buffer: " << value->ToShortString();
VLOG(1) << " ShareWith" << first_allocated_value[hlo_buffer]->ToShortString();
VLOG(1) << "Sharing " << value->ToShortString() << " with " << operand_value->ToShortString() << ", size:" << size_fn_(*value);
VLOG(1) << "Free Buffer: ";
VLOG(1) << " " << value->ToShortString();
VLOG(1) << " Alias size " << colocation_interval.size << ", start " << colocation_interval.start << ", end " << colocation_interval.end << " " << colocation_interval.buffer->ToString();
VLOG(2) << "instr uses something other than a constant or gte(gte_operand, " << tuple_idx << "): " << operand->ToString();
VLOG(2) << "Couldn\'t evaluate while cond: " << result.status();
VLOG(2) << "Loop has static trip count of " << trip_count;
VLOG(2) << "Couldn\'t evaluate induction variable update: " << indvar_next_result.status();
VLOG(1) << "Not initializing StreamExecutor for device " << i << " since it is not in the visible device list";
VLOG(1) << "Started device init " << i;
VLOG(1) << "Finished device init " << i;
VLOG(4) << "New domain: " << domain->ToString();
VLOG(3) << "Skip RemoveUnusedTupleElements due to non-GTE user:\" << user->ToShortString();
VLOG(3) << "Skip RemoveUnusedTupleElements due to some branch " << branch->name() << " has in-compatible root shape, expect " << old_shape.ToString() << ", but got " << root->shape().ToString() << "\" << conditional_op->ToString();
VLOG(3) << "Skip MergeDuplicateTupleElements due not all users are " "kGetTupleElement:\" << conditional->ToShortString();
VLOG(3) << "Skip MergeDuplicateTupleElements due not all branch roots " "are kTuple:\" << conditional->ToShortString();
VLOG(3) << "Merging colocated values, value: " << value->ToShortString();
VLOG(3) << " value is init value to a while; must share buffer with " "while value " << while_value.ToShortString();
VLOG(3) << " value is parameter value of the body or condition of a " "while; must share buffer with while value " << while_value.ToShortString();
VLOG(3) << " value @ " << position << " is root of " << callsite.instruction()->name() << "; body root and while value root must share buffer " "among them : " << while_value.ToShortString();
VLOG(3) << " value @ " << position << " is root of " << callsite.instruction()->name() << "; branch computation roots must share buffer among them : " << cond_value.ToShortString();
VLOG(2) << "Use of value " << value.ToShortString() << ": " << use;
VLOG(1) << values[i - 1]->ToShortString() << " and " << values[i]->ToShortString() << " are not ordered";
VLOG(1) << "In buffer " << buffer.id() << " containing values:\ " << absl::StrJoin(values, ", ", [](string* out, const HloValue* value) { StrAppend(out, value->ToShortString()); }) << "\Value " << values[i - 1]->ToShortString() << " may interfere with value " << values[i]->ToShortString();
VLOG(10) << "Adding new slice: " << user_slice->ToString() << " to replace: " << user->ToString();
VLOG(4) << " " << instruction->name();
VLOG(4) << " User side: " << instruction->name();
VLOG(4) << " " << meta.ToString();
VLOG(4) << " Operand side: " << instruction->name();
VLOG(4) << " " << meta.ToString();
VLOG(2) << "Cowardly refusing to analyze while loop with " << instr->ToString(print_no_metadata) << " used by non-GTE instruction " << user->ToString(print_no_metadata) << " in computation " << instr->parent()->name();
VLOG(2) << "Loop " << while_op->ToString(print_no_metadata) << " uses all of its inputs; no simplification possible.";
VLOG(2) << "Tuple index " << i << " is not passed through loop body unmodified.";
VLOG(2) << "Loop " << while_op->ToString(print_no_metadata) << " uses all of its inputs; no simplification possible.";
VLOG(2) << "Remapping tuple index " << old_idx << " to " << new_idx;
VLOG(3) << "Found loop invariant tuple element " << i << " " << init_tuple_elem->ToString();
VLOG(3) << "tuple index " << instr->tuple_index() << " " << instr->ToString();
VLOG(3) << "Replace use of " << instr->ToString() << " with " << hlo_constant->ToString();
VLOG(10) << "Found existing trip counter at index " << i;
VLOG(10) << "Found induction variable at index " << i;
VLOG(3) << "Adding css for edge " << edge_id << " from node " << from_node->name() << " to node " << to_node->name();
VLOG(3) << "Adding css for edge " << edge_id << " from node " << from_node->name() << " to root tag";
VLOG(2) << "Has dynamic dimension of operand" << operand_num << " @" << input_dim;
VLOG(3) << "Not visiting HLO (id = " << current_id << ") as it was already visited.";
VLOG(2) << "Visiting HLO %" << current_node->name();
VLOG(3) << " %" << new_operand->name();
VLOG(2) << "Constant folding failed for instruction: " << instruction->ToString();
VLOG(4) << "Constant folded: " << instruction->ToString();
VLOG(5) << "Companion instructions at path index " << i << " do not have the same opcode: " << path0[i].ToString() << " vs " << path1[i].ToString();
VLOG(1) << "VISIT instruction: " << instruction->name();
VLOG(3) << "Examining value " << *value;
VLOG(3) << "Has allocation";
VLOG(3) << "No allocation";
VLOG(1) << "Combined temp allocation for color " << color << " is: " << temp_allocation;
VLOG(1) << "Combined allocation absorbing temp allocation: " << temp_allocation;
VLOG(4) << "Can\'t assign: " << value->instruction()->ToString() << " cannot live out of the module";
VLOG(4) << "Can\'t assign: " << buffer_offset_size.first->instruction() << " cannot live out of the module";
VLOG(4) << "Can\'t assign: assignee " << assigned_buffer << " live range interferes with " << new_value->ToShortString();
VLOG(4) << "Can\'t assign: assignee " << assigned_buffer << " may interfere with " << new_value->ToShortString();
VLOG(4) << "Can\'t assign: assignee " << assigned_buffer << " is used at copy instruction " << new_value->ToShortString();
VLOG(3) << "New allocation #" << allocation->index() << " for constant " << *hlo_buffer << " value ptr: " << value;
VLOG(3) << "New allocation #" << allocation->index() << " for tuple-shaped buffer: " << *hlo_buffer;
VLOG(3) << "Reusing (operand) allocation #" << allocation->index() << " for: " << *hlo_buffer;
VLOG(3) << "Reusing allocation #" << allocation->index() << " for: " << *hlo_buffer;
VLOG(3) << "Delaying assignment of temp buffer: " << *hlo_value;
VLOG(3) << "Skip allocation for buffer: " << buffer;
VLOG(3) << "=================================================";
VLOG(3) << "Assigning buffer for " << *buffer;
VLOG(3) << "Created preset buffer allocation " << inserted_allocation->index() << ", color: " << inserted_allocation->color() << ", size: " << inserted_allocation->size();
VLOG(3) << "Preset allocation for value: " << value.ToShortString();
VLOG(2) << "Simulating heap for color " << color;
VLOG(2) << "Simulating heap for color " << color;
VLOG(4) << " " << value->ToString();
VLOG(3) << "maybe_live_out LogicalBuffer: " << *buffer;
VLOG(3) << "maybe_live_out BufferAllocation: " << *alloc;
VLOG(1) << proto->name();
VLOG(3) << "ExecuteGraphParallel created HloModuleConfig computation layout: " << module_config->entry_computation_layout().ToString();
VLOG(2) << "Replaced replicated all-reduce:" << ar->ToString();
VLOG(2) << "Replacing ArCrsPair: " << prev_pair.ToString() << " with ArCrsPair: " << pair.ToString();
VLOG(2) << "KeepProvablyEqualInstructionGroups. Checking AllReduce channel id: " << channel_id << "\";
VLOG(2) << "KeepProvablyEqualInstructionGroups. Erased AllReduce " "channel id: " << channel_id << "\";
VLOG(2) << "KeepProvablyEqualInstructionGroups. Checking AllReduce channel id: " << channel_id << "\";
VLOG(2) << "KeepProvablyEqualInstructionGroups. Erased AllReduce " "channel id: " << channel_id << "\";
VLOG(2) << "KeepProvablyEqualInstructionGroups. Erased AllReduce " "channel id: " << channel_id << "\";
VLOG(5) << "Found first non-trivial reshape operand of " << hlo->ToString(HloPrintOptions().set_print_metadata(false)) << ":\\" << operand->ToString(HloPrintOptions().set_print_metadata(false));
VLOG(3) << "Updating operand #" << i << ": " << operands[i]->ToString(print_no_metadata);
VLOG(5) << "Operand shape differs from output shape; so preventing " "movement\\operand: " << operand->ToString(print_no_metadata) << "\\instruction: " << instruction->ToString(print_no_metadata);
VLOG(5) << "Operand can trivially change shape: " << operand->ToString(print_no_metadata);
VLOG(5) << "Operand can\'t trivially change shape: " << operand->ToString(print_no_metadata);
VLOG(5) << "First reshape operand " << operand->ToString(print_no_metadata);
VLOG(5) << "Operand is an equivalent reshape of the first reshape operand " << operand->ToString(print_no_metadata);
VLOG(5) << "Operand is a reshape but is not equivalent to the first " "Reshape operand" << operand->ToString(print_no_metadata);
VLOG(5) << "candidate " << instruction->ToString();
VLOG_IS_ON(5))
VLOG(5) << "candidate " << instruction->ToString();
VLOG(2) << " " << instruction_to_outline->ToString();
VLOG(5) << "Removing " << operand->name();
VLOG(5) << "Removing " << root->name();
VLOG(3) << "ComputeArrayForGather: slice_sizes[" << i << "] != source->shape().dimensions(" << i << ") -- " << source->shape().dimensions(i) << " vs. " << slice_sizes[i] << " with dim_numbers.collapsed_slice_dims(0) = " << dim_numbers.collapsed_slice_dims(0);
VLOG(2) << instr->ToString() << " -> " << analysis.ToString(t);
VLOG(3) << " Buffer " << buffers_.at(buffer_id).ToString() << " is now live.";
VLOG(3) << " " << buffer.ToString() << " is now dead.";
VLOG(3) << " " << buffer.ToString() << " is immediately dead.";
VLOG(3) << "candidate " << candidate->name() << "(" << candidate->ToShortString() << ")" << " now best when compressed into " << compact_shape.ToString(true);
VLOG(2) << " Replacing use of " << best->name() << " in " << user->name() << " with " << remat->name();
VLOG(5) << " Replacing use of " << best->name() << " in " << user->name() << " with " << uncompressed->name();
VLOG(2) << "Program point at " << instruction->name() << ", memory usage = " << memory_tracker.memory_usage() << ", callee usage = " << callee_usage << ", [" << instruction_index << "/" << instruction_list.size() << "]";
VLOG(2) << "Over memory limit at instruction " << instruction->name() << ", using " << HumanReadableNumBytes(memory_tracker.memory_usage() + callee_usage) << ", limit is " << HumanReadableNumBytes(memory_limit_bytes);
VLOG(1) << "memory_usage after rematerialization = " << HumanReadableNumBytes(memory_tracker.memory_usage());
VLOG(3) << "candidate " << candidate->name() << "(" << candidate->ToShortString() << ")" << " now best when compressed into " << compact_shape.ToString(true);
VLOG(2) << "Over memory limit at instruction " << instruction->name() << ", using " << HumanReadableNumBytes(memory_tracker.memory_usage() + callee_usage) << ", limit is " << HumanReadableNumBytes(memory_limit_bytes);
VLOG(1) << "memory_usage after rematerialization = " << HumanReadableNumBytes(memory_tracker.memory_usage());
VLOG(2) << "-- argument " << a;
VLOG(3) << "Looking at producer " << producer->name() << " and its consumer " << consumer->name();
VLOG(3) << "Consumer " << consumer->name() << " is not eligible as multi-output fusion root.";
VLOG(3) << producer->name() << " and " << consumer->name() << " are not fusible.";
VLOG(3) << producer->name() << " would introduce a cycle when fused.";
VLOG(3) << producer->name() << " and " << consumer->name() << " would be too large of a fusion.";
VLOG(3) << "Considering " << (*i)->name();
VLOG(3) << "Considering " << (*i)->name() << " and " << (*j)->name();
VLOG(2) << "Fuse siblings " << (*i)->name() << " and " << (*j)->name();
VLOG(3) << producer->name() << " is a constant.";
VLOG(2) << "Fuse producer " << producer->name() << " into its consumer " << consumer_for_fusion->name();
VLOG(2) << "Fuse producer " << producer->name() << " and its consumer " << consumer_for_fusion->name() << " into " << input_fusion->name();
VLOG(5) << "Processing logical dimension " << logical_dim << " of size " << dim_size;
VLOG(5) << "logical_reduce_dim = " << logical_reduce_dim << ", " << "physical_reduce_dim = " << physical_reduce_dim;
VLOG(2) << "Executing the thunk for " << thunk->hlo_instruction()->ToString() << " on stream " << stream_no;
VLOG(3) << "Resolved global " << llvm_ir::ConstantBufferAllocationToGlobalName(allocation) << " to " << global.opaque();
VLOG(3) << "H2D memcpy for constant with shape " << ShapeUtil::HumanString(literal.shape());
VLOG(3) << absl::StreamFormat("Device %d assigned ncclComm %p", devices_[i], raw_comms[i]);
VLOG(3) << absl::StreamFormat( "Calling ncclAllReduce(send_buffer=%p, recv_buffer=%p, count=%d, " "comm=%p, stream=%p)", send_buffer, recv_buffer, buffer.element_count, static_cast<const void*>(comm), cu_stream);
VLOG(2) << "Reject unsupported fusion instr " << instr->ToString();
VLOG(2) << "Reject maybe illegal instr " << instr->ToString() << "; including it may create cycles in HLO.";
VLOG(2) << "Reject may-not-be profitable fusion instr " << instr->ToString();
VLOG(2) << "Reject non-row-major fusion instr " << instr->ToString();
VLOG(2) << "Find a fusion candidate " << instr->ToString();
VLOG(1) << "Forward convolution\'s window " << conv->window().ShortDebugString() << " should have stride of 1.";
VLOG(1) << "Forward convolution\'s window " << conv->window().ShortDebugString() << " should have no base (LHS) dilation.";
VLOG(1) << "Padding low should be non-negative.";
VLOG(1) << "Window reversal field not supported";
VLOG(1) << "Forward convolution\'s window " << conv->window().ShortDebugString() << " should have stride of 1.";
VLOG(1) << "Forward convolution\'s window " << conv->window().ShortDebugString() << " should have no window dilation.";
VLOG(1) << "Window reversal field not supported";
VLOG(2) << "Buffer " << i << " -> " << buf.opaque() << " (" << buf.size() << "B)";
VLOG(3) << "Trying algorithm " << AlgorithmToString(alg) << " for " << instr->ToString();
VLOG(1) << "Full module on failure: \" << instr->GetModule()->ToString();
VLOG(3) << "Trying algorithm " << AlgorithmToString(alg) << " for " << instr->ToString();
VLOG(5) << "Processing dimension " << logical_dim << " of size " << shape.dimensions(logical_dim);
VLOG(5) << "This and consecutive dimension are reduced, merging";
VLOG(2) << "cublas gemm algorithm " << algorithm << " took " << profile_result.elapsed_time_in_ms() << "ms" << std::endl;
VLOG(3) << "Executing condition computation";
VLOG(3) << "condition_result = " << condition_result;
VLOG(3) << "Executing body computation";
VLOG(2) << "Found ROCm-Device-Libs dir " << potential_rocdl_dir;
VLOG(2) << "Unable to find potential ROCm-Device-Libs dir " << potential_rocdl_dir;
VLOG(3) << " Arg: alloc #" << arg->index() << ": " << buf.opaque() << " (" << buf.size() << "B)";
VLOG(1) << "Before running FusionInstructionMerger for computation: " << computation->name();
XLA_VLOG_LINES(3, computation->ToString());
VLOG(1) << "After running FusionInstructionMerger for computation: " << computation->name() << " changed: " << changed;
XLA_VLOG_LINES(3, computation->ToString());
VLOG(2) << "Unbinding " << hlo_to_unbind->ToString();
VLOG(2) << "Looking for libdevice at " << libdevice_dir;
VLOG(2) << "Found libdevice dir " << libdevice_dir;
VLOG(0) << "RunBackend() - Will load PTX from file: " << filename;
VLOG(2) << "Parameter " << root->operand(i)->parameter_number() << " with shape " << root->operand(i)->shape().ToString() << " in module " << module->name() << " is passed-through to root tuple element " << i << ": " << root->shape().ToString();
VLOG(3) << "Buffer for " << instr->ToString() << " at " << index.ToString() << " is found in slice " << slice.ToString() << " at GTE index " << gte_index.ToString();
VLOG(10) << "Emit prologue for reduction: " << reduce_inst->ToString();
VLOG(3) << "Added shmem buffer for parameter " << id << ": " << llvm_ir::DumpToString(*param_shmem_buffers[id]);
VLOG(10) << "Input not safe for shmem transpose " << input->ToString();
VLOG(3) << "Emitted initializer for constant with shape " << ShapeUtil::HumanString(literal.shape());
VLOG(3) << "Enqueueing " << queue_name_ << " buffer (of " << buffers.size() << " buffers) with length: " << b->length();
VLOG(2) << "Enqueueing outfeed buffer (for the device to populate) of length " << size_32 << "B";
VLOG(1) << "Compiling ahead-of-time: " << module->name();
VLOG(3) << allocation.ToString();
VLOG(3) << "allocation #" << i << " is a parameter";
VLOG(3) << "allocation #" << i << " is a constant";
VLOG(3) << "buffer #" << i << " is thread-local";
VLOG(3) << "buffer #" << i << " is in the preallocated result ShapedBuffer";
VLOG(3) << "buffer #" << i << " allocated " << buffer_size << " bytes [" << owning_buffers[i]->opaque() << "]";
VLOG(2) << "Assigned parallel task count: " << total_partition_count << " to instruction: " << new_root->name() << " parent: " << new_root->parent()->name();
VLOG(3) << "ParallelForkJoin partition " << i << " done.";
VLOG(2) << s;
VLOG(2) << "Looking for libdevice at " << libdevice_dir;
VLOG(2) << "Found libdevice dir " << libdevice_dir;
VLOG(3) << "Fetching result from device " << i << ": " << ShapeUtil::HumanString(shape);
VLOG(1) << "Requesting to unregister " << handle.ShortDebugString();
VLOG(1) << " edge to sink node " << src->name() << " -> " << e->dst()->name();
VLOG(1) << "Not a TF-TRT candidate, " << "(Op type: " << node->tf_node()->type_string() << "), " << "(Op name: " << node->name() << "), " << "(Reason: excluded by segmenter option)";
VLOG(1) << "Not a TF-TRT candidate, " << "(Op type: " << node->tf_node()->type_string() << "), " << "(Op name: " << node->name() << "), " << "(Reason: " << status << ")";
VLOG(2) << "Accepted as a TF-TRT candidate, " << "(Op type: " << node->tf_node()->type_string() << "), " << "(Op name: " << node->name();
VLOG(3) << "Trying node " << node->name() << " id=" << node->id();
VLOG(3) << "... not a TRT candidate";
VLOG(3) << "... out node " << out_edge->dst()->name() << " ( " << out_edge->dst()->id() << " <- " << node->id() << " )";
VLOG(3) << "... ... Control Edge, Skipping";
VLOG(3) << "... ... not a TRT candidate";
VLOG(3) << "... ... can contract";
VLOG(3) << "... ... cannot contract, would form cycle";
VLOG(3) << "Merge " << src->name() << " <- " << dst->name() << " (" << src->id() << " <- " << dst->id();
VLOG(2) << "Node " << tf_node->name() << " has no device assigned requested device is: " << tf_node->requested_device();
VLOG(1) << "Segment original size: " << segment_nodes.size();
VLOG_IS_ON(2))
VLOG(2) << "----> Need to remove node " << in->name() << " because one of its " << (is_input_nodes ? "output" : "input") << " nodes in the graph was removed: " << node->name();
VLOG(1) << "Segment new size: " << segment_nodes.size();
VLOG_IS_ON(1) && !segment_nodes.empty())
VLOG(1) << "Nodes in segment " << segments->size() << " with parent=" << segment_root << ":" << s;
VLOG(1) << "Segment " << segments->size() << " has only " << num_effective_nodes << " effective nodes, dropping";
VLOG(1) << "No device assigned to segment " << segments->size();
VLOG(1) << "Devices " << s;
VLOG(3) << "... out node " << out_edge->dst()->name() << " ( " << out_edge->dst()->id() << " <- " << node->id() << " )";
VLOG(3) << "... ... Control Edge, Skipping";
VLOG(3) << "... ... not a TRT candidate";
VLOG(3) << "... ... can contract";
VLOG(3) << "... ... cannot contract, would form cycle";
VLOG(3) << "Merge " << src->name() << " <- " << dst->name() << " (" << src->id() << " <- " << dst->id();
VLOG_IS_ON(2))
VLOG(2) << "----> Need to remove node " << in->name() << " because one of its " << (is_input_nodes ? "output" : "input") << " nodes in the graph was removed: " << node->name();
VLOG(1) << "Creating execution context " << i;
VLOG(1) << "Created profile " << profiles_.back().DebugString();
VLOG(1) << "Added optimization profile " << profiles_[i].DebugString() << " to builder config.";
VLOG(2) << "Restored profile " << cfg.DebugString();
VLOG_IS_ON(2))
VLOG(2) << "Setting max_batch_dim to " << max_batch_dim << " using batch dimension of " << f.first << " with shape " << shape;
VLOG(2) << "PADDING_" << i << " pre: " << left << ", post: " << right << "paras: " << input_dims[i] << ", " << stride.d[i] << ", " << "kernel: " << kernel.d[i];
VLOG(1) << " " << trt_plugin_creator_list[i]->getPluginName();
VLOG(2) << "Adding out tensor " << output_name << ": " << output.DebugString();
VLOG(1) << "Marking output TRT tensor " << output.source_tensor_name << " with data type " << DebugString(output.trt_dtype) << ", which feeds TF node " << output.dest_node_name;
VLOG(2) << " " << layer->getName() << " (" << "type: " << static_cast<int>(layer->getType()) << ", precision: " << static_cast<int>(layer->getPrecision()) << ")";
VLOG(1) << "Setting range for: " << tensor->getName() << ": " << range;
VLOG(1) << pattern.first;
VLOG(1) << " Fused output tensor:" << fused_layer->getOutput(i)->getName();
VLOG(1) << "And setting layer " << layer->second->getName() << " precision to fp16.";
VLOG(1) << "Copy quantization range: " << it->first->getName() << " -> " << it->second->getName();
VLOG(2) << "Retrieved input " << name << ": " << input.DebugString();
VLOG(2) << "Converting node " << node_name << ", op=" << node_def.op();
VLOG(2) << "Adding engine input tensor " << node_name << " with shape " << DebugString(trt_dims);
VLOG(1) << "Reusing input " << node_name << " for the edge " << connection.outside_node_name << ":" << connection.outside_port << " -> " << connection.inside_node_name << ":" << connection.inside_port;
VLOG(1) << "Constructing input " << node_name << " for the edge " << connection.outside_node_name << ":" << connection.outside_port << " -> " << connection.inside_node_name << ":" << connection.inside_port;
VLOG(1) << "Reusing output " << node_name << " for the edge " << connection.inside_node_name << ":" << connection.inside_port << " -> " << connection.outside_node_name << ":" << connection.outside_port;
VLOG(1) << "Constructing output " << node_name << " for the edge " << connection.inside_node_name << ":" << connection.inside_port << " -> " << connection.outside_node_name << ":" << connection.outside_port;
VLOG(2) << "Copying " << snode->name() << " to subgraph";
VLOG(1) << "Updating " << snode->name() << ":" << connection.inside_port << " from " << snode->input(connection.inside_port) << " to " << arg_name;
VLOG(1) << "... removing control inputs " << input.first << " from subgraph.";
VLOG(1) << "Copy quantization range: " << it->first->getName() << " -> " << it->second->getName();
VLOG(1) << "... removing control inputs " << input.first << " from subgraph.";
VLOG(1) << "Found TF GPU " << tf_gpu_id.value() << " at cuda device " << platform_gpu_id.value();
VLOG(2) << "Node " << node->name() << " neither have requested device nor assigned device";
VLOG(1) << "Failed to parse " << (node->requested_device().empty() ? "assigned" : "requested") << " device " << device_name << " of node " << node->name();
VLOG(1) << "Node " << node->name() << " was assigned to a non-GPU device " << device_name;
VLOG(1) << "Adding const node " << input_node->name();
VLOG(1) << "Input edge = " << s;
VLOG(1) << "Output edge = " << s;
VLOG(1) << "Engine Control Input " << input_node->name() << " -> " << info.engine_name;
VLOG(1) << "Engine Input " << input_node->name() << ":" << port << " -> " << info.engine_name << ":" << inputs.size() - 1;
VLOG(1) << "Connecting control edge from " << in->name() << " to " << engine_node->name();
VLOG(1) << "Connecting data edge from " << n->name() << ":" << in.index << " to " << engine_node->name() << ":" << i;
VLOG(1) << "Updating control edge from " << engine_node->name() << " to " << output_node->name();
VLOG(1) << "Updating data edge from " << engine_node->name() << ":" << conn.port_number << " to " << output_node->name() << ":" << port;
VLOG_IS_ON(8))
VLOG(1) << "Assigned " << engine.max_workspace_size_bytes << " bytes to " << engine.engine_name;
VLOG_IS_ON(1))
VLOG(1) << msg;
VLOG(1) << "Input: " << input.type << " " << TensorShape(input.dims).DebugString();
VLOG(1) << "Ignoring oversize dims.";
VLOG(1) << "Input: " << input_tensors.back().DebugString();
VLOG(1) << "Expected: " << expected.DebugString();
VLOG(5) << "Copying node " << n->name();
VLOG(5) << "For merge: " << m->DebugString() << " " << state_map_->CondStateToString(m);
VLOG(5) << "In branch: " << Branch_Name(branch) << " " << NodesToString(stack);
VLOG(3) << "FunctionalizeControlFlow (" << branch_name[branch_index] << "): " << DumpGraphToFile( "functionalize_cond_body_" + branch_name[branch_index], *bodies_[branch_index], nullptr);
VLOG(5) << "Processing forward flow for merge: " << e->DebugString() << " " << state_map_.CondStateToString(src);
VLOG(4) << "Processing forward flow for: " << e->DebugString() << " " << state_map_.CondStateToString(dst);
VLOG(5) << dst->name() << " :: " << state_map_.CondStateToString(dst) << " @ " << state_map_.AncestorStateToString(dst);
VLOG_IS_ON(10))
VLOG_IS_ON(4))
VLOG(3) << "Translating " << params.op_kernel->name();
VLOG(2) << "Found function attr for node " << node.name() << ": " << iter.first << " = " << iter.second.func().name();
VLOG(2) << "node: " << node->name() << " (" << node->id() << ") frame_name: " << cf.frame_name << " frame: " << (cf.frame ? cf.frame->name() : "---") << " parent_frame: " << (cf.parent_frame ? cf.parent_frame->name() : "---");
VLOG(1) << "Setting dynamic binding " << i << " -> " << dynamic_size_param_index;
VLOG(2) << " XLA arg " << i << " shape: " << xla::ShapeUtil::HumanString(arg_shapes[i]) << " name: " << arg.name << " TF arg " << input_to_args->at(i) << " node name: " << arg.node_name << (arg_shardings.find(i) == arg_shardings.end() ? "" : absl::StrCat(" sharding: ", arg_shardings.at(i).DebugString()));
VLOG(2) << " resource: num_gradients: " << arg.tensor_array_gradients.size();
VLOG(2) << "Graph has node " << n->type_string() << ". Corresponding function: " << func.name();
VLOG(4) << "Shape[" << i << "] = " << xla::ShapeUtil::HumanStringWithLayout(shapes.back());
VLOG(2) << "XLA op registration: device: " << backend.first << " op: " << op_name;
VLOG(2) << " Input " << i << " type: " << DataTypeString(ctx->input_type(i)) << " shape: " << ctx->InputShape(i).DebugString();
VLOG(2) << " resource " << resource->name() << " type: " << DataTypeString(arg.type) << " shape: " << arg.ShapeHumanString() << " initialized: " << arg.initialized;
VLOG(2) << "Update shape for argument " << update.input_index << " " << update.shape.DebugString();
VLOG(4) << "TensorArray " << resource->name() << " accessed gradient " << grad_source;
VLOG(2) << "Loop-carried variable: pos: " << update.input_index << " name: " << resource->name() << " modified: " << update.modified << " type: " << DataTypeString(update.type) << " shape: " << update.shape.DebugString();
VLOG(2) << "Resource " << resource->name() << " type: " << DataTypeString(arg.type) << " shape: " << arg.HumanString() << " initialized: " << arg.initialized;
VLOG(2) << "Arg type: " << DataTypeString(arg.type) << " shape: " << arg.HumanString();
VLOG(5) << "TensorArray " << resource->name() << " accessed gradient " << grad_source;
VLOG_IS_ON(2))
VLOG(2) << "If variable: pos: " << update.input_index << " name: " << resource->name() << " modified: " << update.modified << " type: " << DataTypeString(update.type) << " shape: " << update.shape.DebugString();
VLOG(2) << "Resource " << resource->name() << " type: " << DataTypeString(arg.type) << " shape: " << arg.HumanString() << " initialized: " << arg.initialized;
VLOG(2) << "Arg type: " << DataTypeString(arg.type) << " shape: " << arg.HumanString();
VLOG(5) << "TensorArray " << resource->name() << " accessed gradient " << grad_source;
VLOG(2) << "Input shape: " << xla::ShapeUtil::HumanString(branch0_input_shape);
VLOG(2) << "Output shape: " << xla::ShapeUtil::HumanString( branch_results[0].xla_output_shape);
VLOG_IS_ON(2))
VLOG(2) << "Case variable: pos: " << update.input_index << " name: " << resource->name() << " modified: " << update.modified << " type: " << DataTypeString(update.type) << " shape: " << update.shape.DebugString();
VLOG(1) << "Roundtripping: " << it.first;
VLOG(2) << "Cloning small host constant " << input->name();
VLOG(2) << "Deleting node " << node->DebugString();
VLOG(3) << i << ": " << args[i].HumanString();
VLOG(2) << name() << ": " << *arg;
VLOG(4) << "Visiting " << n->name();
VLOG(4) << "Revisiting " << n->name();
VLOG(2) << "Running the optimistic mode on frame " << frame_name << " does not converge because node " << merge->name() << " cannot be mapped into the AndRecurrence form.";
VLOG(2) << "Running the optimistic mode on frame " << frame_name << " does not converge. Seeing different Merge predicates: \" << curr_andrec->ToString() << " and \" << prev_andrec->ToString();
VLOG(2) << tensor_id.ToString() << " -> " << it->second->ToString();
VLOG(4) << "Visiting " << curr_node->name();
VLOG(2) << "Done populating frame " << cur_frame_name << " using the " << (success ? "optimistic" : "pessimistic") << " mode.";
VLOG(1) << "Guaranteed const found: " << src_arg.first->DebugString();
VLOG(2) << "Inlining function " << node->name();
VLOG(3) << "Has ref vars = " << has_ref_vars << ", node: " << node->def().SerializeAsString();
VLOG(2) << "Unsafe edge: " << NodeToString(*g.FindNodeId(incoming_op.first), incoming_op.second) << " -> " << NodeToString(*n, *op_kind);
VLOG(3) << n->name() << " -> " << ResourceOpSetToString(*resource_op_set);
VLOG(3) << "Assigning node " << n->name() << " to cluster " << name;
VLOG(1) << "Hit fuel limit; not marking any remaining ops as clusterable.";
VLOG(4) << "Device type for " << node->name() << ": " << device_type.type_string();
VLOG(2) << "Not clustering " << node->name() << ": disallowed by _XlaCompile attribute";
VLOG(2) << "Rejecting " << node->name() << ": could not find JIT device for " << device_type.type();
VLOG(1) << "Rejecting TF operation " << node->def().op() << " as it is not listed in --tf_xla_ops_to_cluster.";
VLOG(2) << "Isolating " << node->name() << ": must-be-constant stateful op";
VLOG(2) << "Rejecting " << node->name() << ": including it can create dependencies between while loop " "condition and body computations with runtime overhead.";
VLOG(2) << " " << cluster_name << " " << RatioToString(size, graph_->num_nodes());
VLOG(3) << " " << op_count.op() << ": " << op_count.count() << " instances";
VLOG(3) << " " << op_count.op() << ": " << op_count.count() << " instances";
VLOG(4) << " " << edge_info_count_pair.first.GetClusterName() << " " << edge_info_count_pair.first.node_name << " # " << edge_info_count_pair.second;
VLOG(4) << " ** Cluster " << cluster_name;
VLOG(1) << "Shape inference failed for node " << n->name() << ": " << status;
VLOG(4) << "Output " << i << " for node " << n->name() << ": " << context->DebugString(handle);
VLOG(4) << node->name() << " output " << i << " shape" << output.shape.DebugString() << " handle_type " << DataTypeString(output.handle_type) << " handle_shape " << output.handle_shape.DebugString();
VLOG(3) << "Updating " << e->dst()->name();
VLOG(2) << " " << p.first << " -> " << p.second;
VLOG(3) << n->DebugString();
VLOG(2) << "Declustering " << n->name() << " because it is a root shape consumer";
VLOG(4) << "Acquiring lock for variable " << reinterpret_cast<void*>(variable);
VLOG(1) << "Constant output tensor on device";
VLOG(2) << "Retval " << i << " shape " << shape.DebugString() << " type " << DataTypeString(type);
VLOG(4) << "Expanding host graph " << host_func;
VLOG(3) << ndef.DebugString();
VLOG(2) << " " << p.first << " -> " << p.second;
VLOG(2) << "Node " << node.def().ShortDebugString() << " has ref input " << incoming_node->name() << " " << incoming_node->type_string();
VLOG(1) << "Cycle detected when adding " << src_type << "->" << dst_type << " edge: " << DescribeCycle(cycles, *graph, src, dst);
VLOG(4) << "Oc -> oc edge: " << e->DebugString();
VLOG(0) << "Iter " << iter << " of " << n;
VLOG_IS_ON(3))
VLOG(1) << "adding node " << new_node;
VLOG(1) << "removing node " << node;
VLOG(1) << "removing edge " << from << " " << to;
VLOG(3) << "Graph expansion";
VLOG(1) << "adding node " << new_node;
VLOG(1) << "removing node " << node;
VLOG(2) << "Allocated buffer at " << index_to_buffer.second.opaque() << " index " << index_to_buffer.first.ToString() << " (" << size << " bytes)";
VLOG(2) << "Released allocation handle " << input_coords[i].handle;
VLOG(2) << "Released computation handle " << key;
VLOG(1) << GetActionSummary("promotes", params, config);
VLOG(1) << GetActionSummary("promotes", params, config);
VLOG(1) << "Failed to seek to the record for tensor " << name << ", slice " << slice_s.DebugString() << ": computed key = " << key;
VLOG(1) << "Failed to parse the record for tensor " << name << ", slice " << slice_s.DebugString() << ": computed key = " << key;
VLOG(1) << "The sequence length is either zero or shorter than the " "target output (CTC works only with shorter target sequence " "than input sequence). You can turn this into a warning by " "using the flag ignore_longer_outputs_than_inputs - " << b << ": " << str_util::Join(labels[b], " ");
VLOG(2) << "label for batch: " << b << ": " << str_util::Join(label, " ");
VLOG(1) << "num_pending_=" << num_pending_ << " cap=" << cap;
VLOG(1) << "Unable to unprotect tensor: " << s;
VLOG(1) << "Unable to unprotect tensor: " << s;
VLOG(2) << "Failed to run optimizer " << stage->optimizer_name() << ", stage " << stage->stage_name() << " node " << node->name() << ". Error: " << stage_status.error_message();
VLOG_IS_ON(1))
VLOG(1) << " " << t;
VLOG(1) << "Popped " << op;
VLOG(1) << "Tensor " << id << " not used";
VLOG(1) << "Tensor " << id << " usage count " << usage_count_it->second;
VLOG(1) << "Tensor " << id << " has no associated op. Deleting gradient";
VLOG(1) << "Tensor " << id << " is source";
VLOG(1) << "Op " << op_id << " missing " << missing_it->second << " output gradients";
VLOG(3) << "changed_this_iteration: " << changed_this_iteration;
VLOG(3) << "changed_this_iteration: " << changed_this_iteration;
VLOG(3) << "Traversing unreachable root: " << root->ToString();
VLOG(3) << "Visiting ordered: " << instruction->ToString();
VLOG(2) << "Released allocation handle " << key;
