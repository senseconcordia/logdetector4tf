
/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/tools/benchmark/benchmark_model.cc
LOG(INFO) << "Initializing graph variables";
LOG(FATAL) << "Initialization values are not supported for strings";
LOG(FATAL) << "Unsupported input type: " << DataTypeString(input.data_type);
LOG(INFO) << "Loading TensorFlow.";
LOG(INFO) << "Got config, " << config.device_count_size() << " devices";
LOG(ERROR) << "Could not create TensorFlow Graph: " << s;
LOG(ERROR) << "Could not create TensorFlow Session: " << s;
LOG(ERROR) << "Error during inference: " << s;
LOG(INFO) << "Running benchmark for max " << num_runs << " iterations, max " << max_time_s << " seconds " << (stats != nullptr ? "with" : "without") << " detailed stat logging, with " << sleep_seconds << "s sleep between inferences";
LOG(INFO) << "Failed on run " << i;
LOG(INFO) << stream.str() << std::endl;
LOG(ERROR) << usage;
LOG(ERROR) << "There must be the same number of items in --input_layer," << " --input_layer_shape, and --input_layer_type, for example" << " --input_layer=input1,input2 --input_layer_type=float,float " << " --input_layer_shape=1,224,224,4:1,20";
LOG(ERROR) << "--input_layer=" << input_layer_string << " (" << input_layers.size() << " items)";
LOG(ERROR) << "--input_layer_type=" << input_layer_type_string << " (" << input_layer_types.size() << " items)";
LOG(ERROR) << "--input_layer_shape=" << input_layer_shape_string << " (" << input_layer_shapes.size() << " items)";
LOG(ERROR) << "Unknown argument " << argv[1] << "" << usage;
LOG(INFO) << "Graph: [" << graph << "]";
LOG(INFO) << "Init ops:" << init_ops_string;
LOG(INFO) << "Input layers: [" << input_layer_string << "]";
LOG(INFO) << "Input shapes: [" << input_layer_shape_string << "]";
LOG(INFO) << "Input types: [" << input_layer_type_string << "]";
LOG(INFO) << "Output layers: [" << output_layer_string << "]";
LOG(INFO) << "Target layers: [" << target_layer_string << "]";
LOG(INFO) << "Num runs: [" << max_num_runs << "]";
LOG(INFO) << "Inter-inference delay (seconds): [" << inference_delay << "]";
LOG(INFO) << "Inter-benchmark delay (seconds): [" << inter_benchmark_delay << "]";
LOG(INFO) << "Num threads: [" << num_threads << "]";
LOG(INFO) << "Benchmark name: [" << benchmark_name << "]";
LOG(INFO) << "Output prefix: [" << output_prefix << "]";
LOG(INFO) << "Show sizes: [" << show_sizes << "]";
LOG(INFO) << "Warmup runs: [" << warmup_runs << "]";
LOG(INFO) << "Initialized session in " << initialization_time_s << "s";
LOG(ERROR) << "Graph variables initialization failed with " << initialize_variables_status;
LOG(ERROR) << "Any unknown sizes in the shapes (-1's) must be replaced" << " with the size you want to benchmark with.";
LOG(ERROR) << "Timing failed with " << warmup_time_status;
LOG(ERROR) << "Timing failed with " << no_stat_time_status;
LOG(ERROR) << "Timing failed with " << stat_time_status;
LOG(INFO) << "Average inference timings in us: " << "Warmup: " << (warmup_runs > 0 ? warmup_time_us / warmup_runs : 0) << ", " << "no stats: " << no_stat_time_us / no_stat_num_runs << ", " << "with stats: " << stat_time_us / stat_num_runs;
LOG(ERROR) << "FLOPs calculation failed with " << flop_status;
LOG(INFO) << "FLOPs estimate: " << strings::HumanReadableNum(total_flops);
LOG(INFO) << "FLOPs/second: " << strings::HumanReadableNum( static_cast<int64>(total_flops / mean_run_time));
LOG(INFO) << "Outputting: [" << time.first << "]";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/tools/proto_text/gen_proto_text_functions.cc
LOG(FATAL) << "Unexpected error at " << filename << "@" << line << ":" << column << " - " << message;
LOG(ERROR) << "Pass output path, relative path, and at least proto file";
LOG(ERROR) << kPlaceholderFile << " must be passed";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/tools/proto_text/gen_proto_text_functions_lib.cc
LOG(FATAL) << "handled earlier";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/tools/graph_transforms/quantize_nodes.cc
LOG(ERROR) << "input_types has incorrect size " << input_types.size() << " <= " << i << ". Assuming everything else is floats.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/tools/graph_transforms/freeze_requantization_ranges.cc
LOG(WARNING) << "Node from log not found in graph: " << record.name;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/tools/graph_transforms/remove_nodes.cc
LOG(INFO) << "Skipping replacement for " << replace_node.name();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/tools/graph_transforms/transform_graph.cc
LOG(ERROR) << usage;
LOG(ERROR) << "Unknown argument " << argv[1] << "." << usage;
LOG(ERROR) << "in_graph graph can't be empty." << usage;
LOG(ERROR) << "out_graph graph can't be empty." << usage;
LOG(ERROR) << "You must specify at least one transform." << usage;
LOG(ERROR) << "Failed to parse --transform argument, error was " << parse_status.error_message();
LOG(ERROR) << "You must specify at least one transform." << usage;
LOG(ERROR) << "Loading graph '" << in_graph_string << "' failed with " << load_status.error_message();
LOG(ERROR) << usage;
LOG(ERROR) << transform_result.error_message();
LOG(ERROR) << usage;
LOG(ERROR) << "Saving graph '" << out_graph_string << "' failed with " << save_status.error_message();
LOG(INFO) << "Applying " << transform_name;
LOG(ERROR) << transform_name << ": Ignoring error " << transform_result.error_message();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/tools/graph_transforms/fold_constants_lib.cc
LOG(ERROR) << "Bad graph structure, no node named '" << node_name << "' found for input lookup";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/tools/graph_transforms/summarize_graph_main.cc
LOG(WARNING) << "Decoding Tensor failed for node" << node.name();
LOG(WARNING) << "Decoding Tensor failed for node" << node.name();
LOG(ERROR) << usage;
LOG(ERROR) << "Unknown argument " << argv[1] << "." << usage;
LOG(ERROR) << "in_graph graph can't be empty." << usage;
LOG(ERROR) << "Loading graph '" << in_graph << "' failed with " << load_status.error_message();
LOG(ERROR) << usage;
LOG(ERROR) << summarize_result.error_message() << "" << usage;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/tools/graph_transforms/compare_graphs.cc
LOG(ERROR) << "compare_graphs expects two file names as arguments";
LOG(ERROR) << "Loading graph '" << argv[1] << "' failed with " << a_load_status.error_message();
LOG(ERROR) << "Loading graph '" << argv[2] << "' failed with " << b_load_status.error_message();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/tools/graph_transforms/transform_utils.cc
LOG(WARNING) << "IN " << __func__ << (num_nodes - processed) << " NODES IN A CYCLE";
LOG(WARNING) << "PENDING: " << SummarizeNodeDef(input_graph_def.node(i)) << "WITH PENDING COUNT = " << pending_count[i];
VLOG(1) << "Looking at node " << node.DebugString();
VLOG(1) << "pattern=" << pattern.DebugString();
VLOG(1) << "match=" << match->DebugString();
VLOG(1) << "node " << node.name() << " has been previously matched";
VLOG(1) << "node.op() != pattern.op()";
VLOG(1) << "non_control_inputs.size() != pattern.inputs.size()";
LOG(WARNING) << "Expected " << expected_output << " to be preserved.";
LOG(WARNING) << "Generator function didn't preserve needed nodes, " << "copying old replacements back in instead.";
LOG(ERROR) << "Invalid input " << invalid_input.second << " for node " << invalid_input.first << " - " << node_map[invalid_input.first]->DebugString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/decode_bmp_op.cc
LOG(FATAL) << "Unexpected number of channels: " << channels;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/save_restore_tensor.cc
VLOG(1) << "About to save tensors to file " << filename_t.flat<tstring>()(0) << "...";
VLOG(1) << "Restoring tensor " << idx << " : " << tensor_name << " : " << restored_full_shape.num_elements();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/conv_ops_using_gemm.cc
LOG(WARNING) << "Conv2D was called with bad input dimensions: " << input_batches << ", " << input_height << ", " << input_width << ", " << input_depth;
LOG(WARNING) << "Conv2D was called with bad filter dimensions: " << filter_width << ", " << filter_height << ", " << filter_count;
LOG(WARNING) << "Conv2D was called with bad output width or height: " << output_width << ", " << output_height;
VLOG(2) << "Conv2D: in_depth = " << in_depth << ", input_cols = " << input_cols << ", filter_cols = " << filter_cols << ", input_rows = " << input_rows << ", filter_rows = " << filter_rows << ", stride_rows = " << stride_rows << ", stride_cols = " << stride_cols << ", out_depth = " << out_depth;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/reduction_ops_common.cc
VLOG(1) << "data reshape: " << absl::StrJoin(data_reshape_, ",");
VLOG(1) << "out reshape: " << absl::StrJoin(out_reshape_, ",");
VLOG(1) << "out shape: " << absl::StrJoin(out_shape_, ",");

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/spectrogram.cc
LOG(ERROR) << "Window length too short.";
LOG(ERROR) << "Step length must be positive.";
LOG(ERROR) << "ComputeComplexSpectrogram() called before successful call " << "to Initialize().";
LOG(ERROR) << "ComputeSquaredMagnitudeSpectrogram() called before " << "successful call to Initialize().";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/collective_nccl_gatherer.cc
VLOG(1) << "NcclGatherer calling NcclManager::AddToAllGather num_tasks " << col_params_->group.num_tasks << " current task " << col_params_->instance.task_names[col_params_->default_rank] << " num local devices " << num_local_devices << " num global devices " << num_global_devices << " rank " << col_params_->default_rank << " device " << col_ctx_->device_name << " instance " << col_params_->instance.instance_key;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/mfcc.cc
LOG(ERROR) << "Mfcc not initialized.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/save_restore_v2_ops.cc
VLOG(1) << "BundleWriter, prefix_string: " << prefix_string;
VLOG(1) << status;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/rocm_sparse.cc
LOG(INFO) << "Creating GpuSparse handles for stream " << gpu_stream_;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/image_ops.cc
LOG(ERROR) << "Invalid interpolation " << interpolation_str << ". Supported types: NEAREST, BILINEAR";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/rocm_solvers.cc
LOG(INFO) << "Creating ROCmSolver handles for stream " << hip_stream_;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/queue_base.cc
VLOG(1) << "Skipping cancelled enqueue attempt";
LOG(WARNING) << name_ << ": Skipping cancelled enqueue attempt with queue not closed";
VLOG(1) << "Skipping cancelled dequeue attempt";
LOG(WARNING) << name_ << ": Skipping cancelled dequeue attempt with queue not closed";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/depthwise_conv_grad_op.cc
VLOG(2) << "DepthwiseConv2dNativeBackpropInput: " << " Input: [" << batch << ", " << input_rows << ", " << input_cols << ", " << in_depth << "]; Filter: [" << filter_rows << ", " << filter_cols << ", " << in_depth << ", " << depth_multiplier << "]; Output: [" << batch << ", " << out_rows << ", " << out_cols << ", " << out_depth << "], stride = " << stride_ << ", pad_rows = " << pad_rows << ", pad_cols = " << pad_cols << ", Use cuDNN: " << use_cudnn;
LOG(ERROR) << "Only half, float, and double are supported.";
VLOG(2) << "DepthwiseConv2dNativeBackpropFilter: " << " Input: [" << batch << ", " << input_rows << ", " << input_cols << ", " << in_depth << "]; Filter: [" << filter_rows << ", " << filter_cols << ", " << in_depth << ", " << depth_multiplier << "]; Output: [" << batch << ", " << out_rows << ", " << out_cols << ", " << out_depth << "], stride = " << stride_ << ", pad_rows = " << pad_rows << ", pad_cols = " << pad_cols << ", Use cuDNN: " << use_cudnn;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/sparse_matmul_op.cc
LOG(FATAL) << "Unsupported type";
LOG(FATAL) << "Have 0 threads in thread pool";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/mfcc_dct.cc
LOG(ERROR) << "Coefficient count must be positive.";
LOG(ERROR) << "Input length must be positive.";
LOG(ERROR) << "Coefficient count must be less than or equal to " << "input length.";
LOG(ERROR) << "DCT not initialized.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/split_op.cc
VLOG(1) << "Split identity";
VLOG(1) << "Slice dim 0: " << input_shape.DebugString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/conv_grad_input_ops.cc
VLOG(2) << "Conv2DBackpropInput:" << " input: " << input_shape.DebugString() << " filter:" << filter.shape().DebugString() << " out_backprop: " << out_backprop.shape().DebugString() << " strides: [" << stride_rows << ", " << stride_cols << "]" << " dilations: [" << dilation_rows << ", " << dilation_cols << "]";
VLOG(3) << "Compute Conv2DBackpropInput with cuDNN:" << " data_format=" << ToString(data_format) << " compute_data_format=" << ToString(compute_data_format);
VLOG(4) << "Transform filter tensor from " << ToString(FORMAT_HWIO) << " to " << ToString(dst_format);
VLOG(4) << "Convert the `out_backprop` tensor from NHWC to NCHW.";
VLOG(4) << "Convert the output tensor back from NCHW to NHWC.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/depthwise_conv_op.cc
VLOG(2) << "DepthwiseConv2dNative: " << " Input: [" << batch << ", " << input_rows << ", " << input_cols << ", " << in_depth << "]; Filter: [" << filter_rows << ", " << filter_cols << ", " << in_depth << ", " << depth_multiplier << "]; Output: [" << batch << ", " << out_rows << ", " << out_cols << ", " << out_depth << "], stride = " << stride_ << ", pad_rows = " << pad_rows << ", pad_cols = " << pad_cols << ", Use cuDNN: " << use_cudnn;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/mutex_ops.cc
VLOG(2) << "Creating mutex with name " << name << ": " << this;
VLOG(3) << "Destroying LockReleaser " << this << " for mutex: " << mutex_;
VLOG(3) << "Destroying LockReleaser " << this << ": sent notifications.";
VLOG(2) << "Finished locking mutex " << mutex << " with lock: " << lock.shared_ptr.get() << " status: " << s.ToString();
VLOG(2) << "Executing ConsumeMutexLockOp";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/partitioned_function_ops.cc
LOG(INFO) << "Ignoring error while destructing PartitionedCallOp: " << status.ToString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/bias_op.cc
VLOG(1) << "BiasAddGrad " << bias_parameters.ToString() << " Native algo latency: " << elapsed_microseconds;
VLOG(1) << "BiasAddGrad " << bias_parameters.ToString() << " Reduction algo latency: " << elapsed_microseconds;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/load_and_remap_matrix_op.cc
LOG(INFO) << "Processing checkpoint : " << ckpt_path;
LOG(INFO) << "Loading slice " << tensor_slice.DebugString();
LOG(INFO) << "Processing old row " << row_index;
LOG(INFO) << "Copied " << rows_copied << " rows from old matrix (with " << tensor_shape.dim_size(0) << " rows) to new matrix (with " << num_rows_ << " rows).";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/logging_ops.cc
LOG(INFO) << ended_msg << std::flush;
LOG(WARNING) << ended_msg << std::flush;
LOG(ERROR) << ended_msg << std::flush;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/scoped_allocator_ops.cc
VLOG(1) << "_ScopedAllocatorOp " << context->op_kernel().name() << " new backing tensor size " << backing_tensor->TotalBytes() << " num_elements_ " << num_elements_ << " buffer " << DMAHelper::buffer(backing_tensor) << " base addr " << DMAHelper::base(backing_tensor);
VLOG(1) << "_ScopedAllocatorConcatOp outputting backing tensor at " << backing_buf;
VLOG(1) << "_ScopedAllocatorSplitOp assigning input " << i << " to output " << i - 1 << " buf addr " << DMAHelper::base(&context->input(i));

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/parameterized_truncated_normal_op.cc
LOG(ERROR) << "TruncatedNormal randn rejection sampler " << "exceeded maximum iterations for " << "normMin=" << normMin << " normMax=" << normMax << " kMaxIterations=" << kMaxIterations;
LOG(ERROR) << "TruncatedNormal uniform rejection sampler " << "exceeded max iterations. Sample may contain " << "outliers.";
LOG(ERROR) << "TruncatedNormal exponential distribution " << "rejection sampler exceeds max iterations. " << "Sample may contain outliers.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/collective_nccl_broadcaster.cc
VLOG(1) << "NcclBroadcast calling NcclManager::AddBroadcastSend/Recv num_tasks " << col_params_->group.num_tasks << " current task " << col_params_->instance.task_names[col_params_->default_rank] << " num local devices " << num_local_devices << " num global devices " << num_global_devices << " rank " << col_params_->default_rank << " device " << col_ctx_->device_name << " instance " << col_params_->instance.instance_key << " source " << col_params_->is_source;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/mkl_slice_op.cc
VLOG(1) << "Slice identity";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/quantized_conv_ops.cc
LOG(WARNING) << "For kernel '" << context->op_kernel().name() << "' from input '" << context->op_kernel().requested_input(0) << "': Zero is not representable in the quantized range used by the" << " input. This means QuantizedConv2d has to fall back to a slow" << " implementation, since the border of zero values can't be" << " represented easily. You should try to construct graphs that" << " avoid this situation.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/remote_fused_graph_rewriter_transform.cc
VLOG(2) << "Border Input(" << i << "): " << border_inputs.at(i);
VLOG(2) << "Border Output(" << i << "): " << border_outputs.at(i);
LOG(FATAL) << "Fuse targets are not specified.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/remote_fused_graph_execute_utils.cc
LOG(FATAL) << "Unsupported input type: " << data_type;
LOG(ERROR) << "Error during inference: " << status;
VLOG(1) << "Failed to dryrun " << status;
VLOG(1) << "Shape inference failed for node: " << node->name();
VLOG(1) << DumpCluster(ci);
VLOG(1) << DumpGraphDef(*subgraph_def);
LOG(INFO) << "Transforming quantized stripped model to a remote fused " "graph execute op by fusing a specified subgraph...";
LOG(INFO) << "Removing existing edge to " << edge->dst()->name() << " from " << edge->src()->name();
LOG(INFO) << "As graph output and subgraph output are same, " << "the graph output node is replaced by identity node";
VLOG(2) << "Graph input: " << name;
LOG(INFO) << "Executor for " << remote_graph_executor_name << " not registered. Do not fuse.";
VLOG(2) << "Graph output: " << name;
VLOG(2) << "Fused node: " << name;
VLOG(2) << "Border input: " << name;
VLOG(2) << "Border output: " << name;
LOG(FATAL);
VLOG(2) << "Input matched!";
LOG(FATAL) << "type " << tensor->dtype() << " is not supported.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/meta_support.cc
LOG(FATAL) << "QuantizedGemm: Meta fastpath not supported.";
LOG(FATAL) << "Requantize: Meta fastpath not supported.";
LOG(FATAL) << "Dequantize: Meta fastpath not supported.";
LOG(FATAL) << "Quantize: Meta fastpath not supported.";
LOG(FATAL) << "QuantizedBiasAdd: Meta fastpath not supported.";
LOG(FATAL) << "Clamp: Meta fastpath not supported.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/transpose_functor_cpu.cc
LOG(FATAL) << "Unsupported TransposeUsingEigen for: " << in.dims();
LOG(FATAL) << "DT_STRING not supported on SYCL device.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/fused_batch_norm_op.cc
VLOG(2) << "FusedBatchNorm:" << " batch_size: " << batch_size << " channels: " << channels << " height: " << height << " width:" << width << " x shape: " << x.shape().DebugString() << " scale shape: " << scale.shape().DebugString() << " offset shape: " << offset.shape().DebugString() << " activation mode: " << ToString(activation_mode) << " tensor format: " << ToString(tensor_format) << " compute format: " << ToString(compute_format);
VLOG(2) << "FusedBatchNormGrad:" << " batch_size: " << batch_size << " channels: " << channels << " height: " << height << " width: " << width << " y_backprop shape: " << y_backprop.shape().DebugString() << " x shape: " << x.shape().DebugString() << " scale shape: " << scale.shape().DebugString() << " tensor format: " << ToString(tensor_format) << " compute format: " << ToString(compute_format);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/fft_ops.cc
LOG(WARNING) << "Invalid value for env-var " << envvar_in_mb << ": " << workspace_limit_in_mb_str;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/check_numerics_op.cc
LOG(ERROR) << "abnormal_detected_host @" << abnormality_indicators.data() << " = {" << is_nan << ", " << is_inf << "} " << message_;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/gpu_utils.cc
LOG(WARNING) << "Failed to allocate memory for convolution redzone " << "checking; skipping this check. This is benign and only " << "means that we won't check cudnn for out-of-bounds reads " << "and writes. This message will only be printed once.";
LOG(WARNING) << "Failed to check cudnn convolutions for out-of-bounds " << "reads and writes with an error message: '" << rz_status.status().error_message() << "'; skipping this check. This only means that we won't " << "check cudnn for out-of-bounds reads and writes. This " << "message will only be printed once.";
LOG(ERROR) << "Detected cudnn out-of-bounds write in convolution buffer! This is " "likely a cudnn bug. We will skip this algorithm in the future, but " "your GPU state may already be corrupted, leading to incorrect " "results. Within Google, no action is needed on your part. Outside " "of Google, please ensure you're running the latest version of " "cudnn. If that doesn't fix the problem, please file a bug with " "this full error message and we'll contact nvidia.";
LOG(ERROR) << rz_check_status.RedzoneFailureMsg();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/quantized_add_op.cc
LOG(INFO) << "ndims=" << ndims;
LOG(INFO) << "bcast.x_reshape()=" << TensorShape(bcast.x_reshape()).DebugString();
LOG(INFO) << "bcast.y_reshape()=" << TensorShape(bcast.y_reshape()).DebugString();
LOG(INFO) << "bcast.x_bcast()=" << TensorShape(bcast.x_bcast()).DebugString();
LOG(INFO) << "bcast.y_bcast()=" << TensorShape(bcast.y_bcast()).DebugString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/decode_proto_op.cc
LOG(WARNING) << "Proto counting error for message type " << message_type_ << ": " << st;
LOG(WARNING) << "Proto counting error for message type " << message_type_ << ": " << st;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/quantized_instance_norm.cc
VLOG(2) << "Calling optimized";
VLOG(2) << "Calling unoptimized";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/maxpooling_op.cc
LOG(FATAL) << "MaxPool currently only supports the following (layout, " "type) combinations: (NHWC, non-qint8), " "(NCHW, non-qint8) or (NCHW_VECT_C, qint8). The " "requested combination (" << ToString(data_format_) << ", " << DataTypeString(DataTypeToEnum<T>::v()) << ") is not supported.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/conditional_accumulator_base.cc
LOG(WARNING) << "Attempt to set current_global_step_ to smaller value: " << "current_global_step_ = " << current_global_step_ << " >= " << new_global_step << " = new_global_step.";
VLOG(1) << "Skipping cancelled TakeGrad attempt";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/non_max_suppression_op.cu.cc
VLOG(1) << "Number of boxes above score threshold " << score_threshold << " is 0";
VLOG(2) << "Number of boxes above threshold=" << score_threshold << " is " << limited_num_boxes;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/mkl_concat_op.cc
VLOG(1) << "mkl_common_format == MEMORY_FORMAT::blocked";
VLOG(1) << "mkl_common_format == MEMORY_FORMAT::blocked";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/collective_nccl_reducer.cc
VLOG(1) << "NcclReducer calling NcclManager::AddToAllReduce num_tasks " << col_params_->group.num_tasks << " current task " << col_params_->instance.task_names[col_params_->default_rank] << " num local devices " << num_local_devices << " num global devices " << num_global_devices << " device " << col_ctx_->device_name << " instance " << col_params_->instance.instance_key;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/draw_bounding_box_op.cc
LOG(WARNING) << "Bounding box (" << min_box_row << "," << min_box_col << "," << max_box_row << "," << max_box_col << ") is inverted and will not be drawn.";
LOG(WARNING) << "Bounding box (" << min_box_row << "," << min_box_col << "," << max_box_row << "," << max_box_col << ") is completely outside the image" << " and will not be drawn.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/mkl_matmul_op.cc
VLOG(2) << "MKL DNN SGEMM called";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/conv_grad_ops_3d.cc
VLOG(2) << "Fallback on Eigen implementation of Conv3DBackpropInputOp: " "col_buffer_overhead=" << col_buffer_overhead;
VLOG(2) << "Fallback on Eigen implementation of Conv3DBackpropFilterOp: " "col_buffer_overhead=" << col_buffer_overhead;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/decode_image_op.cc
LOG(FATAL) << "Should never get here after check above";
VLOG(1) << status;
VLOG(1) << status;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/conv_ops.cc
VLOG(2) << "Conv2D: in_depth = " << dimensions.in_depth << ", patch_depth = " << dimensions.patch_depth << ", input_cols = " << dimensions.input_cols << ", filter_cols = " << dimensions.filter_cols << ", input_rows = " << dimensions.input_rows << ", filter_rows = " << dimensions.filter_rows << ", stride_rows = " << dimensions.stride_rows << ", stride_cols = " << dimensions.stride_cols << ", dilation_rows = " << dimensions.dilation_rows << ", dilation_cols = " << dimensions.dilation_cols << ", out_depth = " << dimensions.out_depth;
LOG(WARNING) << "Invalid value for env-var " << envvar_in_mb << ": " << workspace_limit_in_mb_str;
VLOG(3) << "Compute Conv2D with cuDNN:" << " data_format=" << ToString(data_format) << " compute_data_format=" << ToString(compute_data_format);
VLOG(4) << "Pad input tensor:" << " padding_top=" << padding_top << " padding_bottom=" << padding_bottom << " padding_left=" << padding_left << " padding_right=" << padding_right;
VLOG(4) << "Convert the input tensor from NHWC to NCHW.";
VLOG(4) << "Transform filter tensor from " << ToString(FORMAT_HWIO) << " to " << ToString(dst_format);
VLOG(4) << "Allocate temporary memory for output in compute data format";
VLOG(4) << "Convolution Algorithm: " << algorithm_config.algorithm()->algo_id();
VLOG(4) << "tensor_ops_enabled: " << algorithm_config.algorithm()->tensor_ops_enabled();
VLOG(4) << "Convert the output tensor back from NCHW to NHWC.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/sparse_reduce_op.cc
VLOG(2) << "coords: " << absl::StrJoin(g.group(), ",") << "; idx: " << idx << "; group " << Op::Name() << ": " << reduced_val();
VLOG(2) << "coords: " << absl::StrJoin(g.group(), ",") << "; group " << Op::Name() << ": " << reduced_val();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/function_ops.cc
VLOG(1) << "Instantiating " << func_name << " on " << target_device;
VLOG(1) << "Instantiated " << func_name << " on " << target_device << ", resulting in handle: " << handle << " flr: " << lib;
VLOG(1) << "Running " << func_name << " on " << target_device << " with handle: " << handle;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/spacetodepth_op_gpu.cu.cc
LOG(FATAL) << "5-D tensors should not be used with NHWC format";
LOG(FATAL) << "5-D tensors should not be used with NCHW format";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/immutable_constant_op.cc
LOG(ERROR) << "Deallocating not allocated region for readonly memory region";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/collective_ops.cc
VLOG(1) << "CollectiveOpKernel CompleteParams for collective " << col_params_.name << " device " << c->device()->name() << " group " << col_params_.group.group_key << " instance " << col_params_.instance.instance_key;
VLOG(1) << "CollectiveGatherOpKernel ExecuteAsync done for collective " << c->op_kernel().name() << " device " << c->device()->name() << " group " << group_key << " instance " << instance_key << " status " << s;
VLOG(1) << "CollectiveGatherOpKernel ExecuteAsync start for collective " << col_params_.name << " device " << c->device()->name() << " group " << col_params_.group.group_key << " instance " << col_params_.instance.instance_key;
VLOG(2) << "CollectiveReduce instance " << col_params_.instance.instance_key << " merge_op " << merge_op_name << " final_op " << final_op_name << " communication_hint " << col_params_.instance.impl_details.communication_hint;
VLOG(1) << "CollectiveReduceOpKernel ExecuteAsync done for collective " << c->op_kernel().name() << " device " << c->device()->name() << " group " << group_key << " instance " << instance_key << " status " << s;
VLOG(1) << "CollectiveReduceOpKernel ExecuteAsync start for collective " << col_params_.name << " device " << c->device()->name() << " group " << col_params_.group.group_key << " instance " << col_params_.instance.instance_key;
VLOG(1) << "CollectiveBcastSendOpKernel ExecuteAsync done for collective " << c->op_kernel().name() << " device " << c->device()->name() << " group " << group_key << " instance " << instance_key << " status " << s;
VLOG(1) << "CollectiveBcastSendOpKernel ExecuteAsync start for collective " << col_params_.name << " device " << c->device()->name() << " group " << col_params_.group.group_key << " instance " << col_params_.instance.instance_key;
VLOG(1) << "CollectiveBcastRecvOpKernel ExecuteAsync done for collective " << c->op_kernel().name() << " device " << c->device()->name() << " group " << group_key << " instance_key " << instance_key << " status " << s;
VLOG(1) << "CollectiveBcastRecvOpKernel ExecuteAsync start for collective " << col_params_.name << " device " << c->device()->name() << " group " << col_params_.group.group_key << " instance " << col_params_.instance.instance_key;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/cuda_solvers.cc
LOG(INFO) << "Creating CudaSolver handles for stream " << cuda_stream_;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/quantized_mul_op.cc
LOG(INFO) << "ndims=" << ndims;
LOG(INFO) << "bcast.x_reshape()=" << TensorShape(bcast.x_reshape()).DebugString();
LOG(INFO) << "bcast.y_reshape()=" << TensorShape(bcast.y_reshape()).DebugString();
LOG(INFO) << "bcast.x_bcast()=" << TensorShape(bcast.x_bcast()).DebugString();
LOG(INFO) << "bcast.y_bcast()=" << TensorShape(bcast.y_bcast()).DebugString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/conv_grad_filter_ops.cc
VLOG(2) << "Conv2DBackpropFilter:" << " input: " << input.shape().DebugString() << " filter:" << filter_shape.DebugString() << " out_backprop: " << out_backprop.shape().DebugString() << " strides: [" << stride_rows << ", " << stride_cols << "]" << " dilations: [" << dilation_rows << ", " << dilation_cols << "]";
VLOG(3) << "Compute Conv2DBackpropFilter with cuDNN:" << " data_format=" << ToString(data_format) << " compute_data_format=" << ToString(compute_data_format);
VLOG(4) << "Convert the `out_backprop` tensor from NHWC to NCHW.";
VLOG(4) << "Convert the `input` tensor from NHWC to NCHW.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/variable_ops.cc
VLOG(3) << "TmpVar " << name << " deleted";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/control_flow_ops.cc
LOG(FATAL) << "Abort_op intentional failure; " << error_msg_;
LOG(WARNING) << "Exiting the process: " << error_msg_;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/mkl_input_conversion_op.cc
VLOG(1) << "MklInputConversionOp: Input shapes are: " << context->input(kInputIndex_0).shape().DebugString() << " and " << context->input(kInputIndex_1).shape().DebugString();
VLOG(1) << "MklInputConversionOp: No conversion needed, " << "copying TF inputs to output";
VLOG(1) << "MklInputConversionOp: No conversion needed, " << "copying MKL inputs with identical shapes to output";
VLOG(1) << "MklInputConversionOp: Shape is same, but format is " "different, " << "need to convert to same format";
VLOG(1) << "MklInputConversionOp: Broadcast needed, " << "converted MKL inputs to TF format";
VLOG(1) << "MklInputConversionOp: Inputs in different formats (MKL/TF)";
VLOG(1) << "MklInputConversionOp: No broadcast needed.";
VLOG(1) << "MklInputConversionOp: Converting input " << tf_tensor_index << " to MKL format";
VLOG(1) << "MklInputConversionOp: Broadcast needed.";
VLOG(1) << "MklInputConversionOp: Converting input " << mkl_tensor_index << " to TF format";
VLOG(1) << "MklInputConversionOp: Shapes (output): " << context->mutable_output(kInputIndex_0)->shape().DebugString() << " and " << context->mutable_output(kInputIndex_1)->shape().DebugString();
VLOG(1) << "MklInputConversion completed successfully.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/cuda_sparse.cc
LOG(INFO) << "Creating CudaSparse handles for stream " << gpu_stream_;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/remote_fused_graph_execute_op.cc
LOG(ERROR) << "Executor not found for " << execute_info_.executor_name();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/word2vec_kernels.cc
LOG(INFO) << "Data file: " << filename << " contains " << data.size() << " bytes, " << corpus_size_ << " words, " << word_freq.size() << " unique words, " << ordered.size() << " unique frequent words.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/depthtospace_op_gpu.cu.cc
LOG(FATAL) << "5-D tensors should not be used with NHWC format";
LOG(FATAL) << "5-D tensors should not be used with NCHW format";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/strided_slice_op.cc
VLOG(1) << "Strided slice identity ";
VLOG(1) << "Strided slice dim 0: " << input.shape().DebugString();
LOG(FATAL) << "shape must have type int32 or int64.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/cudnn_rnn_ops.cc
LOG(WARNING) << "No Rnn algorithm found";
VLOG(1) << "Using existing best Cudnn RNN algorithm " << "(algo, tensor_op_enabled) = (" << algo_config->algorithm()->algo_id() << ", " << algo_config->algorithm()->tensor_ops_enabled() << ").";
VLOG(1) << "Profile Cudnn RNN algorithm (algo, tensor_op_enabled) = (" << algo.algo_id() << ", " << algo.tensor_ops_enabled() << ").";
VLOG(1) << "Cudnn RNN algorithm (algo, tensor_op_enabled) = (" << algo.algo_id() << ", " << algo.tensor_ops_enabled() << ")" << " run time: " << total_time << " ms.";
VLOG(1) << "Best Cudnn RNN algorithm (algo, tensor_op_enabled) = (" << best_result.algorithm().algo_id() << ", " << best_result.algorithm().tensor_ops_enabled() << ").";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/lookup_util.cc
LOG(WARNING) << "Truncated " << filename_ << " before its end at " << vocab_size_ << " records.";
LOG(WARNING) << "next_id_ : " << next_id_;
LOG(WARNING) << "Unable to get line count: " << status;
LOG(INFO) << "Table trying to initialize from file " << filename << " is already initialized.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/deep_conv2d.cc
VLOG(2) << "CanUseDeepConv2D" << " deep_conv_cost: " << deep_conv_cost << " direct_conv_cost: " << direct_conv_cost << " deep_direct_ratio: " << (static_cast<float>(deep_conv_cost) / static_cast<float>(direct_conv_cost)) << " use_deep_conv: " << (deep_conv_cost < direct_conv_cost);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/sendrecv_ops.cc
VLOG(2) << "Send " << parsed_key_.buf_;
VLOG(2) << "Send " << in_loop_parsed.buf_;
VLOG(2) << "Recv " << parsed_key_.buf_;
VLOG(2) << "Recv " << in_loop_parsed.buf_;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/generate_box_proposals_op.cu.cc
VLOG(1) << "Starting Compute " << name();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/conv_ops_fused_image_transform.cc
LOG(WARNING) << "Conv2D was called with bad input dimensions: " << input_batches << ", " << padded_height << ", " << padded_width << ", " << input_depth;
LOG(WARNING) << "Conv2D was called with bad filter dimensions: " << filter_width << ", " << filter_height << ", " << filter_count;
LOG(WARNING) << "Conv2D was called with bad output width or height: " << output_width << ", " << output_height;
VLOG(2) << "FusedConv2D: " << name() << ", in_depth = " << in_depth << ", padded_cols = " << padded_cols << ", resized_cols = " << resized_cols << ", filter_cols = " << filter_cols << ", padded_rows = " << padded_rows << ", resized_rows = " << resized_rows << ", filter_rows = " << filter_rows << ", stride_rows = " << stride_rows << ", stride_cols = " << stride_cols << ", out_depth = " << out_depth << ", DoResize=" << DoResize;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/xsmm_conv2d.cc
VLOG(1) << "Cannot use XSMM convolutions: unsupported architecture!";
VLOG(1) << "Cannot use XSMM convolutions: unsupported format!";
VLOG(1) << "Cannot use XSMM convolutions: output features count not" " divisible by vector size!";
VLOG(2) << "Can use XSMM convolutions.";
VLOG(1) << libxsmm_dnn_get_error(status);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/tile_ops.cc
LOG(FATAL) << "TileOp: Invalid combination of Device, DT: " // << typeid(Device).name() << ", " << DataTypeString(DT);
LOG(FATAL) << "TileGradientOp: Invalid combination of Device, DT and NDIM: " << MakeTypeIndex<Device>().name() << ", " << DataTypeString(DT) << ", " << NDIM;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/batch_kernels.cc
LOG(FATAL) << "Not yet implemented";
LOG(ERROR) << "Maximum batch size greater than largest allowed size; " "ignoring allowed sizes constraint";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/mfcc_mel_filterbank.cc
LOG(ERROR) << "Number of filterbank channels must be positive.";
LOG(ERROR) << "Sample rate must be positive.";
LOG(ERROR) << "Input length must greater than 1.";
LOG(ERROR) << "Lower frequency limit must be nonnegative.";
LOG(ERROR) << "Upper frequency limit must be greater than " << "lower frequency limit.";
LOG(ERROR) << "Missing " << bad_channels.size() << " bands " << " starting at " << bad_channels[0] << " in mel-frequency design. " << "Perhaps too many channels or " << "not enough frequency resolution in spectrum. (" << "input_length: " << input_length << " input_sample_rate: " << input_sample_rate << " output_channel_count: " << output_channel_count << " lower_frequency_limit: " << lower_frequency_limit << " upper_frequency_limit: " << upper_frequency_limit;
LOG(ERROR) << "Mel Filterbank not initialized.";
LOG(ERROR) << "Input too short to compute filterbank";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/mkl_cwise_ops_common.cc
VLOG(1) << "Shapes (start mklbinaryop compute): " << in0.shape().DebugString() << " _and_ " << in1.shape().DebugString();
VLOG(1) << "Shapes (output): " << out->shape().DebugString();
VLOG(1) << "Shapes (output): " << out->shape().DebugString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/unary_ops_composition.cc
VLOG(5) << "Register compute fn: name=" << name << " cost=" << cost;
VLOG(2) << "Composed unary op: [" << absl::StrJoin(op_names_, ", ") << "]; cost=" << cost_;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/transpose_functor_gpu.cu.cc
LOG(FATAL) << "Transpose of DT_STRING tensor not supported on GPU.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/slice_op.cc
LOG(FATAL) << "begin must be either int32 or int64";
VLOG(1) << "Slice identity";
VLOG(1) << "Slice dim 0: " << input.shape().DebugString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/conv_grad_shape_utils.cc
VLOG(2) << label << ": expanded_out = " << dim->expanded_output_size << ", effective_filter_size = " << effective_filter_size << ", padded_out = " << padded_out_size << ", pad_before = " << dim->pad_before << ", pad_after = " << dim->pad_after << ", dilation = " << dim->dilation << ", strides = " << dim->stride;
VLOG(2) << "input vs filter_in depth " << dims->in_depth << " " << filter_shape.dim_size(num_dims - 2);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/unicode_ops.cc
LOG(ERROR) << "Could not set unicode error callback on converter";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/fuzzing/identity_fuzz.cc
LOG(ERROR) << "Execution failed: " << s.error_message();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/fuzzing/parse_tensor_op_fuzz.cc
LOG(WARNING) << "Unable to parse proto of tensor";
LOG(WARNING) << "Invalid tensor shape";
LOG(WARNING) << "Requiring a tensor with too many elements";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/batching_util/periodic_function.cc
LOG(WARNING) << error << "Resetting it to 0.";
VLOG(3) << "Running function.";
VLOG(3) << "Reducing interval_micros from " << interval_micros_ << " to " << (deadline - end);
VLOG(3) << "Function took longer than interval_micros, so not sleeping";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/hexagon/hexagon_control_wrapper.cc
LOG(ERROR) << "Hexagon initialization was failed. See log output.";
LOG(INFO) << "Add input: " << input << ", " << i;
LOG(INFO) << "Allocate inout buffer";
LOG(FATAL);
LOG(INFO) << "Setup graph completed";
LOG(INFO) << "Input tensor data: element size = " << tensor.NumElements() << ", byte syze = " << tensor.TotalBytes();
LOG(INFO) << "(" << ((i - 2) / 3) << ") " << line.str();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/hexagon/graph_transfer_utils.cc
LOG(INFO) << "=== Dump ranking ===";
LOG(INFO) << i << ": " << std::get<1>(entry) << ", " << std::get<2>(entry) << ", " << std::get<0>(entry);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/hexagon/hexagon_rewriter_transform.cc
LOG(INFO) << "Transforming quantized stripped model to a remote fused " "graph execute op...";
LOG(INFO) << "Input(" << i << "): name = " << input_name << ", shape = " << shape_string << ", type = " << data_type_string;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/hexagon/graph_transferer.cc
VLOG(1) << "<Node> " << node->name();
VLOG(1) << "Add dependency: " << name << " -> " << node->name();
LOG(ERROR) << "Failed to transfer graph " << status;
VLOG(1) << "Parse file " << graph_def_path;
VLOG(1) << "Failed to load graph " << status;
VLOG(1) << "Dry run graph to obtain shape of nodes";
VLOG(1) << "Load graph with output tensors";
VLOG(1) << "input_node " << input_node->name() << " of " << node.name() << " is not cached yet.";
VLOG(1) << "Register node: " << node.name() << ", " << std::hex << node_name_to_id_cache_map_.at(node.name());
VLOG(1) << "Register constant node: " << node.name();
VLOG(1) << "Cache constant shape.";
VLOG(1) << "Cache const tensor.";
VLOG(1) << "Cache const.";
VLOG(1) << "Register generic node: " << node.name();
LOG(FATAL);
VLOG(1) << "Register input node: " << node.name() << ", " << op_type;
VLOG(1) << "Register flatten node: " << node.name();
VLOG(1) << "Register generic node: " << node.name();
VLOG(1) << "Append input params: " << node.name() << ", " << node.num_inputs() << ", " << extra_inputs.size();
VLOG(1) << "Append output params: " << node.name() << ", " << node.num_outputs();
VLOG(1) << "Append node with io params: " << node.name();
LOG(FATAL);
LOG(FATAL);
LOG(FATAL);
LOG(INFO) << "*** Const Nodes ***";
LOG(INFO) << "[ " << params.node_id() << " "" << params.name() << "" (Const)";
LOG(INFO) << " shape: " << params.shape(0) << params.shape(1) << params.shape(2) << params.shape(3);
LOG(INFO) << " data_name: " << (params.data().length() <= 0 ? "" : DATA_NODE_PREFIX + ToString(params.node_id()));
LOG(INFO) << " data_size: " << params.data().length() << " bytes" << " ]";
LOG(INFO) << "******";
LOG(INFO) << "*** Op Nodes ***";
LOG(INFO) << "[ " << params.node_id() << " "" << params.name();
LOG(INFO) << " type: " << params.type_name();
LOG(INFO) << " padding: " << ToPaddingDebugString(params.padding_id());
LOG(INFO) << " inputs: " << INPUTS_NODE_PREFIX + ToString(params.node_id()) << ", size = " << params.input_count();
LOG(INFO) << " outputs: " << (params.output_count() <= 0 ? NULL_OUTPUT_NAME : (OUTPUTS_NODE_PREFIX + ToString(params.node_id()))) << ", size = " << params.output_count() << " ]";
LOG(INFO) << "******";
LOG(INFO) << "*** Node input params ***";
LOG(INFO) << "[ " << params.node_id() << " ]";
LOG(INFO) << " src node id = " << node_input.node_id() << ", output port = " << node_input.output_port();
LOG(INFO) << "******";
LOG(INFO) << "*** Node output params ***";
LOG(INFO) << "[ " << params.node_id() << " ]";
LOG(INFO) << " max_size = " << max_size;
LOG(INFO) << "******";
LOG(INFO) << sstream.str();
LOG(INFO) << "Const node count = " << graph_transfer_info_->const_node_info_size();
LOG(INFO) << sstream.str();
LOG(INFO) << "Op node count = " << graph_transfer_info_->node_info_size();
LOG(INFO) << sstream.str();
LOG(INFO) << "Input params count = " << graph_transfer_info_->node_input_info_size();
LOG(INFO) << sstream.str();
LOG(INFO) << "Output params count = " << graph_transfer_info_->node_output_info_size();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/boosted_trees/quantile_ops.cc
VLOG(1) << "QuantileStream has already been finalized for feature" << feature_idx << ".";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/boosted_trees/training_ops.cc
LOG(WARNING) << "Not growing tree ensemble as no good splits were found.";
VLOG(1) << "Adding layer #" << new_num_layers - 1 << " to tree #" << current_tree << " of ensemble of " << current_tree + 1 << " trees.";
VLOG(2) << "Breaking ties on feature ids and buckets";
LOG(WARNING) << "Not growing tree ensemble as no good splits were found.";
VLOG(1) << "Adding layer #" << new_num_layers - 1 << " to tree #" << current_tree << " of ensemble of " << current_tree + 1 << " trees.";
VLOG(2) << "Breaking ties on feature ids and buckets";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/neon/neon_depthwise_conv_op.cc
VLOG(2) << "NeonDepthwiseConv2dNative: " << " Input: [" << batch << ", " << input_rows << ", " << input_cols << ", " << in_depth << "]; Filter: [" << filter_rows << ", " << filter_cols << ", " << in_depth << ", " << depth_multiplier << "]; stride = " << stride << ", pad_rows = " << pad_rows << ", pad_cols = " << pad_cols << ", output: [" << batch << ", " << out_rows << ", " << out_cols << ", " << out_depth << "]";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/data/serialization_utils.cc
LOG(WARNING) << "We found the following stateful ops in the dataset " "construction graph whose state would not be serialized and might " "cause subtle bugs: " << absl::StrJoin(stateful_op_names, ", ");

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/data/rewrite_utils.cc
VLOG(3) << "Before graph rewrites: " << graph_def.DebugString();
VLOG(3) << "After graph rewrites: " << graph_def.DebugString();
VLOG(3) << "Failed to find node: " << output_node;
VLOG(3) << "Failed to hash graph: " << s.ToString();
VLOG(3) << "Failed to hash tensor: " << s.ToString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/data/cache_dataset_ops.cc
LOG(WARNING) << "It looks like the cache was already completely written(" << MetaFilename(dataset()->filename_) << ") after the last checkpoint was saved. Attempting to read " << "the cache instead of continuing to write. If this is a " << "mistake, please remove the above file and try running again.";
LOG(WARNING) << "Failed to get matching files on " << filename_ << "* : " << s.ToString();
LOG(WARNING) << "Failed to delete " << path << " : " << s.ToString();
LOG(ERROR) << s;
LOG(WARNING) << "The calling iterator did not fully read the dataset being " "cached. In order to avoid unexpected truncation of the " "dataset, the partially cached contents of the dataset " "will be discarded. This can happen if you have an input " "pipeline similar to `dataset.cache().take(k).repeat()`. " "You should use `dataset.take(k).cache().repeat()` instead.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/data/generator_dataset_op.cc
LOG(WARNING) << "Error occurred when finalizing GeneratorDataset iterator: " << s;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/data/parallel_interleave_dataset_op.cc
VLOG(3) << "Blocked waiting for element " << current_elements_[cycle_index_]->id;
VLOG(4) << "State before save:" << DebugString()
VLOG(2) << "Parallel interleave iterator restored";
VLOG(4) << "State after restore:" << DebugString();
VLOG(3) << "Future worker created element " << element->id;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/data/model_dataset_op.cc
VLOG(2) << "Waiting for " << wait_ms << " ms.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/data/prefetch_dataset_op.cc
VLOG(2) << "Sleeping for: " << slack_us_ * kSleepFactor;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/data/dataset_utils.cc
VLOG(2) << "Ignoring arg: " << input_arg_name << " from node: " << node.name();
LOG(WARNING) << "Cannot find " << node.op() << " in global op registry, so cannot determine which " "inputs are seeds.";
VLOG(1) << "Cancellation manager is not set. Cancellation callback will " "not be registered.";
LOG(ERROR) << "Unrecognized determinism value";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/data/tf_record_dataset_op.cc
VLOG(2) << "Reading file: " << filenames_tensor->flat<tstring>()(i);
LOG(WARNING) << "User buffer size is too small for reading Cloud TPU " << "TFRecords stored in GCS. Overriding " << buffer_size << " to the minimum recommended buffer_size = " << kCloudTpuBlockSize;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/data/captured_function.cc
VLOG(3) << "Running function " << func->func().name() << " short circuit";
VLOG(3) << "Running function " << func->func().name() << " short circuit";
LOG(WARNING) << "Failed to release handle: " << s.error_message(); }
LOG(WARNING) << "Disabling multi-device execution for a function that uses the " << FunctionLibraryDefinition::kIntsOnDeviceAttr << " attribute.";
LOG(WARNING) << "Disabling multi-device execution for a function with " "a vector argument " << arg.name() << ".";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/data/shuffle_dataset_op.cc
LOG(INFO) << "Filling up shuffle buffer (this may take a while): " << num_elements_ << " of " << this->dataset()->buffer_size_;
VLOG(1) << "Starting to fill up shuffle buffer of size: " << this->dataset()->buffer_size_;
LOG(INFO) << "Shuffle buffer filled.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/data/experimental/sliding_window_dataset_op.cc
LOG(WARNING) << "window_shift: " << window_shift << " is equal to window_size: " << window_size << " and window_stride is 1, use `batch` instead.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/data/experimental/directed_interleave_dataset_op.cc
VLOG(2) << "DirectedInterleave selected an exhausted input: " << selected_input;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/data/experimental/sql_dataset_op.cc
LOG(WARNING) << "Failed to close query connection: " << s; }
LOG(WARNING) << "Failed to connect to database: " << s;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/data/experimental/snapshot_dataset_op.cc
LOG(ERROR) << "Compression is unsupported on mobile platforms. Turning " << "off compression.";
LOG(ERROR) << "Could not finish writing file: " << s;
LOG(ERROR) << "Compression is unsupported on mobile platforms. Turning " << "off compression.";
LOG(INFO) << "Graph hash is " << hash_hex << ", writing to " << graph_file;
LOG(INFO) << "Overriding mode to reader.";
LOG(INFO) << "Overriding mode to writer.";
LOG(INFO) << "Overriding mode to passthrough.";
LOG(WARNING) << "Unable to write graphdef to disk, error: " << dump_status.ToString();
LOG(INFO) << "Graph def serialized to hash: " << graph_hash;
VLOG(2) << "Snapshot state: " << state_;
VLOG(2) << "Saving Snapshot iterator: " << state_;
LOG(ERROR) << "Dataset has changed while restoring from the " "checkpoint. Old hash: " << hash_dir << "; new hash: " << hash_dir_;
VLOG(2) << "Restoring Snapshot iterator: " << state_;
LOG(INFO) << "Current read throughput (MBPS): " << read_throughput;
VLOG(2) << "Saving SnapshotReaderIterator: " << num_elements_read_ << "; elements_produced: " << elements_produced_;
LOG(ERROR) << "Restoring read iterator from ckpt with old " << "run_dir: " << run_dir << " but new run_dir is: " << run_dir_ << ". We'll now restart snapshot creation.";
LOG(ERROR) << "Old filenames size: " << filenames_size << "; new filenames size: " << filenames_.size();
VLOG(2) << "Restoring SnapshotReaderIterator: " << num_elements_read_ << "; elements_produced: " << elements_produced_;
VLOG(2) << "Starting to read: " << filename;
VLOG(2) << "Finished reading: " << filename;
LOG(ERROR) << "Encountered an error: " << s.ToString();
LOG(INFO) << "Current write throughput (MBPS): " << write_throughput;
VLOG(2) << "Saving SnapshotWriterIterator: " << num_elements_written_ << "; elements_produced: " << elements_produced_;
LOG(INFO) << "Old hash dir from ckpt: " << hash_dir << " is not the same as the new one: " << hash_dir_;
VLOG(2) << "Restoring SnapshotWriterIterator: " << num_elements_written_ << "; elements_produced: " << elements_produced_;
LOG(ERROR) << "Creating " << snapshot_data_filename << " failed: " << s.ToString();
LOG(INFO) << "Error while writing snapshot data to disk: " << s.ToString();
LOG(INFO) << "Writing compression achieved: " << compression_ratio_;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/data/experimental/sql/sqlite_query_connection.cc
LOG(ERROR) << "Use of unsupported TensorFlow data type by 'SqlQueryConnection': " << DataTypeString(data_type) << ".";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/rnn/lstm_ops.cc
VLOG(1) << "AlignTensor called for " << name << ", shape " << t.shape().DebugString() << ". This is unnecessary copying. Consider using shapes with even " << "sizes";
LOG(WARNING) << "BlockLSTMOp is inefficient when both batch_size and " << "input_size are odd. You are using: batch_size=" << batch_size << ", input_size=" << input_size;
LOG(WARNING) << "BlockLSTMOp is inefficient when both batch_size and " << "cell_size are odd. You are using: batch_size=" << batch_size << ", cell_size=" << cell_size;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/nccl/nccl_manager.cc
VLOG(2) << "New NcclManager " << this;
VLOG(2) << "~NcclManager " << this;
VLOG(2) << "SignalMultiNodeReady collective " << collective_key << " to_run " << to_run;
VLOG(2) << "RunCollective rank " << rank << " global_rank " << p->global_rank << " root_rank " << collective->root_rank;
VLOG(3) << "Locking mutex nccl_stream " << nccl_stream;
VLOG(2) << "call NcclAllReduce collective_key " << collective->collective_key << " participant " << p_idx << " sendbuff " << sendbuff << " recvbuff " << recvbuff << " nccl_comm " << nccl_comm << " comm_stream " << comm_stream << " cuda_stream " << cu_stream;
VLOG(2) << "call NcclBroadcast collective_key " << collective->collective_key << " participant " << p_idx << " sendbuff " << sendbuff << " recvbuff " << recvbuff << " nccl_comm " << nccl_comm << " comm_stream " << comm_stream << " cuda_stream " << cu_stream;
VLOG(2) << "call NcclAllGather collective_key " << collective->collective_key << " participant " << p_idx << " sendbuff " << sendbuff << " sendcount " << p->input->NumElements() << " recvbuff " << recvbuff << " recvcount " << p->output->NumElements() << " nccl_comm " << nccl_comm << " comm_stream " << comm_stream << " cuda_stream " << cu_stream;
VLOG(2) << "done Nccl kernel collective_key " << collective->collective_key << " participant " << p_idx << " ncclResult " << nccl_result;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/util/debug_events_writer.cc
VLOG(1) << "Successfully opened debug events file: " << file_path_;
LOG(ERROR) << "Write failed because file could not be opened.";
VLOG(1) << "Successfully opened debug event file: " << filename;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/util/events_writer.cc
LOG(WARNING) << "Re-initialization, attempting to open a new file, " << num_outstanding_events_ << " events will be lost.";
VLOG(1) << "Successfully opened events file: " << filename_;
LOG(ERROR) << "Write failed because file could not be opened.";
VLOG(1) << "Wrote " << num_outstanding_events_ << " events to disk.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/util/stat_summarizer.cc
LOG(WARNING) << "Number of outputs changed between runs for '" << ns.node_name() << "' - was " << outputs->size() << ", now " << ns.output_size();
LOG(WARNING) << "Output tensor changed between runs for '" << ns.node_name();
LOG(INFO) << line;
LOG(INFO) << "============ Node output tensor sizes in run order ========";
LOG(INFO) << stream.str();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/util/command_line_flags.cc
LOG(ERROR) << "Couldn't interpret value " << arg << " for flag " << flag << ".";
LOG(ERROR) << "Couldn't interpret value " << arg << " for flag " << flag << ".";
LOG(ERROR) << "Couldn't interpret value " << arg << " for flag " << flag << ".";
LOG(ERROR) << "Couldn't interpret value " << arg << " for flag " << flag << ".";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/util/dump_graph.cc
LOG(WARNING) << "Failed to dump " << name << " because dump location is not " << " specified through either TF_DUMP_GRAPH_PREFIX environment " << "variable or function argument.";
LOG(WARNING) << "TF_DUMP_GRAPH_PREFIX=sponge, but " "TEST_UNDECLARED_OUTPUT_DIRS is not set, dumping to log";
LOG(INFO) << proto.DebugString();
LOG(WARNING) << "Failed to create " << dir << " for dumping " << proto_type << ": " << status;
LOG(WARNING) << "Failed to dump " << proto_type << " to file: " << filepath << " : " << status;
LOG(INFO) << "Dumped " << proto_type << " to " << filepath;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/util/matmul_autotune.cc
LOG(ERROR) << status.error_message();
LOG(ERROR) << status.error_message();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/util/tensor_slice_set.cc
LOG(WARNING) << s;
LOG(WARNING) << s;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/util/tensor_slice_reader_cache.cc
LOG(WARNING) << "Caching disabled because the open function is a lambda or " "RTTI is not enabled in this build.";
VLOG(1) << "Creating new TensorSliceReader for " << filepattern;
VLOG(1) << "Cached TensorSliceReader for " << filepattern << ": " << reader;
VLOG(1) << "Using cached TensorSliceReader for " << filepattern << ": " << reader;
LOG(WARNING) << "Caching disabled because the checkpoint file " << "is being opened with two different open functions: " << filepattern;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/util/example_proto_helper.cc
LOG(FATAL) << "not supposed to be here. dtype requested: " << dtype;
LOG(FATAL) << "Not supposed to be here. Saw dtype: " << dtype;
LOG(FATAL) << "Not supposed to be here. Saw dtype: " << dtype;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/util/tensor_format.cc
LOG(FATAL) << "Invalid Format: " << static_cast<int32>(format);
LOG(FATAL) << "Invalid Filter Format: " << static_cast<int32>(format);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/util/util.cc
VLOG(2) << "TF-MKL: Disabling MKL";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/util/use_cudnn.cc
LOG(ERROR) << status;
LOG(ERROR) << "FP16ConvMode only supports two modes, ACCURATE and FAST. " "Got unknown mode: " << value;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/util/strided_slice_op.cc
LOG(FATAL) << "begin must be either int32 or int64";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/util/tensor_slice_writer.cc
VLOG(1) << "Written " << slices_ << " slices for " << sts_.meta().tensor_size() << " tensors (" << file_size << " bytes) to " << filename_;
LOG(ERROR) << "Failed to rename file " << tmpname_ << " to " << filename_;
LOG(FATAL) << "MaxBytesPerElement not implemented for dtype: " << dt;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/util/example_proto_fast_parsing.cc
LOG(FATAL) << "Should not happen.";
LOG(WARNING) << "Data loss! Feature '" << feature_name << "' is present in multiple concatenated " "tf.Examples. Ignoring all but last one.";
LOG(WARNING) << "Data loss! Feature '" << feature_name << "' is present in multiple concatenated " "tf.Examples. Ignoring all but last one.";
LOG(FATAL) << "Should not happen.";
LOG(FATAL) << "Should not happen.";
LOG(FATAL) << "Should not happen.";
LOG(FATAL) << "Should not happen.";
LOG(WARNING) << "Collision found. This should happen only if you have " "around 2^32 entries in your config.";
LOG(WARNING) << "Collision found. This should happen only if you have " "around 2^32 entries in your config.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/util/tensor_slice_reader.cc
LOG(WARNING) << "Could not open " << fname << ": " << s;
VLOG(1) << "TensorSliceReader for " << filepattern;
VLOG(1) << "Loading shard " << preferred_shard << " for " << filepattern_;
VLOG(1) << "Reading meta data from file " << fname << "...";
VLOG(1) << "Loading all shards for " << filepattern_;
VLOG(1) << "Did not find tensor in preferred shard, loading all shards: " << name;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/util/proto/proto_utils.cc
LOG(ERROR) << "output must be non NULL";
LOG(ERROR) << "error_text must be non NULL";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/util/tensor_bundle/tensor_bundle.cc
VLOG(1) << "Appending " << *bytes_written << " bytes to file";
VLOG(1) << "Writing to file " << tmp_data_path_;
VLOG(1) << "Merging bundle:" << prefix;
VLOG(1) << "Renaming " << p.first << " to " << DataFilename(merged_prefix, p.second, merge.shard_ids.size());
VLOG(1) << "Merged bundles to:" << merged_prefix;
VLOG(1) << "Optimized for common case: directly copying into " "pre-allocated buffer; spec: " << slice_spec.DebugString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/framework/function.cc
LOG(WARNING) << "Failed to parse device "" << n.device() << "" in " << n.op() << ":" << n.name();
VLOG(5) << "Instantiate function definition: name=" << signature.name() << " #input_args=" << signature.input_arg_size() << " #output_args=" << signature.output_arg_size() << " #control_output=" << signature.control_output_size();
VLOG(5) << "|| " << line;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/framework/function_handle_cache.cc
LOG(ERROR) << "Failed to clear function handle cache: " << s.ToString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/framework/allocator_registry.cc
LOG(FATAL) << "New registration for AllocatorFactory with name=" << name << " priority=" << priority << " at location " << source_file << ":" << source_line << " conflicts with previous registration at location " << existing->source_file << ":" << existing->source_line;
LOG(FATAL) << "No registered CPU AllocatorFactory";
LOG(FATAL) << "No registered CPU AllocatorFactory";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/framework/run_handler_util.cc
LOG(ERROR) << "Wrong format for " << var_name << ". Use default value.";
LOG(ERROR) << "Wrong format for " << var_name << ". Use default value.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/framework/cpu_allocator_impl.cc
LOG(WARNING) << "Allocation of " << num_bytes << " exceeds " << 100 * kLargeAllocationWarningThreshold << "% of free system memory.";
LOG(WARNING) << "Total allocated memory " << stats_.bytes_in_use << "exceeds " << 100 * kTotalAllocationWarningThreshold << "% of free system memory";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/framework/variant.cc
LOG(ERROR) << "Could not decode variant with type_name: "" << variant_array[i].TypeName() << "". Perhaps you forgot to register a " "decoder via REGISTER_UNARY_VARIANT_DECODE_FUNCTION?";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/framework/op.cc
LOG(WARNING) << "No kernel validator registered with OpRegistry.";
VLOG(1) << status.ToString();
LOG(INFO) << "All registered Ops:";
LOG(INFO) << SummarizeOpDef(op);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/framework/dataset.cc
LOG(ERROR) << "The Encode() method is not implemented for " "DatasetVariantWrapper objects.";
LOG(ERROR) << "The Decode() method is not implemented for " "DatasetVariantWrapper objects.";
VLOG(1) << "Function with name " << function_name << "already exists in" << " the graph. It will not be added again.";
LOG_EVERY_N_SEC(WARNING, 30) << "Input of " << dataset->DebugString() << " will not be optimized because the dataset does not implement the " "AsGraphDefInternal() method needed to apply optimizations.";
VLOG(2) << prefix() << " constructor";
VLOG(2) << prefix() << " destructor";
DVLOG(3) << prefix() << " GetNext enter";
LOG(ERROR) << s;
DVLOG(3) << prefix() << " GetNext exit";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/framework/tensor.cc
LOG(ERROR) << "Input size was " << actual << " and expected " << expected;
LOG(ERROR) << "Could not decode variant with type_name: "" << data[i].TypeName() << "". Perhaps you forgot to register a " "decoder via REGISTER_UNARY_VARIANT_DECODE_FUNCTION?";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/framework/model.cc
VLOG(3) << "Adding " << node->long_name() << " as input for " << output->long_name();
VLOG(3) << "Adding " << node->long_name();
VLOG(3) << "Removing " << (*node)->long_name();
VLOG(2) << "Starting optimization of tunable parameters with GradientDescent";
VLOG(2) << "Number of tunable parameters: " << parameters.size();
VLOG(2) << "Setting tunable parameter " << pair.first << " to " << parameter->value;
VLOG(2) << "Starting optimization of tunable parameters with HillClimb";
VLOG(2) << "Failed to find a tunable parameter that would decrease the " "output time. This means that the autotuning optimization got " "stuck in a local maximum. The optimization attempt will be " "aborted.";
VLOG(2) << "Number of tunable parameters: " << parameters.size();
VLOG(2) << "Setting tunable parameter " << pair.first << " to " << parameter->value;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/framework/ops_util.cc
LOG(FATAL) << "Eigen does not have explicit padding enum " // Crash OK "value";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/framework/log_memory.cc
LOG(INFO) << LogMemory::kLogMemoryLabel << " " << type_name << " { " << proto.ShortDebugString() << " }";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/framework/op_def_util.cc
LOG(WARNING) << "Op " << op_def.name() << " is deprecated." << " It will cease to work in GraphDef version " << dep.version() << ". " << dep.explanation() << ".";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/framework/op_segment.cc
LOG(ERROR) << "Create kernel failed: " << s;
VLOG(1) << "Session " << session_handle << " is not found.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/framework/types.cc
LOG(ERROR) << "Unrecognized DataType enum value " << dtype;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/framework/variant_op_registry.cc
LOG(ERROR) << "DecodeUnaryVariant: Variant type_name before decoding was: " << type_name << " but after decoding was: " << variant->TypeName() << ". Treating this as a failure.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/framework/local_rendezvous.cc
DVLOG(2) << "Send " << this << " " << key_hash << " " << key.FullKey();
DVLOG(2) << "Enqueue Send Item (key:" << key.FullKey() << "). ";
DVLOG(2) << "Consume Recv Item (key:" << key.FullKey() << "). ";
DVLOG(2) << "Clean up Send/Recv queue (key:" << key.FullKey() << "). ";
DVLOG(2) << "Recv " << this << " " << key_hash << " " << key.FullKey();
DVLOG(2) << "Enqueue Recv Item (key:" << key.FullKey() << "). ";
DVLOG(2) << "Consume Send Item (key:" << key.FullKey() << "). ";
DVLOG(2) << "Clean up Send/Recv queue (key:" << key.FullKey() << "). ";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/framework/op_kernel.cc
LOG(WARNING) << "OpKernelContext is tracking allocations but they are not " << "being consumed by the StepStatsCollector.";
VLOG(2) << "Warning: OpKernel " << params_->op_kernel->name() << " called allocate_temp with scope_id " << allocator_attr.scope_id << ". Switch to allocate_output to avoid performance penalty.";
LOG(WARNING) << "OpKernel " << params_->op_kernel->name() << " called both allocate_output and set_output with scope_id " << output_alloc_attr(index).scope_id;
VLOG(1) << "OpKernelContext set_output index " << index << " tensor " << tensor.DebugString() << " never_forward " << never_forward << " params_->forward_from_array[index] " << params_->forward_from_array[index] << " alloc_attr.scope_id " << output_alloc_attr(index).scope_id;
LOG(WARNING) << "Loading UNSAFE library " << fullpath << " because ABI check override is set: " << s.error_message();
LOG(WARNING) << "Not loading plugin library " << fullpath << ": " << s.error_message();
VLOG(1) << "No device-specific kernels found for NodeDef '" << FormatNodeDefForError(node_name, has_experimental_debug_info, experimental_debug_info) << "'" << "Will fall back to a default kernel." << std::endl;
LOG(INFO) << "OpKernel ('" << kernel_def.ShortDebugString() << "')";
VLOG(1) << "Instantiating kernel for node: " << SummarizeNodeDef(node_def);
LOG(ERROR) << "OpKernel ('" << kernel_def.ShortDebugString() << "') for unknown op: " << kernel_def.op();
VLOG(1) << s;
LOG(WARNING) << s;
VLOG(1) << "OP_REQUIRES failed at " << io::Basename(file) << ":" << line << " : " << s;
LOG(WARNING) << "OP_REQUIRES failed at " << io::Basename(file) << ":" << line << " : " << s;
VLOG(1) << s;
LOG(WARNING) << s;
VLOG(1) << "OP_REQUIRES failed at " << io::Basename(file) << ":" << line << " : " << s;
LOG(WARNING) << "OP_REQUIRES failed at " << io::Basename(file) << ":" << line << " : " << s;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/framework/graph_def_util.cc
VLOG(1) << "Ignoring encountered unknown operation " << SummarizeNodeDef(*node) << " when stripping default attributes. It is likely a function, " "in which case ignoring it is fine";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/framework/kernel_def_builder.cc
LOG(INFO) << integer;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/framework/device_base.cc
LOG(FATAL) << "Device does not implement attributes()";
LOG(FATAL) << "Device does not implement name()";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/framework/run_handler.cc
VLOG(3) << "Added " << (is_blocking ? "inter" : "intra") << " work from " << traceme_id_.load(std::memory_order_relaxed);
VLOG(1) << "Creating RunHandlerThreadPool " << name << " with " << num_blocking_threads_ << " blocking threads and " << num_non_blocking_threads_ << " non-blocking threads.";
VLOG(1) << "Exiting RunHandlerThreadPool " << name_;
VLOG(3) << "Running " << (is_blocking ? "inter" : "intra") << " work for " << tws->GetTracemeId();
VLOG(2) << "Running " << (task_from_blocking_queue ? "inter" : "intra") << " work from " << tws->GetTracemeId();
VLOG(4) << "source id " << i << " " << (*thread_work_sources)[i]->ToString();
VLOG(1) << "Creating a RunHandlerPool with max handlers: " << max_handlers_;
VLOG(2) << "Set work for tid=" << i << " with start_request_idx=" << request_idx_list[i];
VLOG(2) << "Set work for tid=" << (i + num_blocking_threads) << " with start_request_idx=" << request_idx_list[i];
VLOG(1) << "Printing time histogram: " << time_hist_.ToString();
VLOG(1) << "Active session runs: " << num_active_requests;
VLOG(1) << "Elapsed times are: " << times_str;
VLOG(1) << "Step ids are: " << ids_str;
VLOG(3) << "Scheduling inter work for " << tws()->GetTracemeId();
VLOG(3) << "Scheduling intra work for " << tws()->GetTracemeId();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/platform/status.cc
LOG(WARNING) << "Failed to parse env variable " "TF_WORKER_NUM_WARNING_ERROR_LOG_IN_STATUS=" << num_msgs_str << " as int. Using the default value " << num_messages_ << ".";
VLOG(5) << "Generated non-OK status: "" << *this << "". " << CurrentStackTrace();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/platform/path.cc
LOG(FATAL) << "GetTempFilename is not implemented in this platform.";
LOG(FATAL) << "Cannot get the directory for temporary files.";
LOG(FATAL) << "Cannot get a temporary file in: " << temp_dir;
LOG(FATAL) << "Failed to create temp file.";
LOG(FATAL) << "No temp directory found.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/platform/logger.cc
VLOG(2) << proto->ShortDebugString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/platform/cpu_feature_guard.cc
LOG(FATAL)#endif << "The TensorFlow library was compiled to use " << feature_name << " instructions, but these aren't available on your machine.";
LOG(INFO) << "Your CPU supports instructions that this TensorFlow " << "binary was not compiled to use:" << missing_instructions;
LOG(INFO) << "This TensorFlow binary is optimized with Intel(R) MKL-DNN " << "to use the following CPU instructions in performance " << "critical operations: " << missing_instructions << std::endl << "To enable them in non-MKL-DNN operations, rebuild " << "TensorFlow with the appropriate compiler flags.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/platform/profile_utils/clock_cycle_profiler.cc
LOG(INFO) << tag << ": average = " << std::chrono::duration_cast<std::chrono::microseconds>( average_time) .count() << " us (" << average_clock_cycle << " cycles)" << ", count = " << count;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/platform/profile_utils/android_armv7a_cpu_utils_helper.cc
LOG(WARNING) << "You enabled clock cycle profile but frequency may " << "be scaled. (max = " << cpu0_scaling_max << ", min " << cpu0_scaling_min << ")";
LOG(WARNING) << "Error opening perf event";
LOG(WARNING) << "Failed to "" << file_path << """;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/platform/profile_utils/cpu_utils.cc
LOG(WARNING) << "Failed to open /proc/cpuinfo";
LOG(WARNING) << "Failed to get CPU frequency: " << freq_ghz << " GHz";
LOG(INFO) << "CPU Frequency: " << freq_n << " Hz";
LOG(WARNING) << "Failed to find bogomips or clock in /proc/cpuinfo; cannot determine " "CPU frequency";
LOG(WARNING) << "Failed to get CPU frequency: " << freq_hz << " Hz";
LOG(FATAL) << "cpu_utils_helper_instance_ is already instantiated.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/platform/s3/aws_logging.cc
LOG(INFO) << message;
LOG(WARNING) << message;
LOG(ERROR) << message;
LOG(FATAL) << message;
LOG(ERROR) << message;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/platform/default/resource_loader.cc
LOG(FATAL) << "Unable to access the data dependencies of this test." "Make sure you are running this test using bazel.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/platform/default/unbounded_work_queue.cc
LOG(ERROR) << "UnboundedWorkQueue named "" << thread_name_ << "" was " << "deleted with pending work in its queue. This may indicate " << "a potential use-after-free bug.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/platform/default/subprocess.cc
LOG(FATAL) << "SetProgram called after the process was started.";
LOG(FATAL) << "SetProgram failed to allocate file string.";
LOG(FATAL) << "SetProgram failed to allocate command argument.";
LOG(FATAL) << "SetChannelAction called after the process was started.";
LOG(FATAL) << "SetChannelAction called with invalid channel: " << chan;
LOG(FATAL) << "SetChannelAction called with invalid action: " << action;
LOG(ERROR) << "Start called after the process was started.";
LOG(ERROR) << "Start called without setting a program.";
LOG(ERROR) << "Start cannot create pipe: " << strerror(errno);
LOG(ERROR) << "Start cannot make pipe non-blocking: " << strerror(errno);
LOG(ERROR) << "Start cannot make pipe close-on-exec: " << strerror(errno);
LOG(ERROR) << "Start cannot fork() child process: " << strerror(errno);
LOG(ERROR) << "Communicate called without a running process.";
LOG(ERROR) << "Communicate cannot get SIGPIPE handler: " << strerror(errno);
LOG(ERROR) << "Communicate cannot ignore SIGPIPE: " << strerror(errno);
LOG(ERROR) << "Communicate cannot poll(): " << strerror(errno);
LOG(ERROR) << "Communicate cannot poll(): timeout not possible";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/platform/default/net.cc
LOG(ERROR) << "socket() failed: " << strerror(errno);
LOG(ERROR) << "setsockopt() failed: " << strerror(errno);
LOG(WARNING) << "bind(port=" << *port << ") failed: " << strerror(errno);
LOG(WARNING) << "getsockname() failed: " << strerror(errno);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/platform/default/cuda_libdevice_path.cc
VLOG(3) << "CUDA root = " << TF_CUDA_TOOLKIT_PATH;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/platform/default/rocm_rocdl_path.cc
VLOG(3) << "ROCM root = " << TF_ROCM_TOOLKIT_PATH;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/platform/default/port.cc
LOG(ERROR) << "Call to hwloc_topology_init() failed";
LOG(ERROR) << "Call to hwloc_topology_load() failed";
LOG(ERROR) << "Could not find hwloc NUMA node " << node;
LOG(ERROR) << "Failed to find hwloc NUMA node " << node;
LOG(ERROR) << "Failed call to hwloc_get_area_memlocation.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/platform/default/env.cc
LOG(FATAL) << "Env::Default() must not be destroyed";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/platform/hadoop/hadoop_file_system.cc
LOG(ERROR) << "HadoopFileSystem load error: " << status_.error_message();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/platform/cloud/gcs_dns_cache.cc
LOG(ERROR) << "Error resolving " << name << " (EAI_SYSTEM): " << strerror(errno);
LOG(ERROR) << "Error resolving " << name << ": " << gai_strerror(error_code);
LOG(ERROR) << "Error resolving " << name << ": " << gai_strerror(error_code);
VLOG(1) << "Starting GCS DNS cache.";
VLOG(1) << "Annotated DNS mapping: " << name << " --> " << chosen_address;
LOG(WARNING) << "No IP addresses available for " << name;
VLOG(1) << "Resolving DNS name: " << name;
LOG(WARNING) << "Non-IPv4 address returned. ai_family: " << i->ai_family << ". sa_family: " << i->ai_addr->sa_family << ".";
LOG(ERROR) << "Error converting response to IP address for " << name << ": " << strerror(errno);
VLOG(1) << "... address: " << buf;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/platform/cloud/compute_engine_zone_provider.cc
LOG(ERROR) << "Failed to parse the zone name from location: " << string(location);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/platform/cloud/gcs_file_system.cc
VLOG(1) << "GCS cache max size = " << max_bytes << " ; " << "block size = " << block_size_ << " ; " << "max staleness = " << max_staleness;
VLOG(1) << "GCS DNS cache is enabled. " << kResolveCacheSecs << " = " << resolve_frequency_secs;
VLOG(1) << "GCS DNS cache is disabled, because " << kResolveCacheSecs << " = 0 (or is not set)";
VLOG(1) << "GCS additional header ENABLED. " << "Name: " << additional_header_->first << ", " << "Value: " << additional_header_->second;
LOG(ERROR) << "GCS additional header DISABLED. Invalid contents: " << add_header_contents;
LOG(ERROR) << "GCS additional header DISABLED. Invalid contents: " << add_header_contents;
VLOG(1) << "GCS additional header DISABLED. No environment variable set.";
VLOG(1) << "File signature has been changed. Refreshing the cache. Path: " << fname;
VLOG(1) << "Successful read of gs://" << bucket << "/" << object << " @ " << offset << " of size: " << bytes_read;
VLOG(2) << "Successful integrity check for: gs://" << bucket << "/" << object << " @ " << offset;
VLOG(1) << "Stat of: gs://" << bucket << "/" << object << " -- " << " length: " << stat->base.length << " generation: " << stat->generation_number << "; mtime_nsec: " << stat->base.mtime_nsec << "; updated: " << updated;
LOG(ERROR) << "Tried to set cache stats of non-initialized file block " "cache object. This may result in not exporting the intended " "monitoring data";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/platform/cloud/google_auth_provider.cc
LOG(WARNING) << "All attempts to get a Google authentication bearer token failed, " << "returning an empty token. Retrieving token from files failed with "" << token_from_files_status.ToString() << ""." << " Retrieving token from GCE failed with "" << token_from_gce_status.ToString() << "".";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/platform/cloud/curl_http_request.cc
LOG(ERROR) << "The transmission of request " << this_object << " (URI: " << that->uri_ << ") has been stuck at " << current_progress << " of " << dltotal + ultotal << " bytes for " << now - that->last_progress_timestamp_ << " seconds and will be aborted. CURL timing information: " << "lookup time: " << lookup_time << " (" << curl_easy_strerror(lookup_time_status) << "), connect time: " << connect_time << " (" << curl_easy_strerror(connect_time_status) << "), pre-transfer time: " << pretransfer_time << " (" << curl_easy_strerror(pretransfer_time_status) << "), start-transfer time: " << starttransfer_time << " (" << curl_easy_strerror(starttransfer_time_status) << ")";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/platform/cloud/retrying_utils.cc
VLOG(1) << "The operation failed and will be automatically retried in " << (delay_micros / 1000000.0) << " seconds (attempt " << (retries + 1) << " out of " << config.max_retries << "), caused by: " << status.ToString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/platform/windows/subprocess.cc
LOG(FATAL) << "WaitForSingleObject on child process output failed. " "Error code: " << wait_result;
LOG(FATAL) << "SetProgram called after the process was started.";
LOG(FATAL) << "SetProgram failed to allocate file string.";
LOG(FATAL) << "SetProgram failed to allocate command argument.";
LOG(FATAL) << "SetChannelAction called after the process was started.";
LOG(FATAL) << "SetChannelAction called with invalid channel: " << chan;
LOG(FATAL) << "SetChannelAction called with invalid action: " << action;
LOG(ERROR) << "Start called after the process was started.";
LOG(ERROR) << "Start called without setting a program.";
LOG(ERROR) << "Cannot create pipe. Error code: " << GetLastError();
LOG(ERROR) << "Cannot set pipe handle attributes.";
LOG(ERROR) << "Call to CreateProcess failed. Error code: " << GetLastError();
LOG(INFO) << "SubProcess ended with return code: " << process_exit_code << std::endl;
LOG(FATAL) << "Wait failed with code: " << GetLastError();
LOG(FATAL) << "WaitForSingleObject call on the process handle failed. " "Error code: " << wait_status;
LOG(ERROR) << "Communicate called without a running process.";
LOG(ERROR) << "Waiting on the io threads failed! result: " << wait_result << std::endl;
LOG(ERROR) << "One of the IO threads failed with code: " << exit_code;
LOG(ERROR) << "Error checking io thread exit statuses. Error Code: " << GetLastError();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/platform/windows/net.cc
LOG(ERROR) << "socket() failed: " << WindowsWSAGetLastErrorMessage();
LOG(ERROR) << "setsockopt() failed: " << WindowsWSAGetLastErrorMessage();
LOG(WARNING) << "bind(port=" << *port << ") failed: " << WindowsWSAGetLastErrorMessage();
LOG(WARNING) << "getsockname() failed: " << WindowsWSAGetLastErrorMessage();
LOG(ERROR) << "Error at WSAStartup()";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/platform/windows/env.cc
LOG(FATAL) << "Env::Default() must not be destroyed";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/graph/subgraph.cc
VLOG(2) << "Found fetch node for " << t;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/graph/optimizer_cse.cc
VLOG(1) << "CSE: equivalent: " << (*candidate)->name() << " and " << n->name();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/graph/collective_order.cc
VLOG(2) << "collective node " << node->DebugString();
VLOG(1) << "Adding control dependency from node " << src_node->name() << " instance " << instance_keys[src_idx] << " to node " << dst_node->name() << " instance " << instance_keys[dst_idx];

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/graph/costmodel.cc
VLOG(1) << "num non_zero vals: " << non_zero.size() << " median_value " << median_value;
LOG(ERROR) << "Unexpected output slot for node " << node->DebugString() << ". Got " << output_slot << " but its num_outputs is " << node->num_outputs();
VLOG(2) << "Node " << n->id() << ": " << n->name() << " type_string: " << n->type_string();
LOG(INFO) << " min_count_=" << min_count_;
LOG(INFO) << "Node " << i << " count " << count_[i] << " total time " << time_[i] << " avg time " << (time_[i] / (std::max(1, count_[i])));

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/graph/graph.cc
LOG(ERROR) << "Failed at updating node: " << status;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/graph/mkl_layout_pass.cc
VLOG(1) << "MklLayoutRewritePass: Skipping rewriting of the node " << n->type_string() << ", reason: " << reason;
VLOG(1) << "MklLayoutRewritePass: Could not find matching " << "Conv2D and BiasAdd node for merging. Input node: " << m->DebugString();
VLOG(1) << "MklLayoutRewritePass: Could not find matching " << "Pad and Conv2D node for merging. Input node: " << m->DebugString();
VLOG(1) << "MklLayoutRewritePass: Could match Pad and _FusedConv2D " << "nodes but cannot merge them. Only conv ops with padding " << "type VALID can be merged with Pad op Input node: " << m->DebugString();
VLOG(1) << "MklLayoutRewritePass: Could not find matching " << "Pad and _FusedConv2D node for merging. Input node: " << m->DebugString();
VLOG(1) << "MklLayoutRewritePass: Could not find matching " << "Conv2DBackpropFilter and BiasAddGrad node for merging. " << "Input node: " << m->DebugString();
VLOG(1) << "DequantizeRewrite: Mode is not SCALED. " << "This case is not optimized by Intel MKL kernel, thus using " "Eigen op for Dequantize op.";
VLOG(1) << "DequantizeRewrite: Trying to dequantize a Const node which " << "could possibly be a filter. " << "This case is not supported by Intel MKL kernel, thus using " "Eigen op for Dequantize op.";
VLOG(1) << "LrnRewrite: The model sets depth_radius as not 2 which" << "case is not optimized by Intel MKL, thus using Eigen op" << "for LRN ";
VLOG(1) << "LeakyReluRewrite: The model sets alpha is greater than 1 " << "which case is not optimized by Intel MKL, thus using Eigen op" << "for LeakyRelu ";
VLOG(1) << "QuantizeOpRewrite: narrow range is enabled for quantization." << "This case is not optimized by Intel MKL, " << "thus using Eigen op for Quantize op ";
VLOG(1) << "QuantizeOpRewrite: dimension is specified for " << "per slice quantization." << "This case is not optimized by Intel MKL, " << "thus using Eigen op for Quantize op ";
VLOG(1) << "QuantizeOpRewrite: Mode is not SCALED or MIN_FIRST and/or" << "rounding mode is not HALF_TO_EVEN. " << "This case is not optimized by Intel MKL, thus using Eigen op" << "for Quantize op ";
VLOG(1) << "QuantizeOpRewrite: Trying to quantize a node which " << "is a constant. " << "This case is not supported by the kernel, thus using Eigen op" << "for Quantize op ";
VLOG(1) << "QuantizeOpRewrite: For MIN_FIRST mode the data type is " << "not DT_UINT8. This case is not optimized by Intel MKL, " << "thus using Eigen op for Quantize op ";
VLOG(1) << "MklLayoutRewritePass: unusual case of same filter" << " feeding multiple Conv2D nodes: " << filter_node->DebugString();
VLOG(1) << "MklLayoutRewritePass: workspace_enabled for " << orig_node->type_string();
VLOG(1) << "MklLayoutRewritePass: workspace_enabled for " << orig_node->type_string();
VLOG(1) << "MklLayoutRewritePass: dummy workspace_enabled for " << orig_node->type_string();
VLOG(1) << "MklLayoutRewritePass: Merged old node:" << pred->DebugString() << ", and node: " << succ->DebugString() << ", into node:" << new_node->DebugString();
VLOG(1) << "MklLayoutRewritePass: Merged old node:" << pred->DebugString() << ", and node: " << succ->DebugString() << ", into node:" << new_node->DebugString();
VLOG(1) << "MklLayoutRewritePass: Merged old node:" << badd->DebugString() << ", and node: " << fltr->DebugString() << ", into node:" << new_node->DebugString();
VLOG(1) << "MklLayoutRewritePass: Original node:" << orig_node->DebugString();
VLOG(1) << "MklLayoutRewritePass: New node:" << new_node->DebugString();
VLOG(1) << "MklLayoutRewritePass: Scheduled nodes " << n1_name << " and " << n2_name << " for merging";
VLOG(1) << "MklLayoutRewritePass: Merged nodes " << n1_name << " and " << n2_name;
VLOG(1) << "MklLayoutRewritePass: Scheduled node " << node_name << " with op " << op_name << " for rewrite using" << " layout optimization.";
VLOG(1) << "MklLayoutRewritePass: rewrote node " << node_name << " with op " << op_name << " for Mkl layout optimization.";
VLOG(1) << "MklLayoutRewritePass: fixed metadata edges for node " << node_name << " with op " << op_name;
VLOG(2) << "TF-MKL: Disabling MKL";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/graph/graph_partition.cc
VLOG(1) << "Receiving data" << " from " << src->name() << " (" << src->type_string() << ")" << " on " << src->assigned_device_name() << " in " << (src_host_memory ? "host memory" : "device memory") << " for " << dst->name() << " (" << dst->type_string() << ")" << " on " << dst->assigned_device_name() << " in " << (host_memory ? "host memory" : "device memory");
VLOG(1) << "Receiving control" << " from " << src->name() << " (" << src->type_string() << ")" << " on " << src->assigned_device_name() << " for " << dst->name() << " (" << dst->type_string() << ")" << " on " << dst->assigned_device_name();
VLOG(1) << "Send/Recv control: " << src->assigned_device_name() << "[" << src->name() << "] -> " << dst->assigned_device_name() << "[" << dst->name() << "]";
VLOG(1) << "Added send/recv: controls=" << num_control << ", data=" << num_data;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/graph/graph_constructor.cc
LOG(WARNING) << "Node '" << node->name() << "' has " << node->num_outputs() << " outputs but the " << kAttrName << " attribute specifies shapes for " << shape_attrs.size() << " outputs. Output shapes may be inaccurate.";
LOG(WARNING) << "Cycle detected:";
LOG(WARNING) << SummarizeNodeDef(get_node_def(*iter));
LOG(WARNING) << "End of cycle";
LOG(WARNING) << "IN " << __func__ << " " << (node_def_count() - processed) << " NODES IN A CYCLE";
LOG(WARNING) << "PENDING: " << SummarizeNodeDef(get_node_def(i)) << " WITH PENDING COUNT = " << pending_count_[i];
VLOG(2) << "Add back edge: " << src_node->name() << " -> " << e.dst_node->name();
LOG(WARNING) << "Importing a graph with a lower producer version " << gdef.versions().producer() << " into an existing graph with producer version " << refiner->graph_def_version() << ". Shape inference will " << "have run different parts of the graph with different " << "producer versions.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/graph/mkl_tfconversion_pass.cc
VLOG(1) << "MklToTfConversionPass: Inserting Conversion node on: " << src->type_string() << " and " << dst->type_string() << " successful.";
VLOG(1) << "MklToTfConversionPass - InputConversion: Inserting input " << "conversion node on: " << n->type_string() << " successful.";
VLOG(1) << "MklToTfConversionPass: InsertConversionNodes: " << src->type_string() << " and " << dst->type_string();
VLOG(1) << "MklToTfConversionPass: Scheduled nodes " << src->name() << " and " << dst->name() << " for inserting conversion nodes";
VLOG(1) << "MklToTfConversionPass: Inserted conversion " << "node on edge between " << src_name << " and " << dst_name;
VLOG(1) << "Before running MklToTfConversionPass - InputConversion";
VLOG(1) << "MklToTfConversionPass: InputConversion: Scheduled node " << n->name() << " for inserting input conversion node";
VLOG(1) << "MklToTfConversionPass: Inserted conversion " << "on node " << n->name();
VLOG(2) << "TF-MKL: Disabling MKL";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/graph/algorithm.cc
VLOG(2) << "Reverse reach : " << n->name() << " from " << in->name();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/common_runtime/base_collective_executor.cc
LOG(FATAL) << "Unsupported type " << DataTypeString(output->dtype()) << " to MakeCollectiveAdapter";
VLOG(1) << "BaseCollectiveExecutor::StartAbort " << s;
VLOG(2) << "CreateCollective type " << DataTypeString(col_params.instance.data_type) << " name " << col_params.instance.impl_details.collective_name;
VLOG(1) << "Collective " << col_params.ToString() << " blocked by instance " << instance;
VLOG(1) << "Unblocking collective " << col_params.ToString();
VLOG(1) << "Unblocking dependencies for collective instance " << col_params.instance.instance_key;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/common_runtime/local_device.cc
LOG(ERROR) << "OverrideGlobalThreadPool: " << status.error_message();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/common_runtime/rendezvous_mgr.cc
VLOG(1) << "IntraProcessRendezvous Recv " << local << " " << parsed.FullKey();
VLOG(1) << "IntraProcessRendezvous Send " << this << " " << key.FullKey();
VLOG(1) << "IntraProcessRendezvous Recv " << this << " " << key.FullKey();
DVLOG(1) << "IntraProcessRendezvous Send " << this << " " << key.FullKey();
DVLOG(1) << "StackAllocatedIntraProcessRendezvous Recv " << this << " " << key.FullKey();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/common_runtime/graph_runner.cc
VLOG(1) << "Cannot run on: " << device_->device_type() << " with a function library for a " << function_library->device()->device_type() << " device.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/common_runtime/hierarchical_tree_broadcaster.cc
LOG(FATAL) << "Unexpected device rank " << device_rank << " for " << task_hi << " devices";
VLOG(2) << "Sorted task names: " << absl::StrJoin(col_params->instance.task_names, ", ");
VLOG(2) << "HierarchicalTreeBroadcaster::InitializeCollectiveParams device=" << device_name << " source_rank=" << col_params->source_rank << " dev_per_task=" << dpt_buf;
VLOG(2) << collective_util::SubdivPermDebugString(*col_params);
VLOG(1) << "Running Broadcast tree device=" << col_ctx_->device_name << " subdiv=" << si << " perm=" << subdiv_buf << " my_rank=" << my_rank << " source_rank=" << source_rank;
VLOG(2) << "copying input to output for device=" << col_ctx_->device_name << " subdiv=" << si;
VLOG(2) << "device=" << col_ctx_->device_name << " return status " << status_;
VLOG(3) << "DispatchSend " << send_buf_key << " from_device " << col_ctx_->device_name << " to_device " << col_params_->instance.device_names[dst_idx] << " subdiv=" << subdiv << " dst_rank=" << dst_rank << " dst_idx=" << dst_idx;
VLOG(3) << "DispatchRecv " << recv_buf_key << " from_device " << col_params_->instance.device_names[src_idx] << " to_device " << col_ctx_->device_name << " subdiv=" << subdiv << " src_rank=" << src_rank << " src_idx=" << src_idx;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/common_runtime/bfc_allocator.cc
VLOG(1) << "Creating bin of max chunk size " << strings::HumanReadableNumBytes(bin_size);
VLOG(2) << "Number of regions allocated: " << region_manager_.regions().size();
VLOG(1) << "Extending allocation by " << strings::HumanReadableNumBytes(bytes) << " bytes.";
VLOG(1) << "Total allocated bytes: " << strings::HumanReadableNumBytes(total_region_allocated_bytes_);
VLOG(1) << "Allocated memory at " << mem_addr << " to " << static_cast<void*>(static_cast<char*>(mem_addr) + bytes);
VLOG(1) << "AllocateRaw " << Name() << " " << num_bytes;
LOG(WARNING) << "Allocator (" << Name() << ") ran out of memory trying " << "to allocate " << strings::HumanReadableNumBytes(num_bytes) << " with freed_by_count=" << freed_by_count << ". The caller indicates that this is not a failure, but" << " may mean that there could be performance gains if more" << " memory were available.";
VLOG(2) << "Found free region with ptr = " << region.ptr();
LOG(WARNING) << "Garbage collection: deallocate free memory regions" << " (i.e., allocations) so that we can re-allocate a larger" << " region to avoid OOM due to memory fragmentation. If you" << " see this message frequently, you are running near the" << " threshold of the available device memory and re-allocation" << " may incur great performance overhead. You may try smaller" << " batch sizes to observe the performance impact." << " Set TF_ENABLE_GPU_GARBAGE_COLLECTION=false if you'd like to" << " disable this feature.";
VLOG(2) << "tried to allocate 0 bytes";
LOG(WARNING) << "Allocator (" << Name() << ") ran out of memory trying " << "to allocate " << strings::HumanReadableNumBytes(num_bytes) << " (rounded to " << rounded_bytes << ")" << op_ident << "Current allocation summary follows.";
LOG(WARNING) << RenderOccupancy();
LOG(INFO) << "missing pending_op_name for " << Name() << " reading addr " << static_cast<const void*>(&pending_op_name) << "" << CurrentStackTrace();
VLOG(4) << "Returning: " << chunk->ptr;
LOG(INFO) << "A: " << RenderOccupancy();
VLOG(1) << "DeallocateRaw " << Name() << " " << (ptr ? RequestedSize(ptr) : 0);
VLOG(2) << "tried to deallocate nullptr";
LOG(INFO) << "F: " << RenderOccupancy();
VLOG(4) << "Merging c->next " << n->ptr << " with c " << c->ptr;
VLOG(4) << "Merging c " << c->ptr << " into c->prev " << n->ptr;
VLOG(1) << "MergeTimestampedChunks queue_len=" << timestamped_chunks_.size() << " required_bytes=" << required_bytes;
LOG(INFO) << "BFCAllocator dump for " << Name();
LOG(INFO) << "Bin (" << b->bin_size << "): Total Chunks: " << bin_info.total_chunks_in_bin << ", Chunks in use: " << bin_info.total_chunks_in_use << ". " << strings::HumanReadableNumBytes(bin_info.total_bytes_in_bin) << " allocated for chunks. " << strings::HumanReadableNumBytes(bin_info.total_bytes_in_use) << " in use in bin. " << strings::HumanReadableNumBytes( bin_info.total_requested_bytes_in_use) << " client-requested in use in bin.";
LOG(INFO) << "Bin for " << strings::HumanReadableNumBytes(num_bytes) << " was " << strings::HumanReadableNumBytes(b->bin_size) << ", Chunk State: ";
LOG(INFO) << c->DebugString(this, true);
LOG(INFO) << "Next region of size " << region.memory_size();
LOG(INFO) << buf;
LOG(INFO) << " Summary of in-use Chunks by size: ";
LOG(INFO) << it.second << " Chunks of size " << it.first << " totalling " << strings::HumanReadableNumBytes(it.first * it.second);
LOG(INFO) << "Sum Total of in-use chunks: " << strings::HumanReadableNumBytes(total_bytes);
LOG(INFO) << "total_region_allocated_bytes_: " << total_region_allocated_bytes_ << " memory_limit_: " << memory_limit_ << " available bytes: " << (memory_limit_ - total_region_allocated_bytes_) << " curr_region_allocation_bytes_: " << curr_region_allocation_bytes_;
LOG(INFO) << "Stats: " << stats_.DebugString();
LOG(ERROR) << "Failed to open file " << file_name;
LOG(ERROR) << "Error on writing to file " << gpu_memory_map_file << ": " << status;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/common_runtime/lower_case_op.cc
VLOG(2) << "Lower Case node (keep_node_fetchable=" << keep_node_fetchable << "): " << SummarizeNode(*n);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/common_runtime/accumulate_n_optimizer.cc
VLOG(3) << "Rewrite AccumulateNV2 into TemporaryVariable and Assign: " << SummarizeNode(*n);
VLOG(3) << "Rewrite AccumulateNV2 into AddN: " << SummarizeNode(*n);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/common_runtime/function.cc
LOG(ERROR) << "Could not find Handle: " << h << " on device: " << device_name_;
VLOG(2) << "Custom creator error: " << s;
VLOG(1) << "Not instantiating function in FLR because failed to " << "find device " << options.target << " in device manager";
VLOG(1) << "Not instantiating function in FLR because target device " << options.target << " is different from FLR's device: " << device_->DebugString();
VLOG(2) << "Graph " << label << " #nodes " << g->num_nodes() << " #edges " << g->num_edges();
VLOG(5) << "|| " << line;
VLOG(2) << "Pruning function body: function_name=" << fdef.signature().name();
VLOG(2) << "Removing dead nodes";
VLOG(2) << "Removing identity nodes";
VLOG(2) << "Remove Identity: " << n->DebugString();
VLOG(2) << "Removing list array converter";
LOG(ERROR) << "RemoveListArrayConverter unexpected duplicated input: " << e->dst_input();
LOG(ERROR) << "RemoveListArrayConverter unexpected missing input: " << e->src_output();
VLOG(4) << "Function instantiation input devices:";
VLOG(4) << " [index " << i << "]" << " device: " << input_devices[i] << " (input: " << input_tensors[i] << ")";
VLOG(3) << "Create default placer for inlined function body.";
VLOG(3) << "Create single device placer for inlined function body.";
VLOG(3) << "Create multi device placer for inlined function body.";
VLOG(3) << "Inline function call: " << SummarizeNode(*caller) << " [" << options.DebugString() << "]";
VLOG(3) << "Created input control node: " << input_control_node->name();
VLOG(4) << "Add control edge from input control node to: " << clone->name();
VLOG(4) << "Add input Identity nodes for each function argument:";
VLOG(4) << " [index " << i << "] " << fbody->fdef.signature().input_arg(i).name() << " as " << n->name() << " (input: " << inputs[i].name() << ", requested_device: " << n->requested_device() << ")";
VLOG(4) << "Add output Identity nodes for each function output argument:";
VLOG(4) << " [index " << i << "] " << fbody->fdef.signature().output_arg(i).name() << " as " << n->name() << " (ret: " << data.node->name() << ":" << data.index << ", requested_device: " << n->requested_device() << ")";
VLOG(4) << "Add output control node: " << output_control_node->name();
VLOG(4) << " [data output] add control edge from: " << n->name();
VLOG(4) << " [control output] add control edge from: " << n->name();
VLOG(4) << "Add add a control edge between input and output control nodes: " << input_control_node->name() << " to " << output_control_node->name();
VLOG(4) << "Function inlining potentially dropped execution frame " "information from outgoing control edges.";
VLOG(3) << "Successfully inlined function call node: " << caller->name();
VLOG(3) << "noinline: " << SummarizeNode(*node);
LOG(ERROR) << "Failed to instantiate a function: " << s.error_message();
VLOG(1) << "Failed to inline function call: node=" << p.first->name() << " error=" << inlined.error_message();
LOG(WARNING) << "Malformed graph node. multiple input edges: " << n->DebugString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/common_runtime/executor.cc
VLOG(2) << "Failed to find expected ScopedAllocator attr on " << use_node->name();
VLOG(2) << "node " << n->name() << " is the sink of an RPC in";
VLOG(2) << "node " << n->name() << " is the sink of a gpu->cpu copy";
VLOG(2) << "default alloc case local type " << local_dev_name.type << " remote type " << parsed_src_name.type;
VLOG(2) << "node " << n->name() << " is the source of an RPC out";
VLOG(2) << "node " << n->name() << " is the source of a cpu->gpu copy";
VLOG(2) << "default alloc case local type " << local_dev_name.type << " remote type " << parsed_dst_name.type;
LOG(WARNING) << " Iteration:";
VLOG(1) << "Process node: " << id << " step " << params.step_id << " " << SummarizeNodeDef(item.kernel->def()) << (tagged_node.is_dead ? " is dead" : "") << " device: " << device->name();
VLOG(2) << "Async kernel done: " << state->item->node_id << " step " << step_id_ << " " << SummarizeNodeDef(state->item->kernel->def()) << (state->tagged_node.is_dead ? " is dead" : "") << " device: " << device->name();
VLOG(2) << "Synchronous kernel done: " << id << " step " << params.step_id << " " << SummarizeNodeDef(item.kernel->def()) << (tagged_node.is_dead ? " is dead: " : "") << " device: " << device->name();
LOG(WARNING) << this << " Compute status: " << s;
VLOG(1) << "[" << device_name << "] Executor start aborting: " << s;
LOG(WARNING) << " Pending Node: " << node_item.DebugString();
LOG(WARNING) << " Input " << i << ": " << strings::StrCat( "Tensor<type: ", DataTypeString(tensor->dtype()), " shape: ", tensor->shape().DebugString(), ">");
LOG(WARNING) << " Input " << i << ": not present";
LOG(WARNING) << " Active Node: " << node_item.DebugString();
LOG(WARNING) << " Input " << i << ": " << strings::StrCat( "Tensor<type: ", DataTypeString(tensor->dtype()), " shape: ", tensor->shape().DebugString(), ">");
LOG(WARNING) << " Input " << i << ": not present";
LOG(WARNING) << " Input " << i << ": " << strings::StrCat( "Tensor<type: ", DataTypeString(tensor->dtype()), " shape: ", tensor->shape().DebugString(), ", bytes: ", tensor->TotalBytes(), ">");
LOG(WARNING) << " Total bytes " << total_bytes;
LOG(WARNING) << "Dumping state";
LOG(WARNING) << frame.first;
VLOG(2) << "Create frame: " << child_name;
VLOG(2) << "Delete frame " << frame_name;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/common_runtime/session_factory.cc
LOG(ERROR) << "Two session factories are being registered " << "under" << runtime_type;
VLOG(2) << "SessionFactory type " << session_factory.first << " accepts target: " << options.target;
VLOG(2) << "SessionFactory type " << session_factory.first << " does not accept target: " << options.target;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/common_runtime/executor_factory.cc
LOG(FATAL) << "Two executor factories are being registered " << "under" << executor_type;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/common_runtime/ring_alg.cc
VLOG(2) << "num_subdivs " << num_subdivs << " num_chunks " << num_chunks << " chunk_size " << chunk_size;
VLOG(2) << "Dynamically generated " << num_subdivs << " subdiv_offsets:" << subdiv_buf << " tensor_size " << tensor_size << " chunk_size " << chunk_size;
VLOG(2) << "Sorted task names: " << absl::StrJoin(col_params->instance.task_names, ", ");
VLOG(2) << "Setting up perms for col_params " << col_params << " subdiv_permutations " << &col_params->instance.impl_details.subdiv_permutations;
VLOG(2) << collective_util::SubdivPermDebugString(*col_params);
LOG(ERROR) << "Aborting Ring" << name_ << " with " << s;
VLOG(2) << this << " InitRingField " << rf->DebugString() << " chunk " << ca_->TBounds(rf->chunk);
VLOG(3) << "IncrRingField old value " << rf->DebugString();
VLOG(3) << "IncrRingField new value " << rf->DebugString();
VLOG(3) << "DispatchSend rank=" << col_params_->default_rank << " send key " << send_buf_key << " chunk " << ca_->TBounds(rf->chunk) << " sc_idx " << rf->sc_idx;
VLOG(3) << "DispatchRecv rank=" << col_params_->default_rank << " recv key " << recv_buf_key << " chunk " << ca_->TBounds(rf->chunk) << " into " << ((col_params_->merge_op != nullptr) ? "tmp_chunk" : "chunk");

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/common_runtime/threadpool_device_factory.cc
LOG(INFO) << "Only " << num_numa_nodes << " NUMA nodes visible in system, " << " assigning device " << name << " to NUMA node " << numa_node;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/common_runtime/graph_execution_state.cc
VLOG(4) << "Graph proto is " << graph_def.DebugString();
VLOG(2) << "Saving " << n->DebugString();
VLOG(2) << "Restored " << n->DebugString();
VLOG(2) << "Mapping " << n->name() << " to " << n->cost_id();
VLOG(3) << added_device.error_message();
VLOG(3) << "Grappler available devices: " << absl::StrJoin(item.devices(), ", ");
VLOG(3) << "Replace function: name=" << func_name;
VLOG(3) << "Add new function: name=" << func_name;
VLOG(1) << "BuildGraph";
VLOG(2) << "Grappler optimization failed. Error: " << s.error_message();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/common_runtime/dynamic_device_mgr.cc
VLOG(1) << "Unknown device: " << name << " all devices: " << absl::StrJoin(device_names, ", ");
LOG(WARNING) << s;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/common_runtime/collective_util.cc
LOG(ERROR) << "Failed to find device " << device_name;
LOG(ERROR) << "Available devices " << d->name();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/common_runtime/pool_allocator.cc
LOG(INFO) << "PoolAllocator: After " << alloc_request_count << " get requests, put_count=" << put_count_ << " evicted_count=" << evicted_count_ << " eviction_rate=" << eviction_rate << " and unsatisfied allocation rate=" << alloc_rate;
LOG(INFO) << "Raising pool_size_limit_ from " << pool_size_limit_ << " to " << new_size_limit;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/common_runtime/optimization_registry.cc
VLOG(1) << "Running optimization phase " << phase.first;
VLOG(1) << "Running optimization pass: " << pass->name();
VLOG(vlog_level) << "Registered optimization pass grouping " << grouping << " phase " << phase.first << ": " << pass->name();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/common_runtime/collective_param_resolver_local.cc
VLOG(1) << "CompleteGroupLocal device=" << device << " cp: " << cp << ": " << cp->ToString();
VLOG(2) << "New group_key=" << gr->group.group_key << " group_size=" << gr->group.group_size << " runtime_details=" << gr->group.runtime_details.ToString();
VLOG(2) << "gr device_type=" << gr->group.device_type << " cp device_type=" << cp->group.device_type << " current device=" << device;
VLOG(1) << "CompleteGroupLocal group_key=" << gr->group.group_key << " group_size=" << gr->group.group_size << " (current" << " devices)=(" << dev_buf << ") (number of" << " devices pending)=" << (gr->group.group_size - gr->device_set.size());
VLOG(2) << "group_size " << gr->group.group_size << " set size " << gr->device_set.size() << " gr " << gr;
VLOG(2) << "Assigned local ranks based on ring order " << gpu_ring_order_str;
VLOG(1) << "EstablishGlobalRank";
VLOG(1) << "SortDevicesAndTasks " << cp << " instance " << &cp->instance;
VLOG(1) << "Modified device_names on " << cp;
VLOG(2) << "Initialized names for instance: " << ir->shared.instance.ToString();
VLOG(2) << "Optimized device order for " << ir->shared.name << ": " << buf;
VLOG(1) << "CompleteParams local " << device << " for " << cp << ": " << cp->ToString();
VLOG(1) << "AssignCollectiveType " << cp->instance.impl_details.collective_name;
VLOG(1) << "CompleteInstanceLocal " << device << " instance_key: " << cp->instance.instance_key << " gr " << gr;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/common_runtime/colocation_graph.cc
LOG(INFO) << "Ignoring device specification " << DeviceNameUtils::ParsedNameToString(requested_device_name_) << " for node '" << dst.name() << "' because the input edge from '" << src.name() << "' is a reference connection and already has a device " "field set to " << DeviceNameUtils::ParsedNameToString( src_root.requested_device_name_);
LOG(ERROR) << "The value for colocation attribute '_class' must be a " "list of strings, not a single string: " << node->DebugString();
VLOG(2) << "Computed IOColocationGroups for node " << node->name() << ":" << groups.DebugString();
VLOG(2) << "Colocated inputs/outputs of node: " << node.DebugString();
VLOG(2) << "[" << absl::StrJoin(NodeAndBoolToString(nodes), "") << "]";
VLOG(2) << "Colocating "" << nodes[0].first->name() << "" and "" << nodes[i].first->name() << """;
LOG(INFO) << "Ignoring request to colocate node '" << node->name() << "' with nodes in colocation group '" << colocation_group << "' because soft placement is on and an attempt at doing " "so resulted in the following error: " << AttachDef(s, *node).ToString();
LOG(WARNING) << "Failed to place the graph without changing the devices of some " "resources. Some of the operations (that had to be colocated with " "resource generating operations) are not supported on the " "resources' devices. Current candidate devices are [ " << absl::StrJoin(DevicesToString(*possible_devices), " ") << "].See below for details of this colocation group:" << DebugInfo(root_id);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/common_runtime/input_colocation_exemption_registry.cc
LOG(WARNING) << "Input colocation exemption for op: " << op << " already registered";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/common_runtime/threadpool_device.cc
LOG(FATAL) << "Unexpected call to ThreadPoolDevice::GetScopedAllocator " << "attr.scope_id = " << attr.scope_id;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/common_runtime/process_state.cc
LOG(ERROR) << "GetCPUAllocator: " << status.error_message();
LOG(ERROR) << "GetCPUAllocator: " << status.error_message();
VLOG(2) << "Using BFCAllocator with memory limit of " << cpu_mem_limit_in_mb << " MB for ProcessState CPU allocator";
VLOG(2) << "Using PoolAllocator for ProcessState CPU allocator " << "numa_enabled_=" << numa_enabled_ << " numa_node=" << numa_node;
VLOG(1) << "AddCPUAllocVisitor";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/common_runtime/lower_while_op.cc
VLOG(2) << "Lower While node (keep_node_fetchable=" << keep_node_fetchable << "): " << SummarizeNode(*n);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/common_runtime/direct_session.cc
VLOG(1) << "Direct session inter op parallelism threads for pool " << pool_number << ": " << num_threads;
LOG(ERROR) << status.error_message();
LOG(INFO) << msg;
VLOG(1) << "Executing Session::Run() synchronously!";
VLOG(1) << "Using RunHandler to scheduler inter-op closures.";
LOG(WARNING) << "An error unrelated to this prun has been detected. " << run_state->status;
VLOG(2) << "Created " << DebugString(graph->get()) << " for " << partition_name;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/common_runtime/device_mgr.cc
VLOG(1) << "Unknown device: " << name << " all devices: " << absl::StrJoin(device_names, ", ");
LOG(WARNING) << s;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/common_runtime/step_stats_collector.cc
VLOG(1) << "Save dev " << device << " node stats " << node_stats->stats();
LOG(WARNING) << "stats saved after finalize will not be collected.";
VLOG(1) << "step_stats_ nullptr or already collected too many nodes.";
VLOG(1) << "Save dev " << device << " thread id " << thread_id << " name " << thread_name;
LOG(WARNING) << "thread_name saved after finalize will not be collected.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/common_runtime/collective_rma_local.cc
VLOG(1) << "RecvFromPeer " << this << " from " << peer_device << " key " << key;
LOG(ERROR) << "Got hook " << hook << " with status " << s << " from ConsumeBuf";
VLOG(1) << "PostToPeer " << this << " key " << key << " step_id_=" << step_id_;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/common_runtime/placer.cc
LOG(INFO) << "Wrote ColocationGraph to " << fname;
LOG(ERROR) << "Failed to write final colocation graph to file " << fname << " with " << status.ToString();
LOG(INFO) << node->name() << ": " << "(" << node->type_string() << "): " << node->assigned_device_name();
VLOG(5) << " " << node->name() << ": requested: '" << node->requested_device() << "' assigned: '" << node->assigned_device_name() << "'";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/common_runtime/constant_folding.cc
VLOG(1) << "Node " << n->name() << " has input shape dimension " << i << " of " << shape.dim_size(i) << " but type INT32 " << " so not replacing as constant: this will trigger a " "runtime error later.";
VLOG(1) << "Node " << n->name() << " has input shape size " << size << " but type INT32 " << " so not replacing as constant: this will trigger a runtime " "error later.";
VLOG(1) << "Replacing " << tensor.first->name() << " :: " << tensor.second << " with a constant";
VLOG(1) << "No constant foldable nodes found";
VLOG(1) << "No constant nodes found that feed into the original graph.";
VLOG(1) << "Constant foldable " << constant_graph->num_node_ids() << " : " << graph->num_node_ids();
VLOG(1) << "Could not fetch constants: " << s;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/common_runtime/lower_function_call_op.cc
VLOG(2) << "Lower function call node: " << SummarizeNode(*n);
VLOG(2) << "Skip SymbolicGradient lowering";
VLOG(2) << "Failed to inline function call node: " << can_inline_function_call.error_message();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/common_runtime/scoped_allocator.cc
VLOG(1) << "~ScopedAllocator " << this << " tbuf_ " << tbuf_ << " data " << static_cast<void*>(tbuf_->data());
VLOG(1) << "expected_call_count_ = " << expected_call_count_ << " at deallocation";
VLOG(1) << "ScopedAllocator index " << id_ << " AllocateRaw " << "field " << field_index << " num_bytes " << num_bytes;
LOG(ERROR) << "Scoped allocator " << name_ << " could not satisfy request for " << num_bytes << " bytes, expected uses exhausted. ";
LOG(ERROR) << "ScopedAllocator " << name_ << " received unexpected field number " << field_index;
LOG(ERROR) << "ScopedAllocator " << name_ << " got request for " << num_bytes << " bytes from field " << field_index << " which has precalculated size " << field->bytes_requested << " and offset " << field->offset;
VLOG(2) << "AllocateRaw returning " << ptr << " bytes_requested " << field->bytes_requested << " bytes_allocated " << field->bytes_allocated;
VLOG(2) << "AllocateRaw requested " << num_bytes << " bytes which is not divisible by kAllocatorAlignment=" << Allocator::kAllocatorAlignment << " and hence we allocated " << field->bytes_allocated << ". Annotating " << extra_bytes << " bytes starting at " << extra_buf << " with TF_ANNOTATE_MEMORY_IS_INITIALIZED";
VLOG(1) << "ScopedAllocator index " << id_ << " VerifyPointer for p=" << p << " failed.";
VLOG(1) << "new ScopedAllocatorInstance " << this << " on SA " << sa << " field_index " << field_index;
VLOG(2) << "ScopedAllocatorInstance::DropFromTable " << this << " allocated_ " << allocated_ << " deallocated_ " << deallocated_ << " in_table_ " << in_table_;
VLOG(2) << "ScopedAllocatorInstance::AllocateRaw " << this << " call to underlying ScopedAllocator unsuccessful," << " allocated_ " << allocated_ << " deallocated_ " << deallocated_ << " in_table_ " << in_table_ << " returning nullptr.";
VLOG(2) << "ScopedAllocatorInstance::AllocateRaw " << this << " allocated_ " << allocated_ << " deallocated_ " << deallocated_ << " in_table_ " << in_table_ << " returning ptr = " << ptr;
VLOG(2) << "ScopedAllocatorInstance::DeallocateRaw " << this << " allocated_ " << allocated_ << " deallocated_ " << deallocated_ << " in_table_ " << in_table_;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/common_runtime/session.cc
LOG(ERROR) << s;
LOG(ERROR) << "Failed to create session: " << s;
LOG(ERROR) << s;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/common_runtime/process_function_library_runtime.cc
VLOG(1) << "Could not find device: " << device_name;
LOG(ERROR) << "Could not find device: " << device_name;
VLOG(3) << "Trying to determine device for node " << node->name() << "[T=" << DataTypeString(dtype) << "]";
VLOG(3) << "Considering src: " << src_node->name() << " src_device: " << *src_device << " colo group: " << colocation_group;
VLOG(3) << "Considering src: " << src_node->name() << " src_device: " << *src_device << " colo group: " << colocation_group;
VLOG(3) << "Considering src: " << src_node->name() << " colo group: " << colocation_group;
VLOG(3) << "Setting output device to " << matching_devices[0]->name() << " for node " << SummarizeNode(*node);
VLOG(3) << "Did not set device for a resource output node " << SummarizeNode(*node);
VLOG(3) << "Setting output device to " << output_devices[index] << " for return at index " << index;
LOG(ERROR) << "Failed to get FunctionBody for "" << function_name << """;
VLOG(1) << "Instantiating MultiDevice function "" << function_name << "" on default device "" << options.target << """;
VLOG(3) << "Requested input devices:";
VLOG(3) << " [input " << index++ << "] " << device;
VLOG(3) << "Requested output devices:";
VLOG(3) << " [output " << index++ << "] " << device;
LOG(WARNING) << "Ignoring multi-device function optimization failure: " << status.ToString();
VLOG(4) << "Main function graph to be partitioned:";
VLOG(4) << DebugString(graph->ToGraphDefDebug());
VLOG(1) << "Start instantiating component function " << unique_name << " on device " << target;
VLOG(4) << DebugString(shard);
VLOG(1) << "Finished instantiating component function " << unique_name << " with handle " << *component_handle << " status: " << s;
VLOG(2) << "Instantiated MultiDevice function "" << function_name << "" with handle " << *handle;
VLOG(1) << "Running multi-device function " << data->function_name_;
VLOG(4) << " with " << opts.DebugString();
VLOG(2) << "Failed to get component function arguments: " << s;
VLOG(1) << "Running component function on device " << target << " with handle " << handle;
VLOG(4) << " with " << opts_copy.DebugString();
VLOG(2) << "Component function execution failed: " << status;
VLOG(1) << "Running component function on device " << target << " with handle " << handle;
VLOG(4) << " with " << opts_copy.DebugString();
VLOG(2) << "Component function execution failed: " << status;
VLOG(1) << "ProcessFLR Instantiate: " << function_name << " on: " << target;
VLOG(1) << "ProcessFLR Instantiate [success]: " << function_name << " on: " << target << " with handle: " << *handle << " (this: " << this << ")";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/common_runtime/placer_inspection_required_ops_utils.cc
VLOG(6) << "Adding identity into " << edge->src()->name() << ":" << edge->src_output() << " -> " << edge->dst()->name() << ":" << input_idx << " " << identity_def.DebugString();
VLOG(6) << "Successfully inserted identity. Modified node: " << node->DebugString();
VLOG(6) << "Adding identity into " << node->name() << ":" << src_output << " -> " << dst->name() << ":" << dst_input << " " << identity_node->DebugString();
VLOG(6) << "Added identity into " << node->name() << ":" << output_idx << " -> <no consumer>: " << identity_node->DebugString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/common_runtime/scoped_allocator_mgr.cc
VLOG(1) << "AddScopedAllocator " << mgr_->device_name() << " step_id_=" << step_id_ << " scope_id=" << scope_id;
VLOG(2) << " container " << this << " step_id " << step_id_;
VLOG(2) << "#fields " << fields.size();
VLOG(2) << "Adding instance with for " << mgr_->device_name() << " scope_id=" << f.scope_id;
LOG(ERROR) << "Failed to find ScopedAllocator for " << scope_id << " in container for step " << step_id_ << " on " << mgr_->device_name();
VLOG(2) << "GetInstance " << scope_id << " step " << step_id_ << " on " << mgr_->device_name();
LOG(FATAL) << "Failed to find instance " << scope_id << " in container " << step_id_ << " on " << mgr_->device_name();
VLOG(2) << "Drop " << scope_id << " from container " << this << " step " << step_id_ << " on " << mgr_->device_name();
VLOG(2) << "~ScopedAllocatorContainer " << this << " step " << step_id_ << " on " << mgr_->device_name();
VLOG(2) << "GetContainer " << step_id << " on " << device_name();
VLOG(1) << "field=" << i << " scope_id=" << field->scope_id << " bytes_requested=" << field->bytes_requested << " offset=" << field->offset << " bytes_allocated=" << field->bytes_allocated;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/common_runtime/buf_rendezvous.cc
LOG(INFO) << strings::StrCat("BufRendezvous ", strings::Hex(reinterpret_cast<uint64>(this)), " step_id=", step_id_, " current contents:");
LOG(INFO) << it.first << ":" << it.second->DebugString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/common_runtime/copy_tensor.cc
VLOG(1) << "Copy " << edge_name;
VLOG(1) << "No function registered to copy from devices of type " << src_device_type.type() << " to devices of type " << dst_device_type.type() << ". Falling back to copying via the host.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/common_runtime/lower_if_op.cc
VLOG(2) << "Lower If node (keep_node_fetchable=" << keep_node_fetchable << "): " << SummarizeNode(*n);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/common_runtime/ring_reducer.cc
VLOG(1) << "RingReducer::Run for device " << col_ctx_->device_name << " default_rank " << col_params_->default_rank << "" << buf;
VLOG(4) << FieldState();
VLOG(2) << this << " device=" << col_ctx_->device_name << " finish;" << " final value " << TensorDebugString(ca_->Value());

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/common_runtime/memory_types.cc
VLOG(2) << "inp mvec " << n->id() << " " << i << " " << inp_mvec[i];
VLOG(2) << "out mvec " << n->id() << " " << i << " " << out_mvec[i];
VLOG(1) << e->src()->id() << ":" << e->src_output() << " -> " << e->dst()->id() << ":" << e->dst_input() << ": " << sm << " -> " << dm;
VLOG(2) << "Dumped graph after EnsureMemoryTypes to " << DumpGraphToFile("EnsureMemoryTypes", *g);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/common_runtime/device_factory.cc
LOG(FATAL) << "Duplicate registration of device factory for type " << device_type << " with the same priority " << priority;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/common_runtime/isolate_placer_inspection_required_ops_pass.cc
VLOG(1) << "Not running IsolatePlacerInspectionRequiredOpsPass because no " "graph is provided";
VLOG(1) << "IsolatePlacerInspectionRequiredOpsPass::Run";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/common_runtime/process_util.cc
VLOG(0) << "Creating new thread pool with default inter op setting: " << mkl_inter_op << ". Tune using inter_op_parallelism_threads for best performance.";
VLOG(1) << "Direct session inter op parallelism threads: " << num_threads;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/common_runtime/ring_gatherer.cc
VLOG(1) << "RingGatherer::Run for device " << col_ctx_->device_name << " default_rank " << col_params_->default_rank << "" << buf;
VLOG(4) << FieldState();
VLOG(2) << this << " device=" << col_ctx_->device_name << " finish;" << " final value " << TensorDebugString(ca_->Value());

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/common_runtime/shape_refiner.cc
LOG(WARNING) << "Function instantiation has undefined input shape at " << "index: " << index << " in the outer inference context.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc
LOG(WARNING) << "Overriding allow_growth setting because the" << " TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original" << " config value was " << gpu_options.allow_growth() << ".";
LOG(WARNING) << "Overriding allow_growth setting because the" << " TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original" << " config value was " << gpu_options.allow_growth() << ".";
LOG(ERROR) << "The TF_FORCE_GPU_ALLOW_GROWTH environment variable is set but could" << " not be parsed: "" << force_allow_growth_string << "". Valid" << " values are "true" or "false". Using original config value" << " of " << gpu_options.allow_growth() << ".";
LOG(ERROR) << "The TF_ENABLE_GPU_GARBAGE_COLLECTION environment variable is set but" << " could not be parsed: "" << enable_gpu_garbage_collection << ""." << " Valid values are "true" or "false"." << " Using the default value "true".";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/common_runtime/gpu/gpu_util.cc
VLOG(1) << "SetProtoFromGPU device_context " << device_context;
LOG(FATAL) << "SetProtoFromGPU: GPU Memcpy failed";
VLOG(2) << "src_ptr " << src_ptr << " dst_ptr " << dst_ptr;
LOG(FATAL) << "GPU->GPU Memcpy failed";
VLOG(1) << "CopyGPUTensorToCPU";
LOG(FATAL) << "GPU->CPU Memcpy failed";
VLOG(1) << "CopyCPUTensorToGPU";
LOG(FATAL) << "CPU->GPU Memcpy failed";
VLOG(1) << "GPUUtil::Sync";
VLOG(1) << "GPUUtil::SyncAll";
VLOG(1) << "CopyGPUTensorToSameGPU";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/common_runtime/gpu/gpu_debug_allocator.cc
LOG(FATAL) << "Could not copy debug mask, " << result;
LOG(ERROR) << "i=" << i << " mask=" << reinterpret_cast<const void*>(mask[i]) << " field=" << reinterpret_cast<const void*>(tmp[i]);
LOG(FATAL) << "Could not copy debug mask, " << result;
LOG(ERROR) << "Could not initialize to NaNs, " << result;
LOG(ERROR) << "Could not initialize to NaNs, " << result;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/common_runtime/gpu/gpu_process_state.cc
LOG(ERROR) << "Invalid allocator type: " << allocator_type;
LOG(INFO) << "Using memory guard allocator for GPU.";
LOG(INFO) << "Using CUDA malloc allocator for GPU.";
LOG(FATAL) << "GPUAllocator unavailable. Not compiled with --config=cuda or " "--config=rocm.";
LOG(ERROR) << "Asked for counter for GPU allocator " << tf_gpu_id.value() << " but only have " << gpu_allocators_.size();
LOG(ERROR) << "GetGpuHostAllocator: " << status.error_message();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc
LOG(WARNING) << "Executing inside EventMgr callback thread: " << CurrentStackTrace();
VLOG(2) << "QueueInUse free_events_ " << free_events_.size() << " used_events_ " << used_events_.size();
VLOG(2) << "PollEvents free_events_ " << free_events_.size() << " used_events_ " << used_events_.size();
LOG(FATAL) << "Unexpected Event status: " << static_cast<int>(s);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/common_runtime/gpu/gpu_stream_util.cc
VLOG(1) << "AssignStreams";
VLOG(2) << "Node " << node_id << " " << n->type_string() << " " << n->name() << " " << n->in_edges().size() << " inputs";
VLOG(2) << " Edge from " << e->src()->id() << " " << e->src()->name() << " fanout " << e->src()->out_edges().size();
VLOG(3) << "Inspecting node " << n->DebugString();
VLOG(1) << "Identified " << highest_stream_id << " candidate streams for " << order.size() << " nodes.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/common_runtime/gpu/gpu_init.cc
LOG(FATAL) << "Could not find Platform with name " << GpuPlatformName();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/common_runtime/gpu/gpu_cudamalloc_allocator.cc
LOG(ERROR) << "cuMemAlloc failed to allocate " << num_bytes;
LOG(ERROR) << "cuMemFree failed to free " << ptr;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc
LOG(FATAL) << "EigenAllocator for GPU ran out of memory when allocating " << num_bytes << ". See error logs for more detailed info.";
VLOG(2) << "Created stream[" << stream_group_within_gpu << "] = " << group->compute;
VLOG(2) << "Created nccl_stream[" << stream_group_within_gpu << "] = " << group->nccl;
VLOG(2) << "Created host_to_device_stream[" << stream_group_within_gpu << "] = " << group->host_to_device;
VLOG(2) << "Created device_to_host_stream[" << stream_group_within_gpu << "] = " << group->device_to_host;
LOG(ERROR) << "Illegal GPUOptions.experimental.num_dev_to_dev_copy_streams=" << num_d2d_streams << " set to 1 instead.";
VLOG(2) << "Created device_to_device_stream[" << stream_group_within_gpu << "] = " << group->device_to_device.back();
LOG(WARNING) << error_message;
VLOG(1) << "GpuDevice::ComputeHelper " << ComputeOpKernelDebugString(*op_kernel, stream_id);
VLOG(1) << "GpuDevice::ComputeHelper finished " << ComputeOpKernelDebugString(*op_kernel, stream_id);
VLOG(1) << "GpuDevice::ComputeHelper scheduled " << ComputeOpKernelDebugString(*op_kernel, stream_id);
VLOG(1) << "GpuDevice::ComputeHelper failed to schedule " << ComputeOpKernelDebugString(*op_kernel, stream_id);
VLOG(1) << "GpuDevice::ComputeAsync " << op_kernel->name() << " op " << op_kernel->type_string() << " on GPU" << tf_gpu_id_ << " stream[" << stream_id << "]";
VLOG(5) << "available_memory = " << available_memory;
VLOG(5) << "min_system_memory = " << min_system_memory;
VLOG(1) << " eigen_gpu_device(" << dc << ") => stream[" << stream_id << "]";
LOG(FATAL) << "Unexpected call to BaseGPUDevice::GetScopedAllocator " << "attr.scope_id = " << attr.scope_id;
VLOG(1) << "TensorFlow compiled with CUDA " << cuda_major_version << "." << cuda_minor_version << " and cuDNN " << CUDNN_MAJOR << "." << CUDNN_MINOR << "." << CUDNN_PATCHLEVEL;
LOG(INFO) << "Device interconnect " << im.name << " with strength " << im.strength << " edge matrix:";
LOG(INFO) << line_buf;
LOG(INFO) << line_buf;
LOG(INFO) << "Created TensorFlow device (" << device_name << " with " << (bytes_limit >> 20) << " MB memory) -> physical GPU (" << GetShortDeviceDescription(platform_gpu_id, *desc) << ")";
LOG(INFO) << "Could not identify NUMA node of platform GPU id " << platform_gpu_id << ", defaulting to 0. Your kernel may not have been built " << "with NUMA support.";
VLOG(1) << "GPUDevice PlatformGpuId " << platform_gpu_id << " TfGpuId " << tf_gpu_id << " on bus " << dev_locality.bus_id() << " numa: " << numa_node << " pci: " << desc->pci_bus_id() << " DeviceLocality: " << dev_locality.DebugString();
LOG(ERROR) << "Invalid minimum GPU multiprocessor count: [" << tf_min_gpu_core_count << "]. " << "Using the default value: " << count;
LOG(WARNING) << "Unable to enable peer access between device ordinals " << platform_gpu_i << " and " << platform_gpu_j << ", status: " << status;
LOG(INFO) << "Found device " << i << " with properties: " << "pciBusID: " << description->pci_bus_id() << " name: " << description->name() << " computeCapability: " << cc_major << "." << cc_minor << "coreClock: " << description->clock_rate_ghz() << "GHz" << " coreCount: " << description->core_count() << " deviceMemorySize: " << strings::HumanReadableNumBytes( description->device_memory_size()) << " deviceMemoryBandwidth: " << strings::HumanReadableNumBytes(description->memory_bandwidth()) << "/s";
LOG(INFO) << "Found device " << i << " with properties: " << "pciBusID: " << description->pci_bus_id() << " name: " << description->name() << " ROCm AMD GPU ISA: gfx" << isa_version << "coreClock: " << description->clock_rate_ghz() << "GHz" << " coreCount: " << description->core_count() << " deviceMemorySize: " << strings::HumanReadableNumBytes( description->device_memory_size()) << " deviceMemoryBandwidth: " << strings::HumanReadableNumBytes(description->memory_bandwidth()) << "/s";
LOG(WARNING) << "Cannot dlopen some GPU libraries. Please make sure the " "missing libraries mentioned above are installed properly " "if you would like to use GPU. Follow the guide at " "https://www.tensorflow.org/install/gpu for how to " "download and setup the required libraries for your " "platform.Skipping registering " "GPU devices...";
LOG(INFO) << "Ignoring visible gpu device " << visible_gpu_id << " whose executor is in invalid state: " << description_status.status().ToString();
LOG(INFO) << "Ignoring visible gpu device " << "(" << GetShortDeviceDescription(visible_gpu_id, *desc) << ") " << "whose CUDA compute capability is not available.";
LOG(INFO) << "Ignoring visible gpu device " << "(" << GetShortDeviceDescription(visible_gpu_id, *desc) << ") " << "with Cuda compute capability " << device_capability << ". The minimum required Cuda capability is " << min_supported_capability << ".";
LOG(INFO) << "Ignoring visible gpu device " << "(" << GetShortDeviceDescription(visible_gpu_id, *desc) << ") " << "with AMDGPU ISA gfx" << device_isa << ". The minimum required AMDGPU ISA is gfx" << min_supported_isa << ".";
LOG(INFO) << "Ignoring visible gpu device " << "(" << GetShortDeviceDescription(visible_gpu_id, *desc) << ") " << "with core count: " << desc->core_count() << ". The minimum required count is " << min_gpu_core_count << ". You can adjust this requirement with the env var " "TF_MIN_GPU_MULTIPROCESSOR_COUNT.";
LOG(INFO) << "Adding visible gpu devices: " << absl::StrJoin(raw_ids, ", ");
VLOG(2) << "kernel: " << ctx->op_kernel().name() << " mem_used: " << mem_used;
VLOG(2) << "RecordQueued queued_count=" << queued_count << " first_available_=" << first_available_ << " last_completed_=" << last_completed_ << " num_pending_=" << num_pending_;
VLOG(1) << "last_completed_=" << last_completed_ << " first_available_=" << first_available_ << " num_pending_=" << num_pending_;
VLOG(2) << this << " RecordTerminated queued_count=" << queued_count << " first_available_=" << first_available_ << " last_completed_=" << last_completed_ << " num_pending_=" << num_pending_ << " LC=" << ((last_completed_ >= 0) ? pending_kernels_[last_completed_].queued_count : -1);
LOG(FATAL) << "Failed to find " << queued_count // Crash OK << " in queue, last_completed_=" << last_completed_ << " index=" << index << " first_available_=" << first_available_ << " pending_kernels_.size()=" << pending_kernels_.size();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/common_runtime/eager/kernel_and_device.cc
LOG(INFO) << "Ignoring error status when releasing multi-device function " "handle " << status.ToString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/common_runtime/eager/eager_executor.cc
DVLOG(3) << "Add node [id " << item->id << "]" << item->node->DebugString() << " with status: " << status_.ToString();
DVLOG(3) << "Wait for Node: [id " << last_id << "] ";
DVLOG(3) << "Node Done: [id " << item->id << "] " << item->node->DebugString() << " with status: " << status.ToString();
DVLOG(3) << "Notify node done: [id " << id << " to " << upperbound_id << "] ";
VLOG(1) << "Failed to run item: " << status;
DVLOG(3) << "Running Node: [id " << item->id << "] " << item->node->DebugString();
DVLOG(3) << "Add Node: [id " << item->id << "] to unfinished map.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/common_runtime/eager/context.cc
LOG(ERROR) << "Unable to close remote context with ID " << context_id << " for worker: " << worker << " due to " << s.error_message();
LOG(WARNING) << "Unable to destroy server_ object, so releasing instead. " "Servers don't support clean shutdown.";
LOG(ERROR) << "Failed to register function remotely due to " << status.error_message() << "This shouldn't happen, please file a bug to " "tensorflow team.";
LOG(ERROR) << "Failed to register function remotely due to " << s.error_message() << "This shouldn't happen, please file a bug to " "tensorflow team.";
LOG(ERROR) << "Cleared remote executor on " << target << " with status: " << status.error_message();
LOG(WARNING) << "Unable to destroy server_ object, so releasing instead. " "Servers don't support clean shutdown.";
VLOG(1) << "Setting device filters for " << remote_worker << ":";
VLOG(1) << " " << filter;
LOG(WARNING) << "Unable to destroy server_ object, so releasing instead. " "Servers don't support clean shutdown.";
LOG(WARNING) << "Keep-alive thread was unable to find " "a client for target " << worker << ". Got error: " << s;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/common_runtime/eager/tensor_handle.cc
DVLOG(3) << "Creating Local TensorHandle: " << this << " device: " << VariantDeviceDebugString(device_);
DVLOG(3) << "Creating Local TensorHandle: " << this << " device: " << VariantDeviceDebugString(device_);
DVLOG(3) << "Creating Local TensorHandle: " << this << " custom device: " << VariantDeviceDebugString(device_);
DVLOG(3) << "Creating empty Local TensorHandle: " << this << " device: " << VariantDeviceDebugString(device_);
DVLOG(3) << "Creating Remote TensorHandle: " << this << " device: " << VariantDeviceDebugString(device_);
DVLOG(3) << "Creating Unshaped Remote TensorHandle: " << this << " device: " << VariantDeviceDebugString(device_);
DVLOG(3) << "SetRemoteShape on TensorHandle: " << this << " device: " << d;
DVLOG(3) << "SetTensor on TensorHandle: " << this;
DVLOG(3) << "Poison on TensorHandle: " << this;
LOG(ERROR) << "Cannot find resource device: " << handle.device() << ".";
DVLOG(1) << "Calling TensorHandle::DebugString() on " << this;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/common_runtime/eager/execute.cc
LOG(WARNING) << "before computing " << op->Name() << " input #" << i << " was expected to be on " << expected_input_device->name() << " but is actually on " << handle_device->name() << " (operation running on " << op_device_name << "). This triggers a copy which can be a performance " "bottleneck.";
DVLOG(2) << "Caller explicitly requested " << (*compile_with_xla ? "" : "not ") << "to compile with XLA: " << op->DebugString();
DVLOG(2) << "Function definition explicitly specifies " << (*compile_with_xla ? "" : "not ") << "to compile with XLA";
DVLOG(2) << "Compiling " << op->Name() << " with XLA because it is running on an XLA device " << op->GetDeviceParsedName().type;
DVLOG(2) << "Creating new kernel for " << op->Name() << " on device " << DeviceNameOrUnspecified(op->Device());
DVLOG(1) << "Placer place op [" << op->Name() << "] on device: " << device->name();
DVLOG(4) << "Available kernels for " << op->Name() << "are " << KernelsRegisteredForOp(op->Name());
LOG(INFO) << msg;
DVLOG(2) << "Running " << ndef.op() << " using multi-device function. " << "Full node_def=" << ndef.DebugString();
DVLOG(2) << "Running " << ndef.op() << " using op kernel. " << ". Full node_def=" << ndef.DebugString();
DVLOG(4) << "Execute remote eager op: " << op->Name() << " (is async?: " << executor.Async() << ").";
DVLOG(2) << "for op " << op->Name() << " input " << i << " " << DataTypeString(tensor_handle->dtype) << " input device = " << resource_device->name() << ", op device = " << op_device->name();
DVLOG(1) << (resource_device != op_device ? "Changing " : "Setting ") << "device of operation " << op->Name() << " to " << resource_device->name() << " because input #" << i << " is a resource in this device.";
DVLOG(2) << "for op " << op->Name() << " input " << i << " " << DataTypeString(tensor_handle->dtype) << " input device = " << input_device->name() << ", op device = " << op_device->name();
DVLOG(1) << "Forcing op " << op->Name() << " to be on the CPU since all input tensors have an " "int32/int64 dtype, and are small (less than 64 elements).";
LOG(INFO) << msg;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/common_runtime/eager/eager_operation.cc
VLOG(1) << "EagerOperation::DebugString() over " << this;
VLOG(1) << "Input ptr: " << input;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/api_def/update_api_def.cc
LOG(ERROR) << "Didn't find doc start";
LOG(ERROR) << "Didn't find doc start";
LOG(ERROR) << "Invalid doc: " << doc_text;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/grappler/mutable_graph_view.cc
VLOG(3) << "Add new function definition: " << fdef.signature().name();
VLOG(2) << absl::Substitute("Update fanouts from '$0' to '$1'.", from_node->name(), to_node->name());
VLOG(2) << absl::Substitute("Attempting to delete missing node(s) [$0].", sort_and_sample(&missing_nodes));

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/grappler/devices.cc
LOG(INFO) << "Number of eligible GPUs (core count >= 8, compute capability >= " << min_cuda_compute_capability.first << "." << min_cuda_compute_capability.second << "): " << num_eligible_gpus;
LOG(INFO) << "Number of eligible GPUs (core count >= 8, compute capability >= " << min_cuda_compute_capability.first << "." << min_cuda_compute_capability.second << "): " << num_eligible_gpus << " (Note: TensorFlow was not compiled with CUDA support)";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/grappler/utils.cc
LOG(WARNING) << "Duplicated node in the graph: " << node_name;
VLOG(1) << "Node could not be found: " << name;
LOG(ERROR) << "Node not found: " << current->input(0);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/grappler/op_types.cc
LOG(WARNING) << "Failed to lookup OpDef for " << op_name << ". Error: " << status.error_message();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/grappler/graph_topology_view.cc
VLOG(0) << "Skip error: " << error_message;
VLOG(0) << "Skip error: " << error_message;
VLOG(3) << "Skip error: " << error_message;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/grappler/grappler_item.cc
VLOG(1) << "Add fetch " << f;
VLOG(1) << "Add feed " << f.first;
VLOG(2) << "Inferred device set: [" << absl::StrJoin(devices_, ", ") << "]";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/grappler/grappler_item_builder.cc
LOG(ERROR) << "id must be non-empty.";
LOG(ERROR) << "Invalid shape for signature input " << input.name() << ": " << s << ", skipping this input";
LOG(ERROR) << "Invalid feed node name skipping this input";
VLOG(1) << "Will use feed node " << feed.first;
LOG(ERROR) << "Invalid fetch node name skipping this input";
VLOG(1) << "Will use fetch node " << fetch;
LOG(ERROR) << "Failed to detect the fetch node(s), skipping this input";
LOG(ERROR) << "Failed to parse AssetFile.";
LOG(ERROR) << "Can't access one or more of the asset files " << asset_filepath << ", skipping this input";
LOG(ERROR) << "Can't parse AssetFileDef on mobile.";
LOG(ERROR) << "Can't access one or more of the asset files, skipping " "this input";
LOG(ERROR) << "Could not parse queue_runners, skipping this input";
LOG(ERROR) << "Queue without a cancel op, skipping this input";
LOG(ERROR) << "Value attribute expected in const op for asset files";
LOG(INFO) << "Unexpected AttrValue proto: " << iter->second.DebugString();
LOG(INFO) << "Using asset file " << it->second << " for node " << node.name();
LOG(ERROR) << "Failed to instantiate default attribute values: " << attr_status.error_message();
VLOG(1) << "Number of nodes in graph before RuntimeGraphOptimizer: " << new_item->graph.node_size();
LOG(ERROR) << "Graph preprocessing failed: " << optimize_status;
VLOG(1) << "Number of nodes in graph after RuntimeGraphOptimizer: " << new_item->graph.node_size();
VLOG(1) << "Pruning graph...";
LOG(ERROR) << "Pruning failed: " << status.error_message();
VLOG(1) << "Number of nodes in graph after pruning: " << new_item->graph.node_size();
LOG(ERROR) << "Feed node " << feed.first << " doesn't exist in graph";
LOG(ERROR) << "Fetch node " << fetch << " doesn't exist in graph";
LOG(ERROR) << "Init node " << init << " doesn't exist in graph";
LOG(ERROR) << "Failed to read " << meta_graph_file;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/grappler/costs/analytical_cost_estimator.cc
VLOG(1) << "input: " << input << " not found for non-Merge node: " << op_name;
VLOG(4) << op_context.name << " has " << node_costs.num_ops_with_unknown_shapes << " unknown shapes";
VLOG(1) << inaccurate_nodes.size() << " out of " << nodes_executed << " nodes have inaccurate time estimation";
VLOG(4) << "Node with inaccurate time estimation: " << node;
VLOG(1) << GetStatsStringFromRunMetadata(*run_metadata, verbose);
VLOG(1) << GetStatsStringFromRunMetadata(run_metadata, verbose);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/grappler/costs/virtual_scheduler.cc
VLOG(3) << "Priority of node " << node->name() << " not found.";
LOG(FATAL) << "Not a valid ready node manager: " << ready_node_manager;
VLOG(3) << "Added ready node: " << curr_node->name();
VLOG(1) << "Some feed nodes were not consumed by the fetch fanin: " << absl::StrJoin(feed_nodes, ",");
VLOG(3) << " Add output: " << output_node->name();
VLOG(3) << "Op scheduled -- name: " << node->name() << ", op: " << node->op() << ", device: " << node->device() << ", execution_count: " << node_state.execution_count << ", ready: " << node_state.time_ready.count() << ", scheduled: " << node_state.time_scheduled.count() << ", finished: " << node_state.time_finished.count();
VLOG(1) << "node [ " << node->name() << ", " << node->op() << " ] " << "is executed more than once. " << "Skip scheduling its output nodes.";
VLOG(1) << graph_costs_.num_ops_total << " ops processed in total, with " << graph_costs_.num_ops_with_unknown_shapes << " having unknown shapes";
VLOG(1) << "Expected execution time: " << graph_costs_.execution_time.count();
VLOG(1) << "Expected compute time: " << graph_costs_.compute_time.count();
VLOG(1) << "Expected memory time: " << graph_costs_.memory_time.count();
VLOG(1) << "Expected intermediate memory time: " << graph_costs_.intermediate_memory_time.count();
VLOG(1) << "Expected max memory: " << graph_costs_.max_memory;
VLOG(1) << "Expected max per-op buffers: " << graph_costs_.max_per_op_buffers;
VLOG(1) << "Expected max per-op streaming buffers: " << graph_costs_.max_per_op_streaming;
VLOG(1) << "Per-op execution time / compute time / memory time" << " / intermediate memory time:";
VLOG(1) << absl::StrFormat(" + %30s : %c %10d / %10d / %10d / %10d", op, (is_op_cost_accurate ? ' ' : '~'), cost, compute_cost, memory_cost, intermediate_memory_cost);
VLOG(1) << "Devices:";
VLOG(1) << "Device = " << name << ", num_nodes = " << state.nodes_executed.size() << ", wall_time_ns = " << wall_time_ns.count() << ", memory usage: " << "persistent = " << HumanReadableNumBytes(persistent_memory_usage) << ", peak = " << HumanReadableNumBytes(state.max_memory_usage) << ", total = " << HumanReadableNumBytes(max_memory_usage) << ", at the end: " << HumanReadableNumBytes(state.memory_usage);
VLOG(1) << state.device_costs.num_ops_total << " ops processed in total, with " << state.device_costs.num_ops_with_unknown_shapes << " having unknown shapes";
VLOG(1) << device_annotation_stats.num_ops_annotated << " ops with shape annotation, with " << device_annotation_stats.num_ops_executed_more_than_once << " executed more than once, " << device_annotation_stats.num_ops_with_dynamic_shapes << " with dynamic shapes, " << device_annotation_stats.num_ops_with_incompatible_shapes << " with incompatible shapes, " << device_annotation_stats.num_ops_executed << " ops executed in total.";
VLOG(1) << "Per-op execution time / compute time / memory time " << " / intermediate memory time" << " (and memory usage at peak memory usage):";
VLOG(1) << absl::StrFormat( " + %30s : %c %10d / %10d / %10d / %10d", op.c_str(), (is_op_cost_accurate ? ' ' : '~'), cost, compute_cost, memory_cost, intermediate_memory_cost) << " (" << HumanReadableNumBytes(op_mem_usage) << " [" << mem_usage_percent << "%] " << (persisent_ops.count(op) > 0 ? ": persistent op)" : ")");
VLOG(1) << "Device = " << name << ", total_compute_time_ns = " << (is_total_cost_accurate ? "" : "~") << total_compute_time_ns.count() << ", utilization = " << utilization << "%";
VLOG(2) << "Node description, counts, cost:";
VLOG(2) << "Node: " << item.first << ", Count: " << item.second << ", Individual Cost: " << (is_cost_accurate ? "" : "~") << cost << " us";
VLOG(1) << "Critical path execution time: " << critical_path_costs.execution_time.count();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/grappler/costs/graph_memory.cc
VLOG(1) << "At time " << event.timestamp << " allocated " << event.tensor->memory_used << " for tensor " << event.tensor->node << ":" << event.tensor->output_id;
VLOG(1) << "At time " << event.timestamp << " deallocated " << event.tensor->memory_used << " for tensor " << event.tensor->node << ":" << event.tensor->output_id;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/grappler/costs/op_level_cost_estimator.cc
VLOG(2) << "Use minimum shape because the rank is unknown.";
VLOG(2) << "Use minimum dim size 1 because the shape is unknown.";
VLOG(1) << "Operation " << op_info.op() << " takes " << costs.execution_time.count() << " ns.";
VLOG(1) << "Missing accurate estimator for op: " << op_info.op();
VLOG(1) << "Device: " << device.type() << " gflops: " << gflops << " gb_per_sec: " << gb_per_sec;
LOG(WARNING) << "Not a cwise op: " << op_info.op();
VLOG(1) << "BAD DEVICE. Op:" << op_info.op() << " device type:" << op_info.device().type() << " device model:" << op_info.device().model();
VLOG(1) << "Op:" << op_info.op() << " GOps:" << operations / 1e9 << " Compute Time (ns):" << compute_cost.count();
VLOG(1) << "Op:" << op_info.op() << " Size (KB):" << (total_io_bytes) / 1e3 << " Memory Time (ns):" << memory_cost.count();
VLOG(1) << "Op:" << op_info.op() << " Size (KB):" << (total_io_bytes) / 1e3 << " Intermediate Memory Time (ns):" << intermediate_memory_cost.count();
VLOG(2) << "op features: " << op_info.DebugString();
VLOG(2) << "Original image shape: " << original_image_shape.DebugString();
VLOG(2) << "Original filter shape: " << original_filter_shape.DebugString();
VLOG(2) << "Image shape: " << image_shape.DebugString();
VLOG(2) << "Filter shape: " << filter_shape.DebugString();
VLOG(1) << "Batch Size:" << batch;
VLOG(1) << "Image Dims:" << ix << "," << iy;
VLOG(1) << "Input Depth:" << iz;
VLOG(1) << "Kernel Dims:" << kx << "," << ky;
VLOG(1) << "Kernel Depth:" << kz;
VLOG(1) << "Output Dims:" << ox << "," << oy;
VLOG(1) << "Output Depth:" << oz;
VLOG(1) << "Strides:" << sx << "," << sy;
VLOG(1) << "Padding:" << (padding == Padding::VALID ? "VALID" : "SAME");
LOG(ERROR) << "Need 2 inputs but got " << op_info.inputs_size();
VLOG(1) << "Key:" << item.first << " Value:" << SummarizeAttrValue(item.second);
VLOG(1) << "transpose_a:" << transpose_a;
VLOG(1) << "transpose_b:" << transpose_b;
VLOG(1) << "M, N, K: " << m_dim << "," << n_dim << "," << k_dim;
LOG(ERROR) << "Incompatible Matrix dimensions";
VLOG(1) << "Operations for Matmul: " << ops;
LOG(ERROR) << "Invalid Operation: " << op_info.op();
LOG(ERROR) << "Expected 2 inputs but got " << op_info.inputs_size();
LOG(WARNING) << "GetTensorShapeProtoFromTensorProto() -- " << "failed to parse TensorProto: " << tensor_proto.DebugString();
LOG(WARNING) << "GetTensorShapeProtoFromTensorProto() -- " << "tensor is not 1D: " << tensor.dims();
LOG(WARNING) << "GetTensorShapeProtoFromTensorProto() -- " << "Unsupported dtype: " << tensor.dtype();
VLOG(1) << "Operations for" << op_info.op() << " " << ops;
VLOG(1) << "Operations for" << op_info.op() << " " << ops;
VLOG(2) << " with " << DataTypeString(tensor.dtype()) << " tensor of shape " << tensor.shape().DebugString();
VLOG(2) << "Count: " << count << " DataTypeSize: " << size;
VLOG(1) << "Input Size: " << input_size << " Total Input Size:" << total_input_size;
VLOG(1) << "Input Count: " << input_count << " Largest Input Count:" << largest_input_count;
VLOG(1) << "Output Size: " << output_size << " Total Output Size:" << total_output_size;
LOG(WARNING) << "unsupported data format: " << data_format;
LOG(WARNING) << "unsupported filter format: " << filter_format;
LOG(WARNING) << "Einsum with malformed equation";
VLOG(1) << "Missing accurate estimator for op: " << op_info.op();
VLOG(1) << "Missing accurate estimator for op: " << op_info.op() << ", ellipsis not supported";
VLOG(1) << "Missing accurate estimator for op: " << op_info.op() << ", equation subscripts don't match tensor rank.";
VLOG(1) << "Missing accurate estimator for op: " << op_info.op() << ", Subscripts where axis appears more than once for a single " "input are not yet supported";
VLOG(1) << "Missing accurate estimator for op: " << op_info.op();
VLOG(1) << "Missing accurate estimator for op: " << op_info.op();
VLOG(1) << "Op:" << op_info.op() << " Execution Time 0 (ns)";
VLOG(1) << "Op:" << op_info.op() << " Execution Time 0 (ns)";
VLOG(1) << "Op:" << op_info.op() << " Execution Time 0 (ns)";
VLOG(2) << "op features: " << op_info.DebugString();
VLOG(2) << "Original image shape: " << original_image_shape.DebugString();
VLOG(2) << "Image shape: " << image_shape.DebugString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/grappler/costs/utils.cc
LOG(ERROR) << "OpInfo's inputs doesn't match the graph! OpInfo: " << op_info->DebugString() << "Current node: " << node.DebugString() << "Input node: " << input_node->DebugString();
VLOG(2) << "CalculateTensorSize() -- unknown rank";
VLOG(2) << "CalculateTensorSize() -- unknown dim: " << i;
LOG(ERROR) << "CalculateOutputSize() -- port_num: " << port_num << " >= output_properties.size(): " << output_properties.size();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/grappler/costs/measuring_cost_estimator.cc
LOG(ERROR) << "Failed to run start measurements: " << status.error_message();
VLOG(1) << "Number of measurement steps: " << measurement_steps_;
LOG(ERROR) << "Failed to measure graph performance: " << status.error_message();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/grappler/costs/graph_properties.cc
VLOG(2) << "Nodes with known inputs, but with unknown output dimensions:";
VLOG(2) << "Node: " << node.name() << ", Op: " << node.op() << ", " << inputs << ", " << outputs;
VLOG(2) << "Op types with known inputs, but with unknown output dimensions " << "(format: <op_type> (<count>)):";
VLOG(2) << p.first << " (" << p.second << ")";
VLOG(3) << "Skip failed to instantiate function call: function_name=" << function_node->op();
VLOG(1) << "UpdateFunction failed for " << node->op() << ". Defaulting to ShapeUnknown." << s.ToString();
VLOG(3) << "Failed to instantiate a function. Error: " << function_instantiated.error_message();
LOG(WARNING) << "UpdateOutputShapesAndValues() -- node: " << node.name() << ", inferred output shape " << "doesn't match for k=" << k << ": " << "ic->output(k): " << ic->DebugString(ic->output(k)) << ", output_shape: " << ic->DebugString(output_shape) << " -- " << node.DebugString();
LOG(WARNING) << "UpdateOutputShapesUsingAnnotatedInformation() -- node: " << node.name() << ", inferred output shape size " << ic->num_outputs() << ", annotated output shape size " << output_size;
LOG(WARNING) << "UpdateOutputShapesUsingAnnotatedInformation() -- node: " << node.name() << ", inferred output shape " << "doesn't match for i=" << i << ": " << "ic->output(k): " << ic->DebugString(ic->output(i)) << ", annotated output shape: " << ic->DebugString(output_shape) << " -- " << node.DebugString();
VLOG(3) << "UpdateOutputShapesUsingAnnotatedInformation() -- node: " << node.name() << ", inferred output shape " << i << ": " << "ic->output(i): " << ic->DebugString(ic->output(i)) << ", annotated output shape: " << ic->DebugString(output_shape) << " -- " << node.ShortDebugString();
VLOG(1) << "Propagating " << new_shapes->size() << " new shapes through " << num_loops << " loops and " << resource_handles.size() << " resources" << std::endl;
VLOG(2) << "Skipping feed node shape: " << node.name();
VLOG(3) << "Filling in graph properties for node: " << node.name();
LOG(WARNING) << incompatible_shape_nodes_.size() << " nodes have incompatible output shapes.";
LOG(WARNING) << "cost_graph is empty: nothing can be inferred!";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/grappler/costs/virtual_placer.cc
LOG(ERROR) << "VirtualPlacer couldn't parse device name from cluster: " << kv.first;
VLOG(3) << "default device name: " << default_device_name_;
VLOG(3) << "default job name: " << default_job_name_lowercase_;
VLOG(3) << "node.name=" << node.name() << " node.device=" << node.device() << " is placed on: " << device;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/grappler/graph_analyzer/sig_node.cc
LOG(INFO) << "DEBUG node " << name() << " mask=" << std::hex << next_hashed_nodes_;
LOG(INFO) << "DEBUG node " << name() << " += " << entry.peer->name() << " mask=" << std::hex << next_hashed_nodes_;
LOG(INFO) << "DEBUG distance=" << 0 << " node " << debug_i++ << " " << node->name() << " mask=" << std::hex << node->last_hashed_nodes_;
LOG(INFO) << "DEBUG distance=" << distance << " node " << debug_i++ << " " << node->name() << " oldmask=" << std::hex << node->last_hashed_nodes_ << " mask=" << std::hex << node->next_hashed_nodes_;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/grappler/graph_analyzer/graph_analyzer_tool.cc
LOG(INFO) << "Loading model from " << filename;
LOG(WARNING) << "Failed to read a binary metagraph: " << st;
LOG(FATAL) << "Failed to read a text metagraph: " << st;
LOG(INFO) << "Fetch node: " << fetch;
LOG(INFO) << "Pruned " << metagraph.graph_def().node_size() - graph->node_size() << " nodes. Original graph size: " << metagraph.graph_def().node_size() << ". New graph size: " << graph->node_size() << ".";
LOG(FATAL) << "Invalid subgraph size " << n << ", must be at least 1";
LOG(INFO) << "Running the analysis";
LOG(FATAL) << "Analysis failed: " << st;
LOG(INFO) << "Printing the result";
LOG(FATAL) << "Failed to print the result: " << st;
LOG(INFO) << "Completed";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/grappler/clusters/single_machine.cc
VLOG(1) << "Number of CPU cores: " << num_cpu_cores << " Number of GPUs: " << num_gpus;
LOG(INFO) << "Cleaning up previous session";
LOG(INFO) << "Starting new session";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/grappler/clusters/utils.cc
LOG(ERROR) << "Failed to get device properties, error code: " << error;
LOG(ERROR) << "Failed to get device properties, error code: " << error;
LOG(ERROR) << s;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/grappler/utils/transitive_fanin.cc
VLOG(2) << "ComputeTransitiveFanin: problem with root node: " << root;
VLOG(2) << "ComputeTransitiveFanin: problem with node: " << input;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/grappler/utils/frame.cc
LOG(WARNING) << "Node '" << node.name() << "' doesn't belong to the graph used for initialization";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/grappler/utils/graph_view.cc
LOG(WARNING) << s.error_message();
LOG(WARNING) << s.error_message();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/grappler/utils/functions.cc
VLOG(3) << absl::Substitute( "Deleted $0 unreachable functions from the Grappler function item " "instantiation of $1 (library size = $2)", flib.num_functions() - function_body.library().function_size(), signature.name(), function_body.library().function_size());
VLOG(3) << "Remove functions output: name=" << output.node_name << "(index = " << i << ")";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/grappler/utils/topological_sort.cc
VLOG(1) << "The graph couldn't be sorted in topological order. Stalled " "at node = " << graph.node(back).DebugString();
VLOG(1) << "Node not ready: " << graph.node(i).DebugString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/grappler/optimizers/memory_optimizer.cc
VLOG(2) << " Recomputation trigger " << current_trigger_node->name() << " depends on " << (*target_input_iterator)->name();
VLOG(1) << "Recomputing a " << recomputed_source_nodes.size() << " node subgraph";
VLOG(2) << " " << original_node->name();
VLOG(1) << "Failed to infer memory usage: " << s.error_message();
VLOG(1) << "Available memory unknown for device " << name;
VLOG(1) << "Failed to infer shapes: " << s.error_message();
VLOG(1) << "Failed to initialize graph topology view: " << initialized_topology.error_message();
VLOG(1) << "Missing properties for " << node->name();
VLOG(1) << "Shape not fully known for " << node->name();
VLOG(1) << "Unsupported dtype for " << node->name();
VLOG(1) << "Temporary variable already exists " << tmp_var_name;
VLOG(1) << "Failed to infer memory usage: " << s.error_message();
VLOG(1) << "Peak memory usage unknown for device " << name;
VLOG(1) << "Not enough time to swap: skipping " << live_tensor.node;
VLOG(1) << "Will swap fanout " << fanout_to_swap.node->name() << ":" << fanout_to_swap.port_id << " of tensor " << mem_info.port.node->name() << ":" << mem_info.port.port_id << " of size " << mem_info.memory_used;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/grappler/optimizers/remapper.cc
VLOG(2) << "Fuse " << contraction.op() << " with BiasAdd: " << " bias_add=" << bias_add.name() << " contraction=" << contraction.name();
VLOG(2) << "Fuse " << contraction.op() << " with BiasAdd and " << activation.op() << ":" << " activation=" << activation.name() << " bias_add=" << bias_add.name() << " contraction=" << contraction.name();
VLOG(2) << "Fuse Conv2D with Squeeze and BiasAdd: " << " bias_add=" << bias_add.name() << " squeeze=" << squeeze.name() << " conv2d=" << contraction.name();
VLOG(2) << "Fuse Conv2D with BatchNorm: batch_norm=" << fused_batch_norm.name() << " conv2d=" << contraction.name();
VLOG(2) << "Fuse Conv2D with BatchNorm and " << activation.op() << ": activation=" << activation.name() << " batch_norm=" << fused_batch_norm.name() << " conv2d=" << contraction.name();
VLOG(2) << "Fuse " << activation.op() << " with FusedBatchNorm:" << " activation=" << activation.name() << " side_input=" << (matched.side_input != kMissingIndex ? graph->node(matched.side_input).name() : "<none>") << " invalidated=" << (matched.invalidated != kMissingIndex ? graph->node(matched.invalidated).name() : "<none>") << " fused_batch_norm=" << fused_batch_norm.name();
VLOG(2) << "Optimizing fused batch norm node " << SummarizeNodeDef(fused_node);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/grappler/optimizers/generic_layout_optimizer.cc
VLOG(0) << "Cancel Transpose nodes around Pad:" << " transpose_before=" << transpose_before->node()->name() << " pad=" << pad->node()->name() << " transpose_after=" << absl::StrJoin(pad_fanout_transposes, ",", MutableNodeViewFormatter());
LOG(WARNING) << "generic layout optimizer was called with cluster == nullptr";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/grappler/optimizers/layout_optimizer.cc
LOG(ERROR) << "Cannot handle explicit_paddings attribute of size " << size;
LOG(ERROR) << "Failed to parse TensorProto.";
LOG(ERROR) << "Failed to parse TensorProto.";
VLOG(1) << "Number of nodes for original graph: " << graph_->node_size();
VLOG(1) << "Number of nodes after Expand: " << graph_->node_size();
VLOG(1) << "Number of nodes after Collapse: " << graph_->node_size();
VLOG(1) << "Annotate shape return status: " << status.ToString();
LOG(WARNING) << "layout optimizer was called with cluster == nullptr";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/grappler/optimizers/meta_optimizer.cc
VLOG(2) << "Registered default graph optimizer: " << optimizer_name;
VLOG(2) << "Registered custom graph optimizer: " << optimizer_name;
VLOG(2) << "Can't register an optimizer by name: " << optimizer_name;
VLOG(2) << "Registered custom configurable graph optimizer: " << optimizer_config.name();
VLOG(2) << "Registered default graph optimizer: " << optimizer_config.name();
VLOG(2) << "Can't register an optimizer by name: " << optimizer_config.name();
VLOG(3) << "Skipping optimization, graph has less than " << min_graph_nodes << " nodes.";
VLOG(2) << "No inter optimizer verifiers have been configured";
VLOG(2) << inter_optimizer_verifiers.size() << " inter optimizer verifiers have been configured";
VLOG(2) << "No post optimization verifiers have been configured";
VLOG(2) << post_optimization_verifiers.size() << " post optimization verifiers have been configured";
VLOG(2) << "Optimize GrapplerItem: item.id=" << item.id << " num_optimizers=" << optimizers.size() << ", num nodes = " << item.graph.node_size();
VLOG(3) << "Skipping graph optimization, no optimizers registered";
VLOG(3) << "Stopping after iteration " << iteration << ", graph is tiny (#nodes = " << optimized_graph->node_size() << " < " << min_graph_nodes << ")";
VLOG(4) << "Starting optimization iteration " << iteration;
VLOG(3) << "Replace function library with a stub for " << optimizer->name();
LOG(WARNING) << optimizer->name() << " failed: " << message;
LOG(ERROR) << optimizer->name() << " failed: " << message;
VLOG(1) << optimizer->name() << ": " << message;
VLOG(1) << "Starting optimization for grappler item: " << item.id;
VLOG(1) << absl::Substitute( "Deleted $0 unreachable functions from the graph (library size = $1)", item.graph.library().function_size() - trimmed_item.graph.library().function_size(), trimmed_item.graph.library().function_size());
VLOG(1) << "Optimized main graph.";
VLOG(3) << "Optimize function: function=" << func_name << " [" << function_idx++ << " of " << optimized_graph->library().function_size() << "]";
VLOG(1) << "Optimized " << optimized_funcs.size() << " functions: " << absl::StrJoin(optimized_funcs, ", ");
VLOG(3) << "Optimized graph =" << optimized_graph->DebugString();
LOG(INFO) << "Optimization results for grappler item: " << graph_result.id;
LOG(INFO) << " " << result.optimizer_name << ": " << result.message;
VLOG(3) << added_device.error_message();
VLOG(3) << "Grappler available devices: " << absl::StrJoin(item.devices(), ", ");

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/grappler/optimizers/function_api_info.cc
VLOG(3) << "Got " << func_info->function_type() << " function: " << function_name << " with interface: " << interface_name;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/grappler/optimizers/dependency_optimizer.cc
VLOG(2) << "***** Replacing " << node_name << " (" << node->op() << ") with NoOp.";
LOG(ERROR) << "Invalid input " << node->input(i);
VLOG(2) << "***** Rerouting input around" << node->DebugString();
VLOG(2) << "consumer before:" << consumer->DebugString();
VLOG(2) << "consumer after:" << consumer->DebugString();
VLOG(1) << "Deleted " << nodes_to_delete.size() << " out of " << optimized_graph_->node_size() << " nodes.";
VLOG(1) << "Removed " << num_controls_removed << " out of " << num_controls << " control dependencies";
VLOG(1) << "DependencyOptimizer::GroupCrossDeviceControlEdges host_granularity=" << host_granularity;
VLOG(2) << "Cross-device " << node->name() << " " << input->device() << " -> " << node->device();
VLOG(2) << "Duplicate input device from " << node->name();
VLOG(1) << "GroupCrossDeviceControlEdges: Added " << SummarizeNodeDef(*noop);
VLOG(2) << "Rewriting input from " << input_name;
LOG(ERROR) << "Iteration = " << iteration << ", topological sort failed with message: " << topo_sort_status.error_message();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/grappler/optimizers/model_pruner.cc
VLOG(1) << "Pruned " << nodes_to_delete.size() << " nodes from the graph. The graph now contains " << optimized_graph->node_size() << " nodes.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc
LOG(FATAL) << "CustomGraphOptimizer is registered twice: " << name;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/grappler/optimizers/implementation_selector.cc
VLOG(3) << "Updated DTYPE for Identity node: " << fanout.node_view()->node()->DebugString();
VLOG(3) << "Node def before swap is: " << node_def->DebugString();
VLOG(3) << "Node def after swap is: " << node_def->DebugString();
VLOG(2) << "Op " << node_def->name() << " runs on " << node_def->device() << " = (" << parsed_name.type << ")";
VLOG(2) << "Swapping: " << function_name << " TO: " << func_name;
VLOG(2) << "Skipping graph since it does not have function def";
VLOG(2) << "Skipping optimization since lib_info is empty";
VLOG(2) << "Skipping optimization due to error while loading function " << "libraries: " << status;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/grappler/optimizers/constant_folding.cc
LOG(ERROR) << "Bad input: " << input;
VLOG(2) << "Folded node: " << SummarizeNodeDef(*node);
VLOG(3) << "Generated constant node: " << SummarizeNodeDef(*const_node);
VLOG(3) << "Preserving edge from " << node->name() << ":" << port << "[" << node->op() << "] to " << output->name() << ":" << i << "[" << output->op() << "]";
VLOG(2) << "foldable(" << graph_->node(i).name() << ") = " << foldable;
VLOG(1) << "Failed to fold node " << node->DebugString() << "Error message: " << s;
VLOG(1) << "Unsupported type " << DataTypeString(dtype);
VLOG(1) << "Unsupported type " << DataTypeString(dtype);
VLOG(1) << "Failed to replace node " << node->name() << " of type " << DataTypeString(dtype) << " with constant tensor of value " << value;
VLOG(1) << "++++++++ PushDown for node " << parent.name() << ": " << parent.op() << "(" << left_child->op() << ", " << right_child->op() << ")";
VLOG(1) << "input_c = " << input_c << "input_x = " << input_x;
LOG(ERROR) << "Bad input: " << input;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/grappler/optimizers/auto_mixed_precision.cc
VLOG(0) << "Skip error: " << error_message;
VLOG(0) << "Skip error: " << error_message;
VLOG(3) << "Skip error: " << error_message;
LOG(ERROR) << "Op present in multiple lists: " << s;
LOG(INFO) << "Saved " << (preop ? "pre-optimization" : "post-optimization") << " graph as binary to " << fname;
LOG(INFO) << "Saved " << (preop ? "pre-optimization" : "post-optimization") << " graph as text to " << fname;
LOG(INFO) << "Saved paint bucket info to " << fname;
VLOG(2) << "Skipping " << node.op() << " node " << node.name() << " because it " << (MustPreserve(node) ? "must be preserved" : "is not on the GPU, or the GPU arch is not suitable");
VLOG(2) << "Changing op of " << node->op() << " node " << node->name() << " to FusedBatchNormV2";
VLOG(2) << "Changing op of " << node->op() << " node " << node->name() << " to FusedBatchNormGradV2";
VLOG(2) << "Identifying nodes that should be processed";
VLOG(2) << "Converting FusedBatchNorm* ops to V2";
VLOG(2) << "Building node type map for graph";
LOG(WARNING) << "Unsupported " << node.op() << " node found in graph (" << node.name() << "), tensor list ops will not be converted.";
VLOG(2) << "Identifying TensorList* nodes";
VLOG(2) << "Constructing graph type attribute topology view";
VLOG(2) << "Beginning pass 1 to add whitelist ops";
VLOG(2) << "Finished pass 1";
LOG(INFO) << "No whitelist ops found, nothing to do";
VLOG(2) << "Beginning pass 2 to propagate black forwards from blacklist ops " "through clear/graylist ops";
VLOG(2) << "Finished pass 2";
VLOG(2) << "Forcing color match between data structure ops";
VLOG(2) << "Beginning pass 3 to set clear and gray nodes to white if they " "are between white ops";
VLOG(2) << "Finished pass 3";
VLOG(2) << "Beginning pass 4 to propagate white from white nodes through " "clearlist ops";
VLOG(2) << "Finished pass 4";
VLOG(2) << "Forcing color match between data structure ops";
VLOG(2) << "Forcing color match on loop edges";
VLOG(2) << "Finding existing casts that can be made white";
VLOG(2) << "Beginning final pass to change type attributes and insert Cast " "ops at paint boundaries";
VLOG(2) << "Finished final pass";
VLOG(2) << "Painting type " << root.type_attr.DebugString() << " of node " << root.node->name() << " WHITE because its op " << root.node->op() << " is on the whitelist";
VLOG(2) << "Painting type " << item.type_attr.DebugString() << " of " << item.node->op() << " node " << item.node->name() << " BLACK";
VLOG(2) << "Painting type " << item.type_attr.DebugString() << " of " << item.node->op() << " node " << item.node->name() << " WHITE";
VLOG(2) << "Painting type " << item.type_attr.DebugString() << " of " << item.node->op() << " node " << item.node->name() << " WHITE";
VLOG(2) << "Painting type T of Merge node " << graph_type_view_.GetNode(merge_idx)->node->name() << " BLACK to match the color of its sibling Merge nodes " "with common NextIteration node " << node.name();
VLOG(2) << "Painting type T of NextIteration node " << node.name() << " BLACK to match the color of its output Merge node(s)";
VLOG(2) << "Painting type T of NextIteration node " << node.name() << " WHITE to match the color of its output Merge node(s)";
VLOG(2) << "Painting type " << node_type.type_attr.DebugString() << " of " << node_type.node->op() << " node " << node_type.node->name() << " " << (any_black ? "BLACK" : "WHITE") << " because at least one of its siblings is " << (any_black ? "BLACK" : "WHITE");
VLOG(1) << "Changing type " << type_attr.DebugString() << " of " << node->op() << " node " << node->name() << " to DT_HALF";
VLOG(1) << "Inserting cast to " << (to_fp16 ? "DT_HALF" : "DT_FLOAT") << " at " << src.node->op() << " " << src.node->name() << ":" << src.port_id;
LOG(WARNING) << "No (suitable) GPUs detected, skipping " << name() << " graph optimizer";
LOG(INFO) << "Running " << name() << " graph optimizer";
VLOG(1) << "Running " << name() << " graph optimizer on " << item.id;
LOG(WARNING) << name() << " graph optimizer FAILED: " << status.ToString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/grappler/optimizers/loop_optimizer.cc
VLOG(1) << "Stack node: " << graph_view.graph()->node(stack_node_idx).name();
VLOG(1) << "Fanout " << fanout_idx << " : " << fanout_node.name();
VLOG(1) << "Converting " << push_node_idx << " : " << push_node->DebugString();
VLOG(1) << "After converting: " << push_node->DebugString();
VLOG(4) << "Evaluate bool op: op_node=" << op_node.name() << " input0=" << constant_operand_0.name() << " input1=" << constant_operand_1.name();
VLOG(3) << "Found switch node with constant predicate:" << " switch_node=" << switch_node.name() << " switch_predicate=" << switch_predicate->name();
VLOG(4) << "Try to find a zero iteration while loop:" << " switch_node=" << switch_node.name();
VLOG(4) << "Check if loop will be 0 iterations:" << "| switch_node : " << switch_node.name() << "| switch_ctrl_node : " << switch_ctrl_node->name() << "| merge_node : " << merge_node->name() << "| constant_ctrl_input: " << constant_ctrl_input->name() << "| enter_node : " << (enter_node ? enter_node->name() : "<n/a>") << "| constant_init_node : " << constant_init_node->name();
VLOG(3) << "Remove 0 iteration while loop:" << " switch_node=" << switch_node.name();
VLOG(4) << "Was not able to prove that loop has 0 iterations.";
LOG(WARNING) << "Skipping loop optimization for Merge node with control input: " << merge_node->name();
LOG(WARNING) << "Skipping loop optimization for Merge node (" << merge_node->name() << ") with unexpected dead_inputs.size() (" << dead_inputs.size() << " or num_data_inputs" << num_data_inputs;
VLOG(3) << "Merge node before cleanup: " << merge_node->DebugString();
VLOG(3) << "Merge node after cleanup: " << merge_node->DebugString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/grappler/optimizers/generic_layout_optimizer_transposer.cc
VLOG(3) << "GenericLayoutOptimizer: transforming node '" << node->GetName() << "' with op '" << node->GetOp() << "' from data format '" << context->src_format << "' to '" << context->dst_format << "'";
LOG(ERROR) << "Failed to parse TensorProto.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/grappler/optimizers/function_optimizer.cc
LOG(WARNING) << "Can't resolve function instantiation attributes: " << SummarizeNodeDef(func_node);
VLOG(3) << "Pruned function library: " << pruned_flib.num_functions() << " functions (" << pruned_functions << ")";
VLOG(3) << "Push const into function body: input=" << input;
VLOG(3) << "Forward control dependency: input=" << ctrl;
VLOG(2) << "Specialize function call: " << SummarizeNodeDef(func_node);
VLOG(2) << "Function was already specialized in identical context: " "specialized_name=" << already_specialized->specialized_func_name;
VLOG(3) << error_message;
VLOG(4) << "Check that node " << side_effect->name() << " will execute after inlining.";
VLOG(4) << "Found a path to control source: " << side_effect->name() << " ---> " << (*it)->name();
VLOG(4) << "Add dead tensors source. Switch node: " << n->name();
VLOG(4) << "Add dead tensors source. Function call: " << func.name() << " node=" << n->name();
VLOG(4) << "Found a path to output node from dead tensor source: " << dead_tensor_source->name() << " ---> " << (*it)->name();
VLOG(3) << "Make function body for inlining: " << SummarizeNode(node);
VLOG(4) << "Instantiate a custom SymbolicGradient: gradient=" << grad << " (function=" << func.name() << ")";
VLOG(4) << "Instantiate a SymbolicGradient for a primitive op: " << func.name();
VLOG(4) << "Instantiate a SymbolicGradient for a function: " << func.name();
VLOG(4) << "Instantiate a function call: function=" << func.name();
VLOG(3) << "Add control edges from all data inputs to enforce strict " "semantics with regard to function inputs";
VLOG(3) << "Add a frame forwarding control edge: from=" << frame->name() << " to=" << caller->name();
LOG(WARNING) << "Enter[is_constant=false] node: " << enter->name() << " does not have an outgoing edge to a Merge.";
VLOG(2) << "Inline function calls: grappler_item_id=" << item.id << " (aggessive_mode=" << is_aggressive << ")";
VLOG(2) << "Lower functional control flow op: " << SummarizeNode(*n);
VLOG(2) << "Ignore error: " << can_inline_function_call.error_message();
VLOG(2) << "Inline function call node: " << n->name();
VLOG(2) << "Failed to inline function call node: " << can_inline_function_call.error_message();
VLOG(4) << "Inlined " << inlined_function_names.size() << " function calls: " << absl::StrJoin(inlined_function_names, ", ");
VLOG(3) << "Not placing graph after function inlining" << " (did not inline any of the function calls).";
VLOG(3) << "Not placing graph after function inlining" << " (device set is empty)";
VLOG(3) << "Run placer for the graph after function inlining. " << "Devices: [" << absl::StrJoin(item.devices(), ", ") << "]";
VLOG(3) << "Run function optimizer pass: grappler_item_id=" << item.id;
VLOG(3) << "Skip specialization error: " << status.error_message();
VLOG(2) << "Skip function specialization: " << func->signature().name();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/grappler/optimizers/scoped_allocator_optimizer.cc
VLOG(1) << "CheckTypesAndGetShapes";
VLOG(2) << "op " << n->name() << " has type " << dtype << " shapes.size() " << shapes->size();
LOG(ERROR) << "Node " << n->DebugString() << " lacks output shape.";
VLOG(2) << "Adding shape " << props.shape().DebugString();
LOG(FATAL) << "Failed to find node " << nd->name() << " in graph";
VLOG(2) << " consider edge " << (*inputs)[edge_index];
VLOG(1) << "Rewrite input " << edge_name << " op " << op->name() << " old output index " << output_index << " with identity " << identity_name << " new output index 0";
VLOG(1) << "Getinputs";
VLOG(2) << "for node " << n->name();
VLOG(2) << "inode " << inode->DebugString() << " output_index " << output_index;
VLOG(2) << "inode after rewrite " << inode->DebugString() << " output_index " << output_index;
VLOG(2) << "GetDataInputs for node " << op->name();
VLOG(2) << "inode " << inode->DebugString() << " output_index " << output_index;
VLOG(log_level) << line;
VLOG(2) << "extending";
VLOG(2) << "setting new attr value";
VLOG(2) << "get attrs for " << nd.from_node_def->name();
LOG(INFO) << "Abandoning ScopedAllocatorOptimizer because input " << nd.from_node_def->name() << " output " << scope_ids[0] << " is already assigned to scope_id " << scope_ids[1];
VLOG(1) << "Remove control output from " << input_node_name << " via edge " << input_name << " to " << n->name();
VLOG(2) << "num_bytes " << num_bytes << " num_elts=" << num_elts;
VLOG(2) << "TransitiveFanout parent: " << node->name() << " child: " << output->name() << " of type " << output->op();
VLOG(2) << "ConstructScopedAllocatorNode " << sa_name;
VLOG(2) << "To input " << i << ": " << nd.from_node_def->name() << " add control input " << "^" << sa_name;
VLOG(2) << "Found node " << inputs_to_first[i].from_node_def->name() << " in the fanout of " << sa_name;
VLOG(2) << "Adding control dependency from " << inputs_to_first[i].from_node_def->name() << " to " << sa_node->name();
LOG(WARNING) << "Found no node from which a control edge can be added to " "scoped allocator node. If you run into issues with " "graphs that contain control flow, turn off the " "ScopedAllocatorOptimizer and file a bug.";
VLOG(2) << "BuildSAConcatNode " << sac_name
LOG(ERROR) << "Data edge between " << old_op_input << " and " << old_op->name() << " cannot build ScopedAllocator.";
VLOG(2) << "New sac_name " << sac_name << " shape " << sa_shape.DebugString();
VLOG(2) << "BuildReplacementOp " << sa_op_name;
VLOG(2) << "new ScopedAllocatorSplit " << sas_name;
VLOG(2) << "RewireSubgraph";
VLOG(3) << "old_op " << old_op->name() << " had " << output_nodes.size() << " outputs. Moving them to the ScopedAllocatorSplit node.";
VLOG(3) << " output: " << n->name();
VLOG(3) << "really checking old output " << n->name() << " for corresponding input.";
VLOG(3) << "Dropping control output from " << old_op->name() << " to " << n->name();
VLOG(3) << "about to iterate over " << n->input_size() << " inputs";
VLOG(3) << "input " << n->input(i);
VLOG(3) << "match pos=" << position;
VLOG(3) << "breaking on success";
VLOG(3) << "other input " << n->input(i);
VLOG(3) << "before HasOp";
VLOG(3) << "bottom of for output_nodes";
VLOG(3) << "Clearing all inputs of " << old_op->name();
VLOG(3) << "after clear: " << old_op->DebugString();
VLOG(1) << "Rewrite";
VLOG(1) << "UnaryElementwiseRewriter::Rewrite " << op_name << " to: " << op_names;
VLOG(2) << "op_instance_name " << nd->name();
VLOG(1) << "ScopedAllocatorOptimizer::ScopedAllocatorOptimizer";
VLOG(3) << "Input graph:";
VLOG(1) << "ScopedAllocatorOptimizer::Optimize() done";
VLOG(3) << "Optimized graph:";
VLOG(1) << "FindOpOccurrences ";
VLOG(1) << "search target " << it;
VLOG(1) << "found " << op_name << " on dev " << node->device();
VLOG(1) << "ProcessGraphDef " << invocation_count;
VLOG(2) << "Processing device " << dt.first;
VLOG(1) << "Processing " << op_name << " set size " << it.second.size();
LOG(ERROR) << "Failed to find Rewriter in ScopedAllocatorOptimizer " << "for op_name " << op_name;
VLOG(2) << "applied to tree node " << t->edge_ << " at depth " << t->depth_ << " of size " << t->nodes_.size();
VLOG(1) << "Applying Rewriter for " << op_name;
VLOG(1) << "ScopedAllocatorOptimizer returning " << status;
LOG(ERROR) << "ScopedAllocatorOptimizer: " << status;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/grappler/optimizers/pin_to_host_optimizer.cc
LOG(WARNING) << "port_id=" << port_id << " but output_properties.size()=" << output_properties.size() << "" << node.DebugString();
LOG(WARNING) << "Could not find OpDef for : " << node.op();
LOG(WARNING) << "Invalid port: " << port_id << "!" << node.DebugString() << "" << op->DebugString();
LOG(INFO) << "Could not find KernelDef for: " << node.op();
LOG(WARNING) << "Could not find OpDef for : " << node.op();
LOG(INFO) << "Could not find KernelDef for: " << node.op();
VLOG(2) << "Moving node " << node.name() << " to device " << device;
VLOG(2) << "Swapping node " << node->name() << " back to device " << device;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/grappler/optimizers/arithmetic_optimizer.cc
VLOG(2) << "Collapse Add/AddN: root=" << group.root_node->name() << " op=" << group.root_node->op() << " num_optimized_nodes=" << group.optimized_nodes.size() << " num_inputs=" << group.inputs.size();
VLOG(3) << "Add/AddN group has " << shape_sig_to_inputs.size() << " unique shapes: " << absl::StrJoin(shape_sig_to_inputs, ", ", [](string* out, SigKV p) { strings::StrAppend(out, p.first); });
VLOG(2) << "Minimize broadcast: root=" << group.root_node->name() << " op=" << group.root_node->op() << " num_optimized_nodes=" << group.optimized_nodes.size();
VLOG(3) << "Skip min-bcast group with single unique shape";
VLOG(3) << "Hoist unary op chain:" << " root=" << root_node->DebugString() << " prefix_length=" << prefix_length << " ctrl_inputs=[" << absl::StrJoin(*ctrl_inputs, ", ") << "]";
VLOG(3) << "Swapped inputs to mul";
VLOG(3) << "Fold multiply into conv: conv=" << conv->name() << " mul=" << mul->name() << " weights=" << weights->name();
VLOG(3) << "Simplify aggregation with identical inputs: node=" << node->name() << " num_inputs=" << num_inputs;
VLOG(2) << "Fuse unary ops: root=" << root->name() << " op_names=[" << absl::StrJoin(op_names, ", ") << "]";
LOG(WARNING) << "Failed to initialize GraphTopologyView.";
VLOG(1) << "Run " << pipeline.NumStages() << " arithmetic optimizer stages: " << absl::StrJoin(pipeline.StageNames(), ", ");
VLOG(1) << "Shape inference failed." << status.error_message();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/grappler/optimizers/auto_parallel.cc
LOG(INFO) << "Number of GPUs: " << num_gpus_;
LOG(INFO) << "Original graph size: " << graph_.node_size();
VLOG(1) << "Init node: " << init;
VLOG(1) << "Fetch node: " << fetch;
VLOG(2) << "Variable: " << var->name();
VLOG(2) << "Apply gradients node: " << graph_.node(i).name();
LOG(INFO) << "Graph size after adding div nodes: " << all_nodes_.size();
LOG(INFO) << "Number of training nodes: " << train_nodes.size();
LOG(INFO) << "Dequeue node: " << dequeue_node->name();
LOG(INFO) << "Number of input nodes: " << input_nodes.size();
LOG(INFO) << "Number of replica nodes: " << replica_nodes_.size();
LOG(INFO) << "Number of shared nodes: " << shared_nodes_.size();
LOG(INFO) << "Parallelized graph size: " << graph->node_size();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/grappler/optimizers/data/map_vectorization.cc
LOG(WARNING) << "VectorizeMapDefun failed. The function will only be " "naively vectorized with MapDefun. Reason: " << s;
VLOG(1) << "Cannot vectorize dataset.map().batch() because the map " "dataset does not have fully defined output shapes.";
VLOG(1) << "Cannot vectorize dataset.map().batch() because the input " "dataset does not have fully defined output shapes.";
VLOG(1) << "Cannot vectorize dataset.map().batch() because the map " "function is stateful.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/grappler/optimizers/data/filter_fusion.cc
VLOG(1) << "Can't fuse Filters because they have different signature";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/grappler/optimizers/data/auto_shard.cc
LOG(WARNING) << "In AUTO-mode, and switching to DATA-based sharding, " "instead of FILE-based sharding as we cannot find " "appropriate reader dataset op(s) to shard. Error: " << s.error_message();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/grappler/optimizers/data/map_fusion.cc
VLOG(1) << "Can't fuse two maps because the output signature of the " "first map function does not match the input signature of the " "second function";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/grappler/optimizers/data/map_and_filter_fusion.cc
VLOG(1) << "Can't fuse map and filter because the output signature of " "the map function does not match the input signature of the " "filter function";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/grappler/optimizers/data/vectorization_utils.cc
VLOG(2) << "Vectorizer for op "" << op_node->type_string() << "" failed with error: " << s;
VLOG(2) << "Could not convert the output at node: " << output_node->DebugString() << "Error: " << s;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/grappler/inputs/file_input_yielder.cc
LOG(WARNING) << "Skipping non existent file " << filename;
LOG(INFO) << "Loading model from " << filename;
LOG(WARNING) << "Failed to read MetaGraphDef from " << filename << ": " << s.ToString();
LOG(ERROR) << "No train op specified";
LOG(ERROR) << "Non existent train op specified: " << train_op;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/lib/io/zlib_outputbuffer.cc
LOG(WARNING) << "ZlibOutputBuffer::Close() not called. Possible data loss";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/lib/io/record_reader.cc
LOG(ERROR) << "Compression is not supported but compression_type is set." << " No compression will be used.";
LOG(ERROR) << "Compression is not supported but compression_type is set." << " No compression will be used.";
LOG(ERROR) << "Unsupported compression_type:" << compression_type << ". No compression will be used.";
LOG(FATAL) << "Zlib compression is unsupported on mobile platforms.";
LOG(FATAL) << "Unrecognized compression type :" << options.compression_type;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/lib/io/record_writer.cc
LOG(ERROR) << "Compression is not supported but compression_type is set." << " No compression will be used.";
LOG(ERROR) << "Compression is not supported but compression_type is set." << " No compression will be used.";
LOG(ERROR) << "Unsupported compression_type:" << compression_type << ". No compression will be used.";
LOG(FATAL) << "Zlib compression is unsupported on mobile platforms.";
LOG(FATAL) << "Failed to initialize Zlib inputbuffer. Error: " << s.ToString();
LOG(FATAL) << "Unspecified compression type :" << options.compression_type;
LOG(ERROR) << "Could not finish writing file: " << s;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/lib/io/snappy/snappy_outputbuffer.cc
LOG(WARNING) << "There is still data in the output buffer. " << "Possible data loss has occurred.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/lib/png/png_io.cc
VLOG(1) << "PNG error: " << msg;
LOG(WARNING) << "PNG warning: " << msg;
LOG(WARNING) << "Warning! Metadata contains 0 character(s).";
VLOG(1) << ": DecodePNG <- png_create_read_struct failed";
VLOG(1) << ": DecodePNG error trapped.";
VLOG(1) << ": DecodePNG <- png_create_info_struct failed";
VLOG(1) << ": DecodePNG <- error during header parsing.";
VLOG(1) << ": DecodePNG <- invalid dimensions";
VLOG(1) << ": DecodePNG error trapped.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/lib/db/sqlite.cc
LOG(FATAL) << "BEGIN failed: " << sqlite3_errmsg(db_->db_);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/lib/jpeg/jpeg_handle.cc
VLOG(1) << "Initializing buffer=" << dest->bufsize << " bytes";
VLOG(1) << "Writing " << dest->bufsize << " bytes";
VLOG(1) << "Writing " << dest->bufsize - dest->pub.free_in_buffer << " bytes";
VLOG(1) << "Total size= " << dest->dest->size();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/lib/jpeg/jpeg_mem.cc
LOG(ERROR) << " Invalid components value " << components << std::endl;
LOG(ERROR) << "Invalid image size: " << cinfo.output_width << " x " << cinfo.output_height;
LOG(ERROR) << "Image too large: " << total_size;
LOG(ERROR) << "Invalid crop window: x=" << flags.crop_x << ", y=" << flags.crop_y << ", w=" << target_output_width << ", h=" << target_output_height << " for image_width: " << cinfo.output_width << " and image_height: " << cinfo.output_height;
LOG(ERROR) << "Incompatible stride: " << stride << " < " << min_stride;
LOG(ERROR) << "Premature end of JPEG data. Stopped at line " << cinfo.output_scanline - skipped_scanlines << "/" << target_output_height;
LOG(ERROR) << "Invalid components value " << components << std::endl;
LOG(ERROR) << "Unhandled case " << error;
LOG(ERROR) << "Invalid crop window: x=" << flags.crop_x << ", y=" << flags.crop_y << ", w=" << target_output_width << ", h=" << target_output_height << " for image_width: " << cinfo.output_width << " and image_height: " << cinfo.output_height;
LOG(ERROR) << "Invalid image size: " << width << " x " << height;
LOG(ERROR) << "Image too large: " << total_size;
LOG(ERROR) << "Incompatible input stride";
LOG(ERROR) << " Invalid components value " << components << std::endl;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/lib/monitoring/collection_registry.cc
LOG(ERROR) << "Cannot register 2 metrics with the same name: " << metric_def->name();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/lib/gif/gif_io.cc
LOG(WARNING) << "Fail to close gif file, reason: " << GifErrorStringNonNull(error_code);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/ops/array_grad.cc
VLOG(1) << "IdentityGrad " << DebugString(*g);
VLOG(1) << "PackGrad " << DebugString(*g);
VLOG(1) << "UnpackGrad " << DebugString(*g);
VLOG(1) << "SplitGrad " << DebugString(*g);
VLOG(1) << "SplitVGrad " << DebugString(*g);
VLOG(1) << "ArrayToListGrad " << DebugString(*g);
VLOG(1) << "ListToArrayGrad " << DebugString(*g);
VLOG(1) << "FillGrad " << DebugString(*g);
VLOG(1) << "TransposeGrad " << DebugString(*g);
VLOG(1) << "ConjugateTransposeGrad " << DebugString(*g);
VLOG(1) << "ReverseGrad " << DebugString(*g);
VLOG(1) << "ReverseGrad " << DebugString(*g);
VLOG(1) << "SliceGrad " << DebugString(*g);
VLOG(1) << "StridedSliceGrad " << DebugString(*g);
VLOG(1) << "StridedSliceGrad " << DebugString(*g);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/profiler/internal/gpu/device_tracer.cc
LOG(INFO) << " GpuTracer has collected " << num_callback_events_ << " callback api events and " << num_activity_events_ << " activity events.";
LOG(INFO) << " GpuTracer has collected " << num_callback_events_ << " callback api events and " << num_activity_events_ << " activity events.";
VLOG(1) << "GpuTracer created.";
VLOG(1) << "No trace data collected, session wasn't started";
LOG(ERROR) << "Cannot collect, xprof failed to start";
VLOG(1) << "No trace data collected";
VLOG(1) << "No trace data collected, session wasn't started";
LOG(ERROR) << "Cannot collect, xprof failed to start";
VLOG(1) << "No trace data collected";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/profiler/internal/gpu/cupti_tracer.cc
LOG(ERROR) << status.error_message();
LOG(ERROR) << "Unsupported memcpy activity observed: " << cbid;
LOG(WARNING) << "Cupti Buffer not allocated, activity records will be dropped";
VLOG(3) << "Allocated Cupti Buffer, buffer=" << std::hex << reinterpret_cast<uintptr_t>(*buffer) << std::dec << " size=" << *size;
VLOG(3) << "Freeing Cupti Buffer, buffer:" << std::hex << reinterpret_cast<uintptr_t>(buffer) << std::dec << " size: " << size << " valid_size: " << valid_size;
VLOG(3) << "Activity profile for stream " << stream_id;
VLOG(3) << "Cuda Kernel Launched: " << event.name;
VLOG(3) << "Cuda Memcpy observed :" << num_bytes;
VLOG(3) << "Cuda P2P Memcpy observed, src: " << src_device << " dst: " << dst_device << " size:" << num_bytes;
VLOG(3) << "Cuda Malloc/Free observed: " << params->bytesize;
LOG(ERROR) << "Unexpected object kind: " << overhead->objectKind;
VLOG(3) << "Cuda Unified Memory Activity, kind: " << record->counterKind << " src: " << record->srcId << " dst: " << record->dstId;
LOG(INFO) << "Collecting " << kernel_records_.size() << " kernel records, " << memcpy_records_.size() << " memcpy records.";
VLOG(1) << "Unhandled cuLaunchCooperativeKernelMultiDevice.";
VLOG(1) << "Unexpected callback id: " << cbid;
VLOG(1) << "Unexpected callback id: " << cbid;
VLOG(3) << "Add annotation: device_id: " << device_id << " correlation_id: " << correlation_id << " annotation: " << annotation;
LOG(INFO) << "Profiler found " << gpu_count << " GPUs";
VLOG(1) << "Enable subscriber";
VLOG(1) << "Disable subscriber";
VLOG(1) << "Registering CUPTI activity callbacks";
VLOG(1) << "Enabling activity tracing for " << option_->activities_selected.size() << " activities";
VLOG(1) << "Enabling activity tracing for: " << activity;
VLOG(1) << "Disabling activity tracing for " << option_->activities_selected.size() << " activities";
VLOG(1) << "Disabling activity tracing for: " << activity;
VLOG(1) << "Flushing CUPTI activity buffer";
LOG(INFO) << "CUPTI activity buffer flushed";
VLOG(3) << "API callback received before creation of CUDA context";
LOG(ERROR) << "Unified memory is not supported on the " "underlying platform.";
LOG(ERROR) << "Unified memory is not supported on the device.";
LOG(ERROR) << "Unified memory is not supported on the " "non-P2P multi-gpu setup.";
LOG(ERROR) << "Error while enabling unified memory profiling: " << errstr;
VLOG(1) << "Configuring Unified memory profiling: " << res;
LOG(WARNING) << "CUPTI activity buffer is freed after flush.";
LOG(ERROR) << "Activity type " << record->kind << " not supported.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/profiler/utils/event_span.cc
LOG(INFO) << "Generation of step-events took " << elapsed_time_ms << " ms" << std::endl;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/profiler/utils/cost_utils.cc
LOG(ERROR) << "Unsupported Op for Roofline Cost Analysis are:" << absl::StrJoin(unsupported_ops_, ",");
VLOG(1) << tf_op.type << "[" << absl::StrJoin(input_tensors, ",") << "]" << " flops:" << costs.compute_time.count() << " bytes:" << costs.memory_time.count();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/profiler/utils/hardware_type_utils.cc
LOG(ERROR) << "Invalid GPU compute capability.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/profiler/lib/profiler_session.cc
LOG(WARNING) << "ProfilerSession: " << s.error_message();
LOG(INFO) << "Profiler session started.";
LOG(WARNING) << "Encountered error while starting profiler: " << start_status.ToString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/profiler/rpc/profiler_server.cc
LOG(INFO) << "Profiling Server listening on " << server_address;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/profiler/rpc/profiler_service_impl.cc
LOG(INFO) << "Received a profile request: " << req->DebugString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/profiler/convert/xplane_to_op_metrics_db.cc
VLOG(1) << "No begin event found for TF activity id=" << tf_op_id << " name=" << activity.tf_op.name << " type=" << activity.tf_op.type;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/profiler/convert/step_events_to_steps_db.cc
VLOG(2) << std::endl << "step_id: " << step << ", step_info:" << std::endl << DebugStepInfo( (*per_core_step_info.mutable_step_info_per_core())[0]);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/profiler/convert/op_stats_to_input_pipeline_analysis.cc
LOG(ERROR) << "Unable to unpack step_breakdown. Expected: generic" << std::endl;
LOG(ERROR) << "Unable to unpack step_breakdown. Expected: generic" << std::endl;
LOG(ERROR) << "Unable to unpack step_breakdown. Expected: generic" << std::endl;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/distributed_runtime/message_wrappers.cc
LOG(FATAL) << "Cannot get a mutable protobuf for an InMemoryRunGraphResponse";
LOG(FATAL) << "Cannot get a mutable protobuf for an InMemoryRunStepResponse";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/distributed_runtime/master_session.cc
VLOG(1) << "Created ReffedClientGraph for node with " << client_graph_before_register_->graph.num_node_ids();
LOG(ERROR) << "Wrong step_id in LoggingResponse";
VLOG(2) << "Register " << c->req.graph_def().DebugString();
LOG(ERROR) << "RunStep still blocked after 60 seconds. Failed with error status: " << status();
LOG(ERROR) << "- No response from RunGraph call to worker: " << *call.worker_name;
VLOG(1) << "Master received error status " << s
VLOG(1) << "Master received error report. Cancelling remaining workers.";
LOG(INFO) << "Client requested cancellation for RunStep, cancelling " "worker operations.";
VLOG(2) << "RunPartitions step_id " << step_id << " execution_count " << execution_count;
VLOG(2) << "RunPartitions step_id " << step_id << " execution_count " << execution_count;
VLOG(1) << "Device " << dev_name << " reports stats for " << ds.node_stats_size() << " nodes";
LOG(WARNING) << "Failed to find node " << ns.node_name() << " for dev " << dev_name;
LOG(INFO) << "DeregisterGraph error: " << s;
VLOG(1) << "Session " << handle_ << " #local " << env->local_devices.size() << " #remote " << remote_devs_->size();
VLOG(1) << "Start master session " << handle_ << " with config: " << session_opts_.config.ShortDebugString();
LOG(WARNING) << "Distributed session does not support the " "place_pruned_graph option.";
LOG(WARNING) << "ConfigProto.Experimental.share_cluster_devices_in_session has " "been promoted to a non-experimental API. Please use " "ConfigProto.share_cluster_devices_in_session instead. The " "experimental option will be removed in the future.";
LOG(WARNING) << status;
LOG(WARNING) << status;
VLOG(1) << "Unseen hash " << hash << " for " << BuildGraphOptionsString(opts) << " is_partial = " << is_partial << "";
VLOG(1) << "Preparing to execute new graph";
VLOG(1) << "Discarding all reffed graphs";
LOG(ERROR) << "Bad status from " "collective_executor_mgr->RefreshStepIdSequence: " << status << ". Retrying.";
LOG(ERROR) << "Cleanup partition error: " << s;
LOG(ERROR) << "Cleanup partition error: " << s;
VLOG(2) << "DoRunWithLocalExecution req: " << req.DebugString();
VLOG(2) << "DoRunCallable req: " << req.DebugString();
LOG(WARNING) << s;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/distributed_runtime/master.cc
LOG(WARNING) << "GC session " << sess->handle() << " after " << session_gc_seconds_ << " seconds. " << "Note that if you are starting multiple replicas " << "on a staggered delay, session_gc_seconds may need " << "to be raised.";
LOG(FATAL) << "Skipping invalid filter: " << filter;
VLOG(2) << "Selectively listing workers in job: " << job_name;
VLOG(2) << "Listing workers in all jobs because some device " << "filter has no job specified. Filters were:";
VLOG(2) << "- <NO FILTERS>";
VLOG(2) << "- " << filter;
LOG(INFO) << "CreateSession still waiting for response from worker: " << targets_[i];
LOG(ERROR) << "CreateSession failed because worker " << targets_[target_index] << " returned error: " << s;
LOG(ERROR) << status;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/distributed_runtime/session_mgr.cc
VLOG(1) << "ClusterSpec propagation is enabled.";
VLOG(1) << "Session state isolation is disabled.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/distributed_runtime/remote_device.cc
LOG(FATAL) << "Accessing the resource manager of a remote device is not " << "supported.";
LOG(WARNING) << s;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/distributed_runtime/graph_mgr.cc
LOG(ERROR) << status.error_message();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/distributed_runtime/cluster_function_library_runtime.cc
VLOG(1) << "CFLR::Instantiate: " << function_name << " on " << target << " (this: " << this << ")";
VLOG(1) << "CFLR::Instantiate: [Success] " << function_name << " on " << target << " (this: " << this << ")" << " with handle: " << *handle;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/distributed_runtime/worker_session.cc
LOG(WARNING) << "Error during worker session deletion: " << s;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/distributed_runtime/worker.cc
LOG(INFO) << "Cancellation requested for RunGraph.";
LOG(INFO) << "Cancellation requested for PartialRunGraph.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/distributed_runtime/collective_param_resolver_distributed.cc
VLOG(1) << "CompleteParamResolverDistributed ctor task={" << task_name << "} config.collective_group_leader={" << config.experimental().collective_group_leader() << "}" << " config.collective_nccl={" << config.experimental().collective_nccl() << "}";
VLOG(1) << "CompleteParams distributed " << device << " for " << cp << ": " << cp->ToString();
LOG(ERROR) << "Bad status from CompleteGroupDistributed: " << s;
VLOG(1) << "New cp " << cp << " for device " << *device << " : " << cp->ToString();
VLOG(2) << "Group communicator_key=" << absl::CEscape(gr->group.runtime_details.communicator_key);
VLOG(2) << "UpdateGroupCache: communicator_key=" << absl::CEscape(gr->group.runtime_details.communicator_key);
VLOG(1) << "CompleteGroupDistributed group_key=" << cp->group.group_key << " dev: " << device << " is_leader=" << (group_leader_.empty());

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/distributed_runtime/server_lib.cc
LOG(ERROR) << "Two server factories are being registered under " << server_type;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/distributed_runtime/base_rendezvous_mgr.cc
VLOG(1) << "Skipping rendezvous re-initialization.";
LOG(WARNING) << s;
VLOG(1) << "BaseRemoteRendezvous Send " << this << " " << parsed.FullKey();
VLOG(1) << "RemoteRendezvous Recv " << this << " " << parsed.FullKey();
VLOG(2) << "RemoteRendezvous Finished Recv " << this << " " << parsed.FullKey();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/distributed_runtime/rpc_collective_executor_mgr.cc
LOG(ERROR) << "Bad response [" << s << "] from GetStepSequenceAsync call to " << group_leader_;
LOG(ERROR) << "GetStepSequence called at non-group-leader";
LOG(ERROR) << "Failed to find graph_key " << graph_key << " to retire.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/distributed_runtime/rpc/grpc_worker_service.cc
VLOG(1) << "Bad response from GetStepSequence:" << s;
VLOG(1) << "Clean cache entry for request " << call->request.request_id();
VLOG(1) << "RunGraph::Done";
VLOG(1) << "Bad response from RunGraph:" << s;
VLOG(1) << "Bad response from RecvTensor:" << s;
VLOG(1) << "Bad response from RecvBuf:" << s;
VLOG(1) << "Bad response from CompleteGroup:" << s;
VLOG(1) << "Bad response from CompleteInstance:" << s;
LOG(INFO) << "Shutting down GrpcWorkerService.";
VLOG(1) << "Enabling gRPC tensor response cache.";
VLOG(1) << "GrpcRecvTensorAsync req: " << request->DebugString();
LOG(WARNING) << "RecvTensor cancelled for " << step_id;
LOG(ERROR) << "Got hook " << hook << " with status " << s << " from ConsumeBuf";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/distributed_runtime/rpc/grpc_channel.cc
LOG(ERROR) << "Invalid GRPC options format: " << grpc_option;
VLOG(3) << "Setting GRPC default for '" << name_value[0] << "' to '" << name_value[1] << "'";
LOG(ERROR) << "Failed to parse escaped string for " << grpc_option << ": " << error;
LOG(ERROR) << "Invalid integer value: " << grpc_option;
VLOG(5) << "Setting GRPC compression : algo='" << rpc_options->compression_algorithm() << "' level=" << rpc_options->compression_level();
VLOG(5) << "Setting GRPC compression : algo='" << rpc_options->compression_algorithm() << "' level=" << rpc_options->compression_level();
LOG(ERROR) << "Invalid compression algorithm: " << rpc_options->compression_algorithm();
VLOG(5) << "Disabling TCP connection sharing";
LOG(INFO) << "Initialize GrpcChannelCache for job " << ToString();
LOG(WARNING) << "Invalid target: " << target;
LOG(WARNING) << "Replica ID must be 0 in target: " << target;
LOG(WARNING) << "Task " << task << " was not defined in sparse job " << job_id_ << ": " << target;
LOG(ERROR) << "Empty channel spec.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/distributed_runtime/rpc/grpc_session.cc
LOG(WARNING) << "GrpcSession::ListDevices will initialize the session with " "an empty graph and other defaults because the session has " "not yet been created.";
LOG(ERROR) << "Could not list devices: " << s;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/distributed_runtime/rpc/grpc_remote_master.cc
LOG(WARNING) << "Too many retries, returning last status: " << s;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/distributed_runtime/rpc/grpc_tensorflow_server.cc
LOG(INFO) << "Peer " << job_name << " " << num_tasks << " {" << absl::StrJoin(host_ports, ", ") << "}";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/distributed_runtime/rpc/grpc_response_cache.cc
VLOG(1) << "GrpcResponseCache Lookup " << request_id;
VLOG(1) << "Reuse cached response for " << request_id;
VLOG(1) << "Found active request for " << request_id << ". Adding entry to response queue.";
VLOG(2) << "No cache entry for " << request_id << ", running user computation.";
LOG(ERROR) << "Unexpected missing response cache entry for request " << request_id;
VLOG(1) << "Operation for " << request_id << " finished. " << "Status: " << status << ", tensor size " << tensor.TotalBytes() << " bytes, " << entry.callbacks.size() << " pending callbacks.";
VLOG(1) << "Erase stale GrpcResponseCache entry " << it->first;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/distributed_runtime/rpc/grpc_state.cc
VLOG(3) << "Completing exchange " << DebugString() << " with " << status.ToString();
LOG(FATAL) << "Found an impossible state transition in the exchange queue: " << p.first << " -> " << p.second;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc
LOG(ERROR) << "Error parsing TF_GRPC_WORKER_CACHE_QUEUES: " << status;
LOG(ERROR) << "Error parsing TF_GRPC_WORKER_CACHE_THREADS: " << status;
LOG(WARNING) << s;
LOG(INFO) << "Started server with target: " << target();
LOG(INFO) << "Server already started (target: " << target() << ")";
LOG(FATAL);
LOG(INFO) << "Server already stopped (target: " << target() << ")";
LOG(FATAL);
LOG(FATAL);
LOG(ERROR) << s;
LOG(ERROR) << s;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/distributed_runtime/rpc/grpc_remote_worker.cc
VLOG(2) << "done callback, req: " << request->DebugString() << " response " << response->DebugString();
VLOG(1) << "RecvTensorAsync req: " << request->DebugString();
LOG(WARNING) << "Bad key: " << key;
VLOG(2) << "done callback, req: " << request->DebugString() << " response " << response->metadata().DebugString();
VLOG(2) << "Send MarkRecvFinishedRequest for request " << request_id;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/distributed_runtime/rpc/grpc_master_service.cc
LOG(INFO) << "Shutting down GrpcMasterService.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/distributed_runtime/rpc/grpc_worker_service_impl.cc
LOG(FATAL) << "Invalid id: this line shouldn't be reached.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/distributed_runtime/rpc/eager/grpc_eager_client.cc
VLOG(4) << "GrpcEagerClientThread got next tag";
VLOG(4) << "GrpcEagerClientThread blocking for next tag";
VLOG(4) << "GrpcEagerClientThread exiting";
VLOG(1) << "Sending RPC to close remote eager context " << request->DebugString();
LOG(ERROR) << "Remote EagerContext with id " << request->context_id() << " does not seem to exist.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/distributed_runtime/eager/remote_tensor_handle_data.cc
LOG_EVERY_N_SEC(INFO, 60) << "Unable to destroy remote tensor handle because the target " << remote_task << " is no longer available.";
VLOG(3) << "Sending request to delete " << request->DebugString();
LOG_EVERY_N_SEC(WARNING, 60) << "Unable to destroy remote tensor handles. If you are " "running a tf.function, it usually indicates some op in " "the graph gets an error: " << status.error_message();
LOG_EVERY_N_SEC(WARNING, 60) << "Unable to destroy remote tensor handles. If you are " "running a tf.function, it usually indicates some op in " "the graph gets an error: " << status.error_message();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/distributed_runtime/eager/eager_service_impl.cc
VLOG(2) << "Creating context on /job:" << request->server_def().job_name() << "/task:" << request->server_def().task_index();
VLOG(2) << " " << da.name();
LOG(WARNING) << "Failed to destroy worker session '" << session_name << "' due to " << s.error_message();
LOG(INFO) << "Creating " << (request->async() ? "async" : "sync") << " eager service context with rendezvous_id on host " << port::Hostname() << " " << worker_session->worker_name();
VLOG(1) << "EagerContext::InitializeRemoteWorker failed with " << s.ToString();
VLOG(1) << "Processing simplified UpdateContextRequest on " << ctx->HostCPU()->name();
VLOG(1) << "On existing server " << worker_session->worker_name() << " updating remote workers";
VLOG(2) << "Remote worker " << rw;
VLOG(1) << "EagerContext::UpdateRemoteWorker failed with " << s.ToString();
VLOG(3) << "ServerContext: Calling EagerExecute for op " << operation.id();
VLOG(1) << "Executing EagerService::CloseContext for context " << request->context_id();
LOG(INFO) << "Ignoring CloseContext request with a stale context_view_id " << request->context_view_id() << " for context_id " << request->context_id() << ". The current context_view_id is " << context->Context()->GetContextViewId() << ".";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/distributed_runtime/eager/remote_mgr.cc
LOG(ERROR) << "EagerExecutor shutdown with error " << s.error_message();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/distributed_runtime/eager/cluster_function_library_runtime.cc
VLOG(1) << "CFLR::Instantiate: " << function_name << " on " << target << " (this: " << this << ")";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/distributed_runtime/eager/remote_execute_node.cc
VLOG(3) << "Issuing: " << rpc_description;
VLOG(3) << "Completed successfully: " << rpc_description;
VLOG(3) << "Failed: " << rpc_description << " with status " << status.ToString();
LOG(ERROR) << "Ignoring an error encountered when setting " "remote shape of tensor handle: " << retvals[i] << " with status: " << status.ToString() << "This should never happen. " "Please file an issue with the TensorFlow Team.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/distributed_runtime/eager/remote_copy_node.cc
LOG(ERROR) << "Ignoring an error encountered when setting remote " "shape of tensor received by remote Recv op: " << status.ToString() << "This should never happen. " "Please file an issue with the TensorFlow Team.";
LOG(ERROR) << "Ignoring an error encountered when setting remote " "shape of tensor received by SendTensor rpc: " << status.ToString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/summary/summary_db_writer.cc
LOG(INFO) << "IdAllocator collision at tier " << tier_ << " (of " << kMaxIdTier << ") so auto-adjusting to a higher tier";
LOG(WARNING) << "IdAllocator (attempt #" << i << ") " << "resulted in a collision at the highest tier; this " "is problematic if it happens often; you can try " "pruning the Ids table; you can also file a bug " "asking for the ID space to be increased; otherwise " "writes will gradually slow down over time until they " "become impossible";
LOG(ERROR) << s.ToString();
LOG(ERROR) << "Failed to set Runs[" << run_id << "].finish_time: " << s.ToString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/summary/loader.cc
LOG(INFO) << "Opening SQLite file: " << path;
LOG(INFO) << "Initializing TensorBoard schema";
LOG(INFO) << "Creating SummaryDbWriter";
LOG(INFO) << "Loading TF event log: " << events;
LOG(FATAL) << "Corrupt tf.Event record" << " offset=" << (offset - record.size()) << " size=" << static_cast<int>(record.size());
LOG(INFO) << "Loaded " << AddCommas(offset) << " bytes with " << AddCommas(records) << " records at " << AddCommas(bps) << " bps";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/summary/vacuum.cc
LOG(INFO) << "Opening SQLite DB: " << path;
LOG(INFO) << "Deleting orphaned Experiments";
LOG(INFO) << "Deleting orphaned Runs";
LOG(INFO) << "Deleting orphaned Tags";
LOG(INFO) << "Deleting orphaned Tensors";
LOG(INFO) << "Deleting orphaned TensorStrings";
LOG(INFO) << "Deleting orphaned Graphs";
LOG(INFO) << "Deleting orphaned Nodes";
LOG(INFO) << "Deleting orphaned NodeInputs";
LOG(INFO) << "Running VACUUM";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/debug/debug_graph_utils.cc
LOG(INFO) << "Tolerating failure to create debug node: " << "tensor name = " << tensor_name << "; " << "debug op name = " << debug_op_name;
VLOG(1) << "Changing the parallel_iterations attribute of the " << "Enter/RefEnter node "" << node->name() << "" on device "" << device->name() << "" from " << parallel_iterations->i() << " to 1.";
LOG(INFO) << "For debugging, tfdbg has set the parallel_iterations " << "attribute of all scheduled Enter/RefEnter nodes to 1. (This " << "does not affect subsequent non-debug runs.)";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/debug/debug_io_utils.cc
LOG(WARNING) << "Failed to convert DebuggerEventMetadata proto to JSON. " << "The debug_node_name is " << debug_node_key.debug_node_name << ".";
LOG(ERROR) << "Attempt to disable a watch key that is not currently " << "enabled at " << grpc_debug_url << ": " << watch_key;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/debug/bfc_dump_reader.cc
LOG(ERROR) << "Failed to get size of " << fname;
LOG(ERROR) << "read from file " << fname << " failed " << status;
LOG(ERROR) << "read from file " << fname << " failed " << status;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/python/util/kernel_registry.cc
LOG(WARNING) << "Error parsing node_def";
LOG(WARNING) << "Op " << node_def.op() << " not found: " << status;
LOG(WARNING) << "Failed to parse device from node_def: " << node_def.ShortDebugString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/python/util/util.cc
LOG(WARNING) << "Sets are not currently considered sequences, " "but this may change in the future, " "so consider avoiding using them.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/python/grappler/cost_analyzer.cc
LOG(ERROR) << "Could not estimate the cost for item " << item_->id << ": " << status.error_message();
VLOG(1) << "Graph size: " << item_->graph.node_size();
VLOG(1) << "cost_graph_measured size: " << cost_graph_measured.node_size();
VLOG(1) << "cost_graph_analytical size: " << cost_graph_analytical.node_size();
VLOG(1) << "cost_graph_analytical_filtered size: " << cost_graph_analytical_filtered.node_size();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/python/lib/core/bfloat16.cc
LOG(FATAL) << "Invalid op type " << op;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/python/lib/core/py_func.cc
LOG(WARNING) << "InitializeCallback should only be called once";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/python/profiler/internal/profiler_wrapper.cc
LOG(INFO) << ss.str();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/python/client/session_ref.cc
LOG(INFO) << "Constructing new session logger for " << log_name;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/python/eager/pywrap_tfe_src.cc
VLOG(1) << "Gradient length is " << len;
VLOG(1) << "Falling back to slow path for Op "" << op_def.name() << "", Input "" << op_def.input_arg(i).name() << "" since we expected a sequence, but got " << item->ob_type->tp_name;
VLOG(1) << "Falling back to slow path for Op "" << op_def.name() << "", Input "" << op_def.input_arg(i).name() << "", Index " << j << " since we expected an EagerTensor/ResourceVariable, " "but got " << inner_item->ob_type->tp_name;
VLOG(1) << "Falling back to slow path for Op "" << op_def.name() << "", Input "" << op_def.input_arg(i).name() << "" since we expected an EagerTensor/ResourceVariable, but got " << item->ob_type->tp_name;
VLOG(1) << "Falling back to slow path for Op "" << op_def->name() << "" since we are unable to set the value for attr "" << attr.name() << "" due to: " << TF_Message(status);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/python/eager/pywrap_tensor.cc
LOG(ERROR) << "Invoking created() on EagerTensor profiler failed";
LOG(ERROR) << "Cannot create an eager tensor before eager context has " "been set or after it has been deleted";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/js/ops/ts_op_gen.cc
LOG(WARNING) << "Could not find OpDef::ArgDef for " << api_def_.arg_order(i);
LOG(WARNING) << "Could not find ApiDef::Arg for " << api_def_.arg_order(i);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/interpreter.cc
TFLITE_LOG_PROD_ONCE(TFLITE_LOG_INFO, "Initialized TensorFlow Lite runtime.");
TFLITE_LOG_ONCE(TFLITE_LOG_INFO, "Initialized TensorFlow Lite runtime.");

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/micro/kernels/concatenation.cc
TF_LITE_KERNEL_LOG( context, "Op Concatenation does not currently support num dimensions >4 " "Tensor '%s' has %d dimensions.", input->name, num_dimensions);
TF_LITE_KERNEL_LOG( context, "Op Concatenation does not currently support Type '%s'.", TfLiteTypeGetName(output_type));

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/micro/kernels/conv.cc
TF_LITE_KERNEL_LOG(context, "Type %s (%d) not supported.", TfLiteTypeGetName(input->type), input->type);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/micro/kernels/depthwise_conv.cc
TF_LITE_KERNEL_LOG(context, "Type %s (%d) not supported.", TfLiteTypeGetName(input->type), input->type);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/micro/kernels/softmax.cc
TF_LITE_KERNEL_LOG( context, "Only 1D, 2D and 4D tensors supported currently, got %dD.", NumDimensions(input));
TF_LITE_KERNEL_LOG(context, "Only 2D and 4D tensors supported currently, got %dD.", NumDimensions(input));
TF_LITE_KERNEL_LOG( context, "Only float32, uint8_t and int8_t supported currently, got %d.", input->type);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/micro/kernels/strided_slice.cc
TF_LITE_KERNEL_LOG(context, "Type %d is currently not supported " "by StridedSlice.", op_context.input->type);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/micro/kernels/prelu.cc
TF_LITE_KERNEL_LOG( context, "Only float32 and uint8 are supported currently, got %d.", TfLiteTypeGetName(input->type));

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/micro/kernels/split.cc
TF_LITE_KERNEL_LOG(context, "Type %s currently not supported.", TfLiteTypeGetName(input->type));

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/micro/kernels/fully_connected.cc
TF_LITE_KERNEL_LOG( context, "Quantized FullyConnected expects output data type uint8 or int16");
TF_LITE_KERNEL_LOG(context, "Type %d not currently supported.", filter->type);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/micro/kernels/pad.cc
TF_LITE_KERNEL_LOG(context, "Type %s not currently supported by Pad.", TfLiteTypeGetName(op_context.input->type));

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/micro/kernels/mul.cc
TF_LITE_KERNEL_LOG(context, "Type %d not currently supported.", input1->type);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/micro/kernels/dequantize.cc
TF_LITE_KERNEL_LOG(context, "Input %s, output %s not supported.", TfLiteTypeGetName(input->type), TfLiteTypeGetName(output->type));
TF_LITE_KERNEL_LOG(context, "Input %s, output %s not supported.", TfLiteTypeGetName(input->type), TfLiteTypeGetName(output->type));
TF_LITE_KERNEL_LOG(context, "Input %s, output %s not supported.", TfLiteTypeGetName(input->type), TfLiteTypeGetName(output->type));

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/micro/kernels/unpack.cc
TF_LITE_KERNEL_LOG(context, "Type '%s' is not supported by unpack.", TfLiteTypeGetName(input->type));

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/micro/kernels/arg_min_max.cc
TF_LITE_KERNEL_LOG(context, "Only float32, uint8 and int8 are " "supported currently, got %s.", TfLiteTypeGetName(input->type));
TF_LITE_KERNEL_LOG(context, "Only int32 are supported currently, got %s.", TfLiteTypeGetName(output->type));
TF_LITE_KERNEL_LOG(context, "Only int32 are supported currently, got %s.", TfLiteTypeGetName(axis->type));

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/micro/kernels/activations.cc
TF_LITE_KERNEL_LOG(context, "Only float32 is supported currently, got %s", TfLiteTypeGetName(input->type));
TF_LITE_KERNEL_LOG(context, "Only float32 is supported currently, got %s", TfLiteTypeGetName(input->type));

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/micro/kernels/logistic.cc
TF_LITE_KERNEL_LOG(context, "Input %s, output %s not supported.", TfLiteTypeGetName(input->type), TfLiteTypeGetName(output->type));
TF_LITE_KERNEL_LOG(context, "Input %s, output %s not supported.", TfLiteTypeGetName(input->type), TfLiteTypeGetName(output->type));
TF_LITE_KERNEL_LOG(context, "Input %s, output %s not supported.", TfLiteTypeGetName(input->type), TfLiteTypeGetName(output->type));

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/micro/kernels/elementwise.cc
TF_LITE_KERNEL_LOG(context, "Input data type %s (%d) is not supported.", TfLiteTypeGetName(input->type), input->type);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/micro/kernels/pooling.cc
TF_LITE_KERNEL_LOG(context, "Input type %s is not currently supported", TfLiteTypeGetName(input->type));
TF_LITE_KERNEL_LOG(context, "Type %s not currently supported.", TfLiteTypeGetName(input->type));

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/micro/kernels/svdf.cc
TF_LITE_KERNEL_LOG(context, "Type %s not currently supported.", TfLiteTypeGetName(weights_feature->type));

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/micro/kernels/maximum_minimum.cc
TF_LITE_KERNEL_LOG(context, "Type %s (%d) is not supported by Maximum/Minimum.", TfLiteTypeGetName(op_context.output->type), op_context.output->type);
TF_LITE_KERNEL_LOG(context, "Kernel type not supported by Maximum/Minimum.");

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/micro/kernels/pack.cc
TF_LITE_KERNEL_LOG(context, "Type '%s' is not supported by pack.", TfLiteTypeGetName(output->type));

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/micro/kernels/comparisons.cc
TF_LITE_KERNEL_LOG( context, "Does not support type %d, requires bool|float|int|uint8", input1->type);
TF_LITE_KERNEL_LOG( context, "Does not support type %d, requires bool|float|int|uint8", input1->type);
TF_LITE_KERNEL_LOG(context, "Does not support type %d, requires float|int|uint8", input1->type);
TF_LITE_KERNEL_LOG(context, "Does not support type %d, requires float|int|uint8", input1->type);
TF_LITE_KERNEL_LOG(context, "Does not support type %d, requires float|int|uint8", input1->type);
TF_LITE_KERNEL_LOG(context, "Does not support type %d, requires float|int|uint8", input1->type);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/micro/kernels/quantize.cc
TF_LITE_KERNEL_LOG(context, "Input %s, output %s not supported.", TfLiteTypeGetName(input->type), TfLiteTypeGetName(output->type));
TF_LITE_KERNEL_LOG(context, "Input %s, output %s not supported.", TfLiteTypeGetName(input->type), TfLiteTypeGetName(output->type));
TF_LITE_KERNEL_LOG(context, "Input %s, output %s not supported.", TfLiteTypeGetName(input->type), TfLiteTypeGetName(output->type));

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/micro/kernels/neg.cc
TF_LITE_KERNEL_LOG( context, "Neg only currently supports float32, got %d.", input->type);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/micro/kernels/add.cc
TF_LITE_KERNEL_LOG(context, "Inputs and outputs not all float|uint8|int8 types.");

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/micro/kernels/cmsis-nn/conv.cc
TF_LITE_KERNEL_LOG(context, "Type %s (%d) not supported.", TfLiteTypeGetName(input->type), input->type);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/micro/kernels/cmsis-nn/depthwise_conv.cc
TF_LITE_KERNEL_LOG(context, "Type %s (%d) not supported.", TfLiteTypeGetName(input->type), input->type);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/micro/kernels/cmsis-nn/fully_connected.cc
TF_LITE_KERNEL_LOG( context, "Quantized FullyConnected expects output data type uint8 or int16");
TF_LITE_KERNEL_LOG(context, "Type %d not currently supported.", filter->type);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/micro/kernels/cmsis-nn/mul.cc
TF_LITE_KERNEL_LOG(context, "Type %s (%d) not supported.", TfLiteTypeGetName(input1->type), input1->type);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/micro/kernels/cmsis-nn/pooling.cc
TF_LITE_KERNEL_LOG(context, "Input type %s is not currently supported", TfLiteTypeGetName(input->type));
TF_LITE_KERNEL_LOG(context, "Type %s not currently supported.", TfLiteTypeGetName(input->type));

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/micro/kernels/cmsis-nn/add.cc
TF_LITE_KERNEL_LOG(context, "Inputs and outputs not all float|uint8|int8 types.");

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/micro/kernels/arc/conv.cc
TF_LITE_KERNEL_LOG(context, "Type %s (%d) not supported.", TfLiteTypeGetName(input->type), input->type);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/micro/kernels/arc/depthwise_conv.cc
TF_LITE_KERNEL_LOG(context, "Type %s (%d) not supported.", TfLiteTypeGetName(input->type), input->type);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/micro/kernels/arc/fully_connected.cc
TF_LITE_KERNEL_LOG( context, "Quantized FullyConnected expects output data type uint8 or int16");
TF_LITE_KERNEL_LOG(context, "Type %d not currently supported.", filter->type);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/micro/kernels/arc/pooling.cc
TF_LITE_KERNEL_LOG(context, "Input type %s is not currently supported", TfLiteTypeGetName(input->type));
TF_LITE_KERNEL_LOG(context, "Type %s not currently supported.", TfLiteTypeGetName(input->type));

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/micro/kernels/xtensa-hifimini/conv.cc
TF_LITE_KERNEL_LOG(context, "Type %s (%d) not supported.", TfLiteTypeGetName(input->type), input->type);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/micro/kernels/xtensa-hifimini/depthwise_conv.cc
TF_LITE_KERNEL_LOG(context, "Type %s (%d) not supported.", TfLiteTypeGetName(input->type), input->type);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/micro/kernels/xtensa-hifimini/softmax.cc
TF_LITE_KERNEL_LOG(context, "Only 2D tensors supported currently, got %dD.", NumDimensions(input));
TF_LITE_KERNEL_LOG(context, "Only int8_t supported currently, got %d.", input->type);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/micro/kernels/xtensa-hifimini/fully_connected.cc
TF_LITE_KERNEL_LOG(context, "Type %d not currently supported.", filter->type);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/micro/kernels/xtensa-hifimini/svdf.cc
TF_LITE_KERNEL_LOG(context, "HiFi Mini kernel SVDF only supports full integer.");

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/micro/kernels/xtensa-hifimini/quantize.cc
TF_LITE_KERNEL_LOG(context, "Input %s, output %s not supported.", TfLiteTypeGetName(input->type), TfLiteTypeGetName(output->type));

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/micro/kernels/portable_optimized/depthwise_conv.cc
TF_LITE_KERNEL_LOG( context, "Size too large for reshaped weight buffer (%d needed, %d available)", needed_size, kReshapedFilterDataSize);
TF_LITE_KERNEL_LOG( context, "Multiple depthwise conv ops match optimization parameters, but " "only the first will use the fast path, because there's only one " "RAM cache available");
TF_LITE_KERNEL_LOG(context, "Type %s (%d) not supported.", TfLiteTypeGetName(input->type), input->type);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/micro/examples/micro_speech/esp/audio_provider.cc
ESP_LOGE(TAG, "Error in i2s_driver_install");
ESP_LOGE(TAG, "Error in i2s_set_pin");
ESP_LOGE(TAG, "Error in initializing dma buffer with 0");
ESP_LOGE(TAG, "Error in I2S read : %d", bytes_read);
ESP_LOGW(TAG, "Partial I2S read");
ESP_LOGE(TAG, "Could Not Write in Ring Buffer: %d ", bytes_written);
ESP_LOGW(TAG, "Partial Write");
ESP_LOGE(TAG, "Error creating ring buffer");
ESP_LOGE(TAG, " Model Could not read data from Ring Buffer");
ESP_LOGV(TAG, " Could only read %d bytes when required %d bytes ", bytes_read, new_samples_to_get * sizeof(int16_t));

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/experimental/delegates/hexagon/hexagon_implementation.cc
TFLITE_LOG_PROD(TFLITE_LOG_ERROR, "Function %s is NULL", name);
TFLITE_LOG_PROD(TFLITE_LOG_ERROR, "Failed to load libhexagon_interface.so, Error: %s", dlerror());

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/experimental/delegates/hexagon/hexagon_delegate.cc
TFLITE_LOG_PROD(tflite::TFLITE_LOG_INFO, "Hexagon delegate: %d nodes delegated out of %d nodes.", supported_nodes[0], plan->size);
TFLITE_LOG_PROD_ONCE(tflite::TFLITE_LOG_INFO, "Hexagon Delegate is not supported.");
TFLITE_LOG_PROD_ONCE(tflite::TFLITE_LOG_INFO, "Created TensorFlow Lite delegate for Hexagon.");

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/tools/command_line_flags.cc
TFLITE_LOG(TFLITE_LOG_ERROR, "Too few command line arguments");
TFLITE_LOG(TFLITE_LOG_ERROR, "Failed to parse positional flag: %s", flag.name_.c_str());
TFLITE_LOG(TFLITE_LOG_ERROR, "Failed to parse flag: %s", flag.name_.c_str());
TFLITE_LOG(TFLITE_LOG_ERROR, "Required flag not provided: %s", flag.name_.c_str());

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/tools/benchmark/nnapi_delegate_provider.cc
TFLITE_LOG(INFO) << "Use nnapi : [" << params.Get<bool>("use_nnapi") << "]";
TFLITE_LOG(INFO) << "nnapi execution preference: [" << params.Get<std::string>("nnapi_execution_preference") << "]";
TFLITE_LOG(INFO) << log_string;
TFLITE_LOG(WARN) << "The provided value (" << string_execution_preference << ") is not a valid nnapi execution preference.";
TFLITE_LOG(WARN) << "NNAPI acceleration is unsupported on this platform.";
TFLITE_LOG(WARN) << "`--use_nnapi=true` must be set for the provided NNAPI accelerator (" << params.Get<std::string>("nnapi_accelerator_name") << ") to be used.";
TFLITE_LOG(WARN) << "`--use_nnapi=true` must be set for the provided NNAPI " "execution preference (" << params.Get<std::string>("nnapi_execution_preference") << ") to be used.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/tools/benchmark/benchmark_model.cc
TFLITE_LOG(INFO) << "Average inference timings in us: " << "Warmup: " << warmup_us.avg() << ", " << "Init: " << init_us << ", " << "Inference: " << inference_us.avg();
TFLITE_LOG(INFO) << "Min num runs: [" << params_.Get<int32_t>("num_runs") << "]";
TFLITE_LOG(INFO) << "Min runs duration (seconds): [" << params_.Get<float>("min_secs") << "]";
TFLITE_LOG(INFO) << "Max runs duration (seconds): [" << params_.Get<float>("max_secs") << "]";
TFLITE_LOG(INFO) << "Inter-run delay (seconds): [" << params_.Get<float>("run_delay") << "]";
TFLITE_LOG(INFO) << "Num threads: [" << params_.Get<int32_t>("num_threads") << "]";
TFLITE_LOG(INFO) << "Benchmark name: [" << params_.Get<std::string>("benchmark_name") << "]";
TFLITE_LOG(INFO) << "Output prefix: [" << params_.Get<std::string>("output_prefix") << "]";
TFLITE_LOG(INFO) << "Min warmup runs: [" << params_.Get<int32_t>("warmup_runs") << "]";
TFLITE_LOG(INFO) << "Min warmup runs duration (seconds): [" << params_.Get<float>("warmup_min_secs") << "]";
TFLITE_LOG(INFO) << "Running benchmark for at least " << min_num_times << " iterations and at least " << min_secs << " seconds but" << " terminate if exceeding " << max_secs << " seconds.";
TFLITE_LOG(INFO) << stream.str() << std::endl;
TFLITE_LOG(INFO) << "The input model file size (MB): " << model_size_mb;
TFLITE_LOG(INFO) << "Initialized session in " << startup_latency_us / 1e3 << "ms.";
TFLITE_LOG(INFO) << "Note: as the benchmark tool itself affects memory footprint, the " "following is only APPROXIMATE to the actual memory footprint of the " "model at runtime. Take the information at your discretion.";
TFLITE_LOG(INFO) << "Peak memory footprint (MB): init=" << init_mem_usage.max_rss_kb / 1024.0 << " overall=" << overall_mem_usage.max_rss_kb / 1024.0;
TFLITE_LOG(ERROR) << usage;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/tools/benchmark/benchmark_performance_options.cc
TFLITE_LOG(INFO) << "==============Summary of All Runs w/ Different " "Performance Options==============";
TFLITE_LOG(INFO) << stream.str();
TFLITE_LOG(ERROR) << usage;
TFLITE_LOG(ERROR) << "Cannot parse --perf_options_list: '" << perf_options_list << "'. Please double-check its value.";
TFLITE_LOG(ERROR) << "There are invalid perf options in --perf_options_list: '" << perf_options_list << "'. Valid perf options are: [" << valid_options_str << "]";
TFLITE_LOG(ERROR) << "The 'none' option can not be used together with " "other perf options in --perf_options_list!";
TFLITE_LOG(INFO) << "The list of TFLite runtime options to be benchmarked: [" << params_.Get<std::string>("perf_options_list") << "]";
TFLITE_LOG(WARN) << "WARNING: unrecognized commandline flag: " << argv[i];

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/tools/benchmark/benchmark_tflite_model.cc
TFLITE_LOG(ERROR) << "The number of items in" << " --input_layer_shape (" << shapes_string << ", with " << shapes.size() << " items)" << " must match the number of items in" << " --input_layer (" << names_string << ", with " << names.size() << " items)." << " For example --input_layer=input1,input2" << " --input_layer_shape=1,224,224,4:1,20";
TFLITE_LOG(ERROR) << "Any unknown sizes in the shapes (-1's) must be replaced" << " with the size you want to benchmark with.";
TFLITE_LOG(FATAL) << "Wrong input value range item specified: " << val;
TFLITE_LOG(FATAL) << "Wrong low and high value of the input value range specified: " << val;
TFLITE_LOG(INFO) << "Graph: [" << params_.Get<std::string>("graph") << "]";
TFLITE_LOG(INFO) << "Input layers: [" << params_.Get<std::string>("input_layer") << "]";
TFLITE_LOG(INFO) << "Input shapes: [" << params_.Get<std::string>("input_layer_shape") << "]";
TFLITE_LOG(INFO) << "Input value ranges: [" << params_.Get<std::string>("input_layer_value_range") << "]";
TFLITE_LOG(INFO) << "Use legacy nnapi : [" << params_.Get<bool>("use_legacy_nnapi") << "]";
TFLITE_LOG(INFO) << "Allow fp16 : [" << params_.Get<bool>("allow_fp16") << "]";
TFLITE_LOG(INFO) << "Require full delegation : [" << params_.Get<bool>("require_full_delegation") << "]";
TFLITE_LOG(INFO) << "Enable op profiling: [" << params_.Get<bool>("enable_op_profiling") << "]";
TFLITE_LOG(INFO) << "Max profiling buffer entries: [" << params_.Get<int32_t>("max_profiling_buffer_entries") << "]";
TFLITE_LOG(INFO) << "CSV File to export profiling data to: [" << params_.Get<std::string>("profiling_output_csv_file") << "]";
TFLITE_LOG(ERROR) << "Please specify the name of your TF Lite input file with --graph";
TFLITE_LOG(FATAL) << "Don't know how to populate tensor " << t->name << " of type FLOAT16 on this platform.";
TFLITE_LOG(FATAL) << "Populating the tensor " << t->name << " of type FLOAT16 is disabled.";
TFLITE_LOG(ERROR) << "Don't know how to populate tensor " << t->name << " of type " << t->type;
TFLITE_LOG(ERROR) << "Failed to construct interpreter";
TFLITE_LOG(ERROR) << "Failed to apply " << delegate.first << " delegate.";
TFLITE_LOG(ERROR) << "Disallowed CPU fallback detected.";
TFLITE_LOG(INFO) << "Applied " << delegate.first << " delegate.";
TFLITE_LOG(WARN) << "Tensor # " << i << " is named " << t->name << " but flags call it " << input.name;
TFLITE_LOG(ERROR) << "Failed to allocate tensors!";
TFLITE_LOG(ERROR) << "Failed to mmap model " << graph;
TFLITE_LOG(INFO) << "Loaded model " << graph;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/tools/benchmark/benchmark_main.cc
TFLITE_LOG(INFO) << "STARTING!";
TFLITE_LOG(ERROR) << "Benchmarking failed.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/tools/benchmark/hexagon_delegate_provider.cc
TFLITE_LOG(INFO) << "Use Hexagon : [" << params.Get<bool>("use_hexagon") << "]";
TFLITE_LOG(INFO) << "Hexagon lib path : [" << params.Get<std::string>("hexagon_lib_path") << "]";
TFLITE_LOG(INFO) << "Hexagon Profiling : [" << params.Get<bool>("hexagon_profiling") << "]";
TFLITE_LOG(WARN) << "Could not create Hexagon delegate: platform may not support " "delegate or required libraries are missing";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/tools/benchmark/gpu_delegate_provider.cc
TFLITE_LOG(INFO) << "Use gpu : [" << params.Get<bool>("use_gpu") << "]";
TFLITE_LOG(INFO) << "Allow lower precision in gpu : [" << params.Get<bool>("gpu_precision_loss_allowed") << "]";
TFLITE_LOG(INFO) << "GPU delegate wait type : [" << params.Get<std::string>("gpu_wait_type") << "]";
TFLITE_LOG(WARN) << "The GPU delegate compile options are only supported " "to be benchmarked on Android or iOS platforms.";
TFLITE_LOG(WARN) << "GPU acceleration is unsupported on this platform.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/tools/benchmark/benchmark_plus_flex_main.cc
TFLITE_LOG(INFO) << "STARTING!";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/tools/benchmark/benchmark_tflite_performance_options_main.cc
TFLITE_LOG(INFO) << "STARTING!";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/tools/benchmark/xnnpack_delegate_provider.cc
TFLITE_LOG(INFO) << "Use xnnpack : [" << params.Get<bool>("use_xnnpack") << "]";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/tools/optimize/quantization_utils.cc
TFLITE_LOG(TFLITE_LOG_ERROR, "No tensor to quantize.");
TFLITE_LOG(TFLITE_LOG_ERROR, "Missing buffer.");
TFLITE_LOG(TFLITE_LOG_ERROR, "No tensor to quantize.");
TFLITE_LOG(TFLITE_LOG_ERROR, "Missing buffer.");

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/tools/optimize/quantize_weights.cc
LOG(INFO) << "Skipping optional tensor input " << op_input_idx << " of operation " << EnumNameBuiltinOperator(op_code->builtin_code);
LOG(INFO) << "Skipping quantization of tensor " << tensor->name << " that is not type float.";
LOG(INFO) << "Skipping quantization of tensor " << tensor->name << " because it has fewer than " << weights_min_num_elements << " elements (" << num_elements << ").";
LOG(INFO) << "Skipping quantization of tensor " << tensor->name << " because it has no allocated buffer.";
LOG(ERROR) << "An op that passes quantization has more than one quantized output";
LOG(ERROR) << "An op that passes quantization has more than one quantized input";
LOG(ERROR) << "Quantize weights tool only supports tflite models with one " "subgraph.";
LOG(ERROR) << "Quantize weights tool only supports tflite models with one " "subgraph.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/tools/accuracy/ilsvrc/imagenet_accuracy_eval.cc
LOG(ERROR) << "Empty output file path.";
LOG(ERROR) << "Unable to open output file path: '" << output_file_path << "'";
LOG(ERROR) << "Starting model evaluation: " << total_num_images;
LOG(ERROR) << "Could not write to file";
LOG(ERROR) << "Evaluated " << num_evaluated << "/" << total_num_images_ << " images, " << std::setprecision(2) << std::fixed << current_percent << "%";
LOG(INFO) << "The result metrics proto is written to " << proto_output_file;
LOG(INFO) << "Metrics proto output file path is not specified!";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/tools/accuracy/ilsvrc/imagenet_model_evaluator.cc
LOG(ERROR) << "Invalid: num_examples";
LOG(ERROR) << "Unable to read ground truth labels from: " << params_.ground_truth_labels_path << " Perhaps file doesn't exist or is unreadable.";
LOG(ERROR) << "Images and ground truth labels don't match";
LOG(ERROR) << "Could not filter by blacklist";
LOG(ERROR) << "Could not read: " << params_.model_output_labels_path;
LOG(ERROR) << "Invalid number of labels: " << model_labels.size();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/tools/accuracy/ilsvrc/imagenet_accuracy_eval_main.cc
LOG(ERROR) << "Invalid number of threads.";
LOG(ERROR) << "Fail to create the ImagenetModelEvaluator.";
LOG(ERROR) << "Fail to create the ResultsWriter.";
LOG(ERROR) << "Starting evaluation with: " << num_threads << " threads.";
LOG(ERROR) << "Failed to evaluate the model!";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/tools/evaluation/tasks/inference_diff/run_eval.cc
LOG(ERROR) << "Could not evaluate model!";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/tools/evaluation/tasks/imagenet_image_classification/run_eval.cc
LOG(INFO) << "Evaluated: " << i / step << "%";
LOG(ERROR) << "Could not read ground truth labels file";
LOG(ERROR) << "Number of images and ground truth labels is not same";
LOG(ERROR) << "Could not read model output labels file";
LOG(ERROR) << "Could not evaluate model";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/tools/evaluation/tasks/coco_object_detection/run_eval.cc
LOG(INFO) << "Finished: " << i / step << "%";
LOG(ERROR) << "Could not read model output labels file";
LOG(ERROR) << "Could not evaluate model";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/tools/evaluation/stages/image_classification_stage.cc
LOG(ERROR) << "ImageClassificationParams not provided";
LOG(ERROR) << "Inference_params not provided";
LOG(ERROR) << "Model must have 1 input & 1 output";
LOG(ERROR) << "Invalid input shape for model";
LOG(ERROR) << "all_labels not set for TopkAccuracyEvalStage";
LOG(ERROR) << "Input image not set";
LOG(ERROR) << "Ground truth label not provided";
LOG(ERROR) << "Could not read: " << blacklist_file_path;
LOG(ERROR) << "Invalid number of filtered images";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/tools/evaluation/stages/inference_profiler_stage.cc
LOG(ERROR) << "InferenceProfilerStage only supports float/int8/uint8 " "input types";
LOG(ERROR) << "InferenceProfilerStage only supports float/int8/uint8 " "output types";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/tools/evaluation/stages/tflite_inference_stage.cc
LOG(ERROR) << "Stage not initialized before calling ApplyCustomDelegate";
LOG(WARNING) << "Tried to apply null TfLiteDelegatePtr to TfliteInferenceStage";
LOG(ERROR) << "TfliteInferenceParams not provided";
LOG(ERROR) << "Model path not provided";
LOG(ERROR) << "Model file not found";
LOG(ERROR) << "Could not build interpreter";
LOG(WARNING) << "NNAPI not supported";
LOG(WARNING) << "GPU not supported";
LOG(WARNING) << "Could not create Hexagon delegate: platform may not support " "delegate or required libraries are missing";
LOG(FATAL) << "Failed to apply delegate " << i;
LOG(ERROR) << "Input data not set";
LOG(ERROR) << "TFLite interpreter failed to invoke at run " << i;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/tools/evaluation/stages/topk_accuracy_eval_stage.cc
LOG(ERROR) << "Value of k not provided for TopkAccuracyEvalStage";
LOG(ERROR) << "Ground-truth labels are empty";
LOG(ERROR) << "k is too large";
LOG(ERROR) << "Model output details not correctly set";
LOG(ERROR) << "Invalid model_output_shape_";
LOG(ERROR) << "model_output_type_ not supported";
LOG(ERROR) << "model_output_ not set correctly";
LOG(ERROR) << "ground_truth_label_ not provided";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/tools/evaluation/stages/image_preprocessing_stage.cc
LOG(ERROR) << "No preprocessing params";
LOG(ERROR) << "Invalid cropping fraction";
LOG(ERROR) << "Image path not set";
LOG(ERROR) << "Extension " << image_ext << " is not supported";
LOG(WARNING) << "Image cropping will not be performed on raw images";
LOG(WARNING) << "Image resizing will not be performed on raw images";
LOG(WARNING) << "Image padding will not be performed on raw images";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/tools/evaluation/stages/object_detection_average_precision_stage.cc
LOG(ERROR) << "num_classes cannot be <= 0";
LOG(ERROR) << "Encountered invalid class ID: " << class_id;
LOG(ERROR) << "Encountered invalid class ID: " << class_id;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/tools/evaluation/stages/object_detection_stage.cc
LOG(ERROR) << "ObjectDetectionParams not provided";
LOG(ERROR) << "inference_params not provided";
LOG(ERROR) << "Detection output labels not provided";
LOG(ERROR) << "Object detection model must have 1 input & 4 outputs";
LOG(ERROR) << "Invalid input shape for model";
LOG(ERROR) << "Input image not set";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/tools/evaluation/stages/utils/image_metrics.cc
LOG(ERROR) << "recall points are not in order: " << pr[i - 1].r << ", " << item.r;
LOG(ERROR) << "Number of recall points should be > 0";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/tools/versioning/op_version.cc
LOG(ERROR) << "Can't access tenor " << idx;
LOG(ERROR) << "Can't set operator " << EnumNameBuiltinOperator(op_code->builtin_code()) << " to version " << op_ver;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/core/subgraph.cc
TFLITE_LOG( tflite::TFLITE_LOG_INFO, "Replacing %d node(s) with delegate (%s) node, yielding %zu partitions.", nodes_to_replace->size, registration.custom_name ? registration.custom_name : "unknown", node_subsets.size());

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/nnapi/nnapi_implementation.cc
NNAPI_LOG("nnapi error: unable to open function %s", name);
NNAPI_LOG("nnapi error: requires android sdk version to be at least %d", 27);
NNAPI_LOG("nnapi error: unable to open library %s", "libneuralnetworks.so");
NNAPI_LOG("nnapi error: unable to open neither libraries %s and %s", "libandroid.so", "libcutils.so");

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/examples/label_image/bitmap_helpers.cc
LOG(FATAL) << "Unexpected number of channels: " << channels;
LOG(FATAL) << "input file " << input_bmp_name << " not found";
LOG(INFO) << "len: " << len << "";
LOG(INFO) << "width, height, channels: " << *width << ", " << *height << ", " << *channels << "";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/examples/label_image/label_image.cc
LOG(INFO) << "GPU acceleration is unsupported on this platform.";
LOG(INFO) << "NNAPI acceleration is unsupported on this platform.";
LOG(INFO) << "Hexagon acceleration is unsupported on this platform.";
LOG(FATAL) << "Labels file " << file_name << " not found";
LOG(INFO) << std::fixed << std::setw(10) << std::setprecision(3) << (e->end_timestamp_us - e->begin_timestamp_us) / 1000.0 << ", Subgraph " << std::setw(3) << std::setprecision(3) << subgraph_index << ", Node " << std::setw(3) << std::setprecision(3) << op_index << ", OpCode " << std::setw(3) << std::setprecision(3) << registration.builtin_code << ", " << EnumNameBuiltinOperator( static_cast<BuiltinOperator>(registration.builtin_code)) << "";
LOG(ERROR) << "no model file name";
LOG(FATAL) << "Failed to mmap model " << s->model_name << "";
LOG(INFO) << "Loaded model " << s->model_name << "";
LOG(INFO) << "resolved reporter";
LOG(FATAL) << "Failed to construct interpreter";
LOG(INFO) << "tensors size: " << interpreter->tensors_size() << "";
LOG(INFO) << "nodes size: " << interpreter->nodes_size() << "";
LOG(INFO) << "inputs: " << interpreter->inputs().size() << "";
LOG(INFO) << "input(0) name: " << interpreter->GetInputName(0) << "";
LOG(INFO) << i << ": " << interpreter->tensor(i)->name << ", " << interpreter->tensor(i)->bytes << ", " << interpreter->tensor(i)->type << ", " << interpreter->tensor(i)->params.scale << ", " << interpreter->tensor(i)->params.zero_point << "";
LOG(INFO) << "input: " << input << "";
LOG(INFO) << "number of inputs: " << inputs.size() << "";
LOG(INFO) << "number of outputs: " << outputs.size() << "";
LOG(FATAL) << "Failed to apply " << delegate.first << " delegate.";
LOG(INFO) << "Applied " << delegate.first << " delegate.";
LOG(FATAL) << "Failed to allocate tensors!";
LOG(FATAL) << "cannot handle input type " << interpreter->tensor(input)->type << " yet";
LOG(FATAL) << "Failed to invoke tflite!";
LOG(FATAL) << "Failed to invoke tflite!";
LOG(INFO) << "invoked ";
LOG(INFO) << "average time: " << (get_us(stop_time) - get_us(start_time)) / (s->loop_count * 1000) << " ms ";
LOG(FATAL) << "cannot handle output type " << interpreter->tensor(output)->type << " yet";
LOG(INFO) << confidence << ": " << index << " " << labels[index] << "";
LOG(INFO) << "label_image" << "--accelerated, -a: [0|1], use Android NNAPI or not" << "--old_accelerated, -d: [0|1], use old Android NNAPI delegate or not" << "--allow_fp16, -f: [0|1], allow running fp32 models with fp16 or not" << "--count, -c: loop interpreter->Invoke() for certain times" << "--gl_backend, -g: use GL GPU Delegate on Android" << "--hexagon_delegate: use Hexagon Delegate on Android" << "--input_mean, -b: input mean" << "--input_std, -s: input standard deviation" << "--image, -i: image_name.bmp" << "--labels, -l: labels for the model" << "--tflite_model, -m: model_name.tflite" << "--profiling, -p: [0|1], profiling or not" << "--num_results, -r: number of results to show" << "--threads, -t: number of threads" << "--verbose, -v: [0|1] print more information" << "--warmup_runs, -w: number of warmup runs" << "";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/delegates/gpu/gl_delegate.cc
TFLITE_LOG_PROD_ONCE(tflite::TFLITE_LOG_INFO, "Created TensorFlow Lite delegate for GPU.");

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/delegates/gpu/delegate.cc
TFLITE_LOG_PROD_ONCE(tflite::TFLITE_LOG_INFO, "Initialized OpenCL-based API.");
TFLITE_LOG_PROD_ONCE(tflite::TFLITE_LOG_INFO, "Initialized OpenGL-based API.");
TFLITE_LOG_PROD_ONCE(tflite::TFLITE_LOG_INFO, "Created TensorFlow Lite delegate for GPU.");

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/delegates/nnapi/nnapi_delegate.cc
TFLITE_LOG(TFLITE_LOG_INFO, "Changing Android NN SDK version %d to version " "supported by target devices: %d", nnapi->android_sdk_version, devices_sdk_version);
TFLITE_LOG_PROD_ONCE(tflite::TFLITE_LOG_INFO, "Created TensorFlow Lite delegate for NNAPI.");

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/delegates/flex/delegate.cc
TFLITE_LOG_PROD_ONCE(TFLITE_LOG_INFO, "Created TensorFlow Lite delegate for select TF ops.");

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/toco/tensorflow_util.cc
VLOG(log_level) << dump;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/toco/allocate_transient_arrays.cc
LOG(FATAL) << "A RNN state array, " << array_name << ", still does not " << "have a known data type after all graph transformations have " << "run.";
LOG(FATAL) << "An array, " << array_name << ", still does not " << "have a known data type after all graph transformations have " << "run.";
LOG(INFO) << "Total transient array allocated size: " << model->transient_data_size << " bytes, " << "theoretical optimal value: " << optimal_transient_alloc_size << " bytes.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/toco/toco_tooling.cc
LOG(FATAL) << "Unhandled input_format='" << FileFormat_Name(toco_flags.input_format()) << "'";
LOG(INFO) << "Estimated count of arithmetic ops: " << ops_count << " ops, equivalently " << ops_count / 2 << " MACs";
LOG(INFO) << "Number of parameters: " << params_count;
LOG(ERROR) << status.error_message();
LOG(FATAL) << "Unhandled output_format='" << FileFormat_Name(toco_flags.output_format()) << "'";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/toco/tooling_util.cc
LOG(FATAL) << "Unhandled array data type " << static_cast<int>(data_type);
LOG(FATAL) << "Unhandled op type";
VLOG(log_level) << "Operators summary (" << model.operators.size() << " operators):";
VLOG(log_level) << " " << OperatorTypeName(*it) << ": " << count;
VLOG(log_level) << "Array: " << name;
VLOG(log_level) << " DOES NOT EXIST";
VLOG(log_level) << " Data type: " << ArrayDataTypeName(array.data_type);
VLOG(log_level) << " Final type: " << ArrayDataTypeName(array.final_data_type);
VLOG(log_level) << " Constant Buffer";
VLOG(log_level) << " Transient Alloc";
VLOG(log_level) << " (Zero dimensions)";
VLOG(log_level) << message;
VLOG(log_level) << " MinMax: " << array.minmax->min << " .. " << array.minmax->max;
VLOG(log_level) << " QuantizationParams: zero_point=" << static_cast<int>(array.quantization_params->zero_point) << ", scale=" << array.quantization_params->scale;
LOG(INFO) << "DUMPING GRAPHVIZ VIDEO FRAME: " << dump_id;
VLOG(log_level) << "BEGIN DUMP OF TOCO MODEL (" << message << ")";
VLOG(log_level) << HelpfulOperatorTypeName(*op) << " :";
VLOG(log_level) << " " << FormatArraysList(model, op->inputs) << " -> " << FormatArraysList(model, op->outputs);
VLOG(log_level) << " (with fused activation function)";
VLOG(log_level) << "END DUMP OF TOCO MODEL (" << message << ")";
LOG(FATAL) << "Unsupported data type: " << ArrayDataTypeName(lhs_array.data_type);
LOG(INFO) << "Error: Orphaned array: " << array;
LOG(ERROR) << "No viable ordering of operators was found. " << "Here is a 'backtrace' of at least one part of the graph that is " << "problematic. It starts with the first operator that has as " << "problematic input array, and then walks back the graph to " << "the operator that produced that input array, etc., until we find " << "the root cause:";
LOG(ERROR) << "BEGIN TRACE OF OPERATOR WITH BAD INPUT";
LOG(ERROR) << "Here is the first-encountered operator with a bad input: ";
LOG(ERROR) << HelpfulOperatorTypeName(*bad_op) << " : " << FormatArraysList(*model, bad_op->inputs) << " -> " << FormatArraysList(*model, bad_op->outputs);
LOG(ERROR) << "The bad input here is: " << bad_input;
LOG(FATAL) << "Cycle found! We already encountered that " << "input array, " << bad_input << ", earlier in the " << "above trace! We expect graphs to be acyclic, even " << "RNNs. Let us know if some graph actually needs to have " << "cycles, but first, please check if it really is " << "an *inference* graph. *Training* graphs are out-of-scope " << "for toco.";
LOG(ERROR) << "And that's the root cause: " << "that array, " << bad_input << ", isn't produced by any " << "operator, or provided in any other way.";
LOG(ERROR) << "END TRACE OF OPERATOR WITH BAD INPUT";
LOG(FATAL) << "(The above was a multi-line fatal error)";
LOG(ERROR) << "And that array is the output of the following operator:";
LOG(WARNING) << "Fixing constant output array " << output_array_name << " by inserting a copy. This is not optimal.";
VLOG(1) << "Deduplicating arrays; using " << lhs_array_name << " in place of " << rhs_array_name;
LOG(FATAL) << "Unsupported data type: " << ArrayDataTypeName(source_array.data_type);
LOG(FATAL) << "Should not get here: " << num_dims;
LOG(FATAL) << "Array " << input << ", which is an input to the " << HelpfulOperatorTypeName(*op) << " operator producing the output " << "array " << op->outputs[0] << ", is lacking min/max data, " << "which is necessary for quantization. If accuracy matters, either " << "target a non-quantized output format, or run quantized training " << "with your model from a floating point checkpoint to change the " << "input graph to contain min/max information. If you don't care " << "about accuracy, you can pass --default_ranges_min= and " << "--default_ranges_max= for easy experimentation.";
LOG(FATAL) << "Transient arrays with strings are not supported yet";
LOG(FATAL) << "Unknown data_type = " << static_cast<int>(data_type);
LOG(WARNING) << "Dropping MinMax information in array " << array_name << ". Expect inaccuracy in quantized inference.";
LOG(FATAL) << "Could not find an available array name starting with " << sanitized_name << ". Tried " << kNumSuffixesToTry << " suffixes, all were taken!";
LOG(INFO) << name << " has no shape";
LOG(INFO) << name << " has shape: " << ShapeToString(model->GetArray(name).shape());
LOG(FATAL) << "Bad shuffle";
LOG(FATAL) << "Bad AxesOrder";
LOG(ERROR) << "arrays_extra_info_file: No matching arrays found for " << (entry.has_name() ? entry.name() : "") << (entry.has_name_regexp() ? entry.name_regexp() : "");

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/toco/toco_cmdline_flags.cc
LOG(WARNING) << "--force_select_tf_ops should always be used with " "--enable_select_tf_ops.";
LOG(WARNING) << "--input_type is deprecated. It was an ambiguous flag that set both " "--input_data_types and --inference_input_type. If you are trying " "to complement the input file with information about the type of " "input arrays, use --input_data_type. If you are trying to control " "the quantization/dequantization of real-numbers input arrays in " "the output file, use --inference_input_type.";
LOG(WARNING) << "--input_types is deprecated. It was an ambiguous flag that set " "both --input_data_types and --inference_input_type. If you are " "trying to complement the input file with information about the " "type of input arrays, use --input_data_type. If you are trying to " "control the quantization/dequantization of real-numbers input " "arrays in the output file, use --inference_input_type.";
LOG(WARNING) << "--quantize_weights is deprecated. Falling back to " "--post_training_quantize. Please switch --post_training_quantize.";
LOG(WARNING) << "--post_training_quantize quantizes a graph of inference_type " "FLOAT. Overriding inference type QUANTIZED_UINT8 to FLOAT.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/toco/import_tensorflow.cc
LOG(INFO) << "Unsupported data type in placeholder op: " << dtype;
LOG(WARNING) << "Op node missing output type attribute: " << node.name();
LOG(INFO) << "Converting unsupported operation: " << node.op();
LOG(INFO) << "Unable to determine output type for op: " << node.op();
LOG(INFO) << "Skipping wildcard output shape(s) for node: " << node.name();
LOG(FATAL) << "TFLite does not support DepthToSpace with type T:" << enum_descriptor->FindValueByNumber(dtype)->name() << ". " << "T must be one of {DT_FLOAT, DT_UINT8, DT_INT32, DT_INT64}.";
LOG(FATAL) << "TFLite does not support SpaceToDepth with type T:" << enum_descriptor->FindValueByNumber(dtype)->name() << ". " << "T must be one of {DT_FLOAT, DT_UINT8, DT_INT32, DT_INT64}.";
LOG(WARNING) << "Found MaxPool operator missing 'T' attribute";
LOG(FATAL) << "Bad padding (only SAME and VALID are supported)";
LOG(FATAL) << "Bad padding (only SAME and VALID are supported)";
LOG(FATAL) << "Expected Concat or ConcatV2";
LOG(FATAL) << "Expected MirrorPad.";
LOG(FATAL) << "Only SAME and VALID padding supported on " "Conv2DBackpropInput nodes.";
LOG(FATAL) << "What?";
VLOG(kLogLevelModelUnchanged) << "No functions to inline.";
LOG(ERROR) << "tensorflow::ImportGraphDef failed with status: " << tf_convert_status.ToString();
LOG(INFO) << "Found and inlined TensorFlow functions.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/toco/export_tensorflow.cc
LOG(FATAL) << "Unsupported data type '" << ArrayDataTypeName(data_type) << "' in " << error_location;
LOG(FATAL) << "Bad padding (only SAME and VALID are supported)";
LOG(FATAL) << "Bad padding (only SAME and VALID are supported)";
LOG(FATAL) << "Bad padding (only SAME and VALID are supported)";
LOG(FATAL) << "Bad padding (only SAME and VALID are supported)";
LOG(FATAL) << "Bad padding (only SAME and VALID are supported)";
LOG(FATAL) << "Bad padding (only SAME and VALID are supported)";
LOG(FATAL) << "Unsupported: the input model has a fused activation function";
LOG(FATAL) << "Unhandled operator type " << OperatorTypeName(src_op.type);
LOG(FATAL) << "Unexpected data type in array "" << name << """;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/toco/model_cmdline_flags.cc
LOG(FATAL) << "Unknown key '" << key << "' in --rnn_states";
LOG(FATAL) << "Unknown key '" << key << "' in --model_checks";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/toco/tflite/export.cc
LOG(WARNING) << "FAKE_QUANT operation " << LogName(*op) << " was not converted. If running quantized make sure you " "are passing --inference_type=QUANTIZED_UINT8 and values " "for --std_values and --mean_values.";
LOG(INFO) << "Quantizing TFLite model after conversion to flatbuffer. " "dump_graphviz will only output the model before this " "transformation. To visualize the output graph use " "lite/tools/optimize.py.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/toco/tflite/types.cc
LOG(FATAL) << "Unhandled tensor type '" << tensor_type << "'.";
LOG(FATAL) << "Unhandled array data type.";
LOG(FATAL) << "Unhandled tensor type.";
LOG(FATAL) << "Unhandled padding type.";
LOG(FATAL) << "Unhandled padding.";
LOG(FATAL) << "Unhandled fused activation function type.";
LOG(FATAL) << "Unhandled fused activation function type.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/toco/tflite/operator.cc
LOG(ERROR) << "Unhandled FC weights format";
LOG(ERROR) << "Unhandled FC weights format";
LOG(ERROR) << "Unhandled Kernel Type";
LOG(ERROR) << "Failed to parse TensorFlow NodeDef";
LOG(INFO) << "Writing flex op: " << node_def.op();
LOG(ERROR) << "Failed to parse TensorFlow NodeDef";
LOG(INFO) << "Writing flex op: " << node_def.op();
LOG(WARNING) << "Ignoring unsupported type in list attribute with key '" << key << "'";
LOG(WARNING) << "Ignoring unsupported attribute type with key '" << key << "'";
LOG(WARNING) << "Ignoring unsupported attribute type with key '" << key << "'";
LOG(WARNING) << "Op " << tensorflow_op_name << " is a valid TensorFlow op but has not been whitelisted for" " the TensorFlow Lite flex op set.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/toco/tflite/import.cc
LOG(FATAL) << "Index " << index << " must be between zero and " << operators_table.size();
LOG(FATAL) << "Internal logic error: TENSORFLOW_UNSUPPORTED not found.";
LOG(FATAL) << "Expected a TensorFlowUnsupportedOperator";
LOG(FATAL) << "Invalid flatbuffer.";
LOG(FATAL) << "Number of subgraphs in tflite should be exactly 1.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/toco/graph_transformations/resolve_constant_concatenation.cc
LOG(FATAL) << "ArrayDataType not supported";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/toco/graph_transformations/propagate_activation_function_into_constants.cc
LOG(FATAL) << "Unsupported activation function " << LogName(*ac_op);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/toco/graph_transformations/resolve_constant_gather.cc
LOG(FATAL) << "Unsupported data type given to Gather op with output "" << op->outputs[0] << """;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/toco/graph_transformations/resolve_constant_binary.cc
LOG(FATAL) << "should not get here";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/toco/graph_transformations/resolve_constant_fake_quant.cc
LOG(FATAL) << "unhandled quantized data type";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/toco/graph_transformations/remove_trivial_quantized_activation_func.cc
LOG(FATAL) << "Unsupported fused activation type: " << static_cast<int>(activation_function);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/toco/graph_transformations/resolve_constant_strided_slice.cc
LOG(FATAL) << "Unsupported data type input to StridedSlice op with output "" << op->outputs[0] << """;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/toco/graph_transformations/resolve_constant_transpose.cc
LOG(FATAL) << "Unsupported data type given to Transpose op with output "" << op->outputs[0] << """;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/toco/graph_transformations/dequantize.cc
LOG(FATAL) << "Unhandled data type";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/toco/graph_transformations/resolve_fake_quant_args_from_vars.cc
LOG(WARNING) << "For " << LogName(*fakequant_op) << " the MinMax range " << "[" << minmax.min << ", " << minmax.max << "] does not contain 0. " << "Proceeding by tweaking it to contain 0, which will result " "in poor accuracy.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/toco/graph_transformations/resolve_constant_unary.cc
LOG(FATAL) << "Unsupported cast op input type";
LOG(FATAL) << "should not get here.";
LOG(FATAL) << "Unsupported activation function " << LogName(*unary_op);
LOG(FATAL) << "should not get here.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/toco/graph_transformations/fuse_activation_functions.cc
LOG(FATAL) << "Unhandled activation function type";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/toco/graph_transformations/resolve_constant_reshape.cc
LOG(FATAL) << "Unsupported data type: " << ArrayDataTypeName(input_array.data_type);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/toco/graph_transformations/fuse_binary_into_following_affine.cc
LOG(FATAL) << "Missing bias parameter";
LOG(FATAL) << "Should not get here";
LOG(FATAL) << "Should not get here.";
LOG(FATAL) << "Should not get here";
LOG(FATAL) << "should not get here";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/toco/graph_transformations/graph_transformations.cc
LOG(INFO) << label << ": " << model.operators.size() << " operators, " << model.GetArrayMap().size() << " arrays (" << quantized_arrays << " quantized)";
LOG(INFO) << "Model is empty!!!";
VLOG(log_level) << transformation->Name() << " " << made_a_change_msg << " at op_index=" << op_index << "/" << model->operators.size() - 1;
VLOG(log_level) << transformation->Name() << " " << made_a_change_msg << " at op_index=" << op_index << "/" << model->operators.size() - 1 << ": " << message;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/toco/graph_transformations/resolve_constant_slice.cc
LOG(FATAL) << "Unsupported data type input to Slice op with output "" << op->outputs[0] << """;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/toco/graph_transformations/ensure_bias_vectors.cc
LOG(FATAL) << "Unhandled operator type";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/toco/graph_transformations/resolve_constant_random_uniform.cc
LOG(WARNING) << "RandomUniform op outputting "" << op->outputs[0] << "" is truly random (using /dev/random system entropy). " "Therefore, cannot resolve as constant. Set "seed" or " ""seed2" attr non-zero to fix this";
LOG(FATAL) << "Unsupported data type given to RandomUniform op with output "" << op->outputs[0] << """;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/toco/graph_transformations/fuse_binary_into_preceding_affine.cc
LOG(FATAL) << "Missing bias parameter";
LOG(FATAL) << "Operand shape mismatch.";
LOG(FATAL) << "Should not get here.";
LOG(FATAL) << "Missing bias parameter";
LOG(FATAL) << "Operand shape mismatch.";
LOG(FATAL) << "Should not get here";
LOG(FATAL) << "Should not get here";
LOG(FATAL) << "Should not get here";
LOG(FATAL) << "Should not get here";
LOG(FATAL) << "Should not get here";
LOG(FATAL) << "should not get here";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/toco/graph_transformations/resolve_constant_fill.cc
LOG(FATAL) << "Unsupported data type given to Fill op with output "" << op->outputs[0] << """;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/toco/graph_transformations/hardcode_min_max.cc
LOG(WARNING) << "Tweaking the MinMax of array " << input << ", which is " << "an input to " << LogName(*op) << ", because we want all inputs " << "and outputs of a Concatenation operator to have the same " << "MinMax so that it can be implemented as a pure byte-copy, no " "arithmetic.";
LOG(WARNING) << "Tweaking the MinMax of the output array of " << LogName(*op) << ", because we want all inputs " << "and outputs of a Concatenation operator to have the same MinMax " << "so that it can be implemented as a pure byte-copy, no " << "arithmetic.";
LOG(WARNING) << "Using the input range for output in reduce_sum op." << "This could have an impact on your model accuracy.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/toco/graph_transformations/propagate_fixed_sizes.cc
LOG(FATAL) << "Only supporting SAME or VALID padding";
LOG(FATAL) << "TransposeConv only supports SAME or VALID padding";
LOG(FATAL) << "Not a reduction operator!";
LOG(FATAL) << "missing dimensions for " << input_name;
LOG(WARNING) << "Skipping StridedSlice op with output "" << op->outputs[0] << "". ellipsis_mask is not supported (mask=" << op->ellipsis_mask << ")";
LOG(WARNING) << "Skipping StridedSlice op with output "" << op->outputs[0] << "". new_axis_mask is not supported (mask=" << op->new_axis_mask << ")";
LOG(FATAL) << "Unhandled operator type " << OperatorTypeName(op->type);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/toco/graph_transformations/resolve_reorder_axes.cc
LOG(FATAL) << "Cannot ReorderAxes unless input buffer is float or uint8.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/toco/graph_transformations/ensure_uint8_weights_safe_for_fast_int8_kernels.cc
LOG(FATAL) << "Bad value for " << name << " at index " << i << ", previous bad value at index " << index_of_previous_bad_value << ", distance=" << distance << ", kMinDistanceBetweenBadValues=" << kMinDistanceBetweenBadValues << ". Consider passing " << "--allow_nudging_weights_to_use_fast_gemm_kernel " << "if you don't care about accuracy.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/toco/graph_transformations/quantize.cc
LOG(WARNING) << "Constant array " << array_name << " lacks MinMax information. To make up for that, we will now " "compute" << " the MinMax from actual array elements. That will result in" << " quantization parameters that probably do not match whichever " "arithmetic" << " was used during training, and thus will probably be a cause of " "poor" << " inference accuracy.";
LOG(FATAL) << "Array " << array_name << " does not have MinMax information, " "and is not a constant array. Cannot " "proceed with quantization.";
LOG(FATAL) << "Unhandled case.";
LOG(WARNING) << "Can't quantize input array " << input << " because it lacks min/max info";
LOG(WARNING) << HelpfulOperatorTypeName(op) << " is a quantized op" << " but it has a model flag that sets the output arrays to float.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/toco/graph_transformations/identify_dilated_conv.cc
LOG(INFO) << "Replaced sub-network with Dilated Conv2D op outputting "" << conv_base_op->outputs[0] << "".";
LOG(INFO) << "Replaced sub-netork with Dilated DepthwiseConv2D op outputting "" << conv_base_op->outputs[0] << "".";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/toco/graph_transformations/resolve_constant_tile.cc
LOG(FATAL) << "Unsupported data type given to Tile op with output "" << op->outputs[0] << """;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/toco/graph_transformations/quantization_util.cc
LOG(FATAL) << "Unhandled final quantization type " << static_cast<int>(array.final_data_type);
LOG(FATAL) << "Unhandled final quantization type " << static_cast<int>(quantized_data_type);
LOG(FATAL) << "Unhandled case.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/toco/graph_transformations/resolve_constant_pack.cc
LOG(FATAL) << "Unsupported data type given to Pack op with output "" << op->outputs[0] << """;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/toco/logging/conversion_log_util.cc
LOG(ERROR) << "Failed to parse Tensorflow NodeDef";
LOG(ERROR) << "Cannot get OS info.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/toco/tensorflow_graph_matching/resolve_svdf.cc
LOG(FATAL) << "Unexpected input node for SVDF op! Accepted inputs are: " "weights_feature, weights_time and bias.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/java/src/gen/cc/op_specs.cc
LOG(FATAL) << "Cannot resolve data type of argument "" << arg_def.name() << "" in operation "" << op_def_.name() << """;
LOG(FATAL) << "Cannot resolve data type for attribute "" << attr_type << "" in operation "" << op_def_.name() << """;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/examples/wav_to_spectrogram/main.cc
LOG(ERROR) << usage;
LOG(ERROR) << "Unknown argument " << argv[1] << "" << usage;
LOG(ERROR) << "WavToSpectrogram failed with " << wav_status;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/examples/label_image/main.cc
LOG(ERROR) << read_labels_status;
LOG(INFO) << labels[label_index] << " (" << label_index << "): " << score;
LOG(ERROR) << "Expected label #" << expected << " but got #" << indices_flat(0);
LOG(ERROR) << usage;
LOG(ERROR) << "Unknown argument " << argv[1] << "" << usage;
LOG(ERROR) << load_graph_status;
LOG(ERROR) << read_tensor_status;
LOG(ERROR) << "Running model failed: " << run_status;
LOG(ERROR) << "Running check failed: " << check_status;
LOG(ERROR) << "Self-test failed!";
LOG(ERROR) << "Running print failed: " << print_status;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/examples/android/jni/object_tracking/object_tracker_jni.cc
LOGI("Initializing object tracker. %dx%d @%p", width, height, thiz);
LOGI("Initialized!");
LOGI("Registering the position of %s at %.2f,%.2f,%.2f,%.2f", id_str, x1, y1, x2, y2);
LOGI( "Registering the position of %s at %.2f,%.2f,%.2f,%.2f" " at time %lld", id_str, x1, y1, x2, y2, static_cast<long long>(timestamp));
LOGI("Registering the position of %s at %.2f,%.2f,%.2f,%.2f", id_str, x1, y1, x2, y2);
LOGE("null array!");
LOGE("null array!");
LOGW("Received null arrays, hopefully this is a test!");

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/examples/android/jni/object_tracking/tracked_object.cc
LOGV("Tracked correlation to last localization: %.6f", last_localization_correlation);
LOGV("Tracked correlation to model: %.6f", tracked_correlation_);
LOGV("Tracked match score with model: %.6f", tracked_match_score_.value);
LOGD("Tracked match score is way too low (%.6f), aborting track.", tracked_match_score_.value);
LOGI("Not relocalizing since new match is worse: %.6f < %.6f + %.6f", match_score.value, tracked_match_score_.value, kMatchScoreBuffer.value);
LOGI("Relocalizing! From (%.1f, %.1f)[%.1fx%.1f] to " "(%.1f, %.1f)[%.1fx%.1f]: %.6f > %.6f", last_known_position_.left_, last_known_position_.top_, last_known_position_.GetWidth(), last_known_position_.GetHeight(), detection_position.left_, detection_position.top_, detection_position.GetWidth(), detection_position.GetHeight(), match_score.value, tracked_match_score_.value);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/examples/android/jni/object_tracking/frame_pair.cc
LOGW("Median not found! %d points, sum of %.2f", num_items, sum);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/examples/android/jni/object_tracking/optical_flow.cc
LOGE("Backward error!");

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/examples/android/jni/object_tracking/object_tracker.cc
LOGV("Found %d of %d keypoint correspondences", num_keypoints_found, frame_pair->number_of_keypoints_);
LOGV("Received frame %d", num_frames_);
LOGV("Tracking %zu targets", objects_.size());
VLOG(2) << "Set tracked position for " << id << " to " << bounding_box << std::endl;
LOGV("Forgetting object %s", id.c_str());
LOGV("Got %d keypoints.", curr_keypoint);
LOGV("Went %d out of %d frames before finding frame. (index: %d)", num_frames_back, curr_num_frame_pairs_, GetNthIndexFromEnd(i));
LOGW("History did not go back far enough! %" PRId64 " vs %" PRId64, frame_pairs_[GetNthIndexFromEnd(0)].end_time_ - frame_pairs_[GetNthIndexFromStart(0)].end_time_, frame_pairs_[GetNthIndexFromEnd(0)].end_time_ - timestamp);
LOGV("Looking for matches in %zu objects!", objects_.size());
LOGV("Distance: %.2f, Allowed distance %.2f, Overlap: %.2f", jump_distance, allowed_distance, overlap);
LOGV("Initial detection done, iterating over %zu detections now.", detections->size());
LOGV("No match, adding it!");
LOGV("No objects to search for, aborting.");
LOGV("Trying to detect %zu models", object_models.size());
LOGV("Creating test vector!");
LOGV("Created test vector!");
LOGV("Detecting!");
LOGV("Found %zu detections", detections.size());
LOGV("Done detecting!");
LOGV("Tracking %zu objects!", objects_.size());
LOGE("Removing object! %s", iter->c_str());
LOGV("%zu objects tracked!", objects_.size());

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/examples/android/jni/object_tracking/keypoint_detector.cc
LOGV("Selected %d keypoints!", curr_change->number_of_keypoints_);
LOGW("Hit cap of %d for temporary keypoints!", max_num_keypoints);
LOGW("Hit cap of %d for temporary keypoints (FAST)! %d keypoints", kMaxTempKeypoints, number_of_tmp_keypoints);
LOGW("Hit cap of %d for temporary keypoints (boxes)! %d keypoints", kMaxTempKeypoints, number_of_tmp_keypoints);
LOGV("Scoring %d keypoints!", number_of_tmp_keypoints);
LOGV("%d keypoints to select from!", number_of_tmp_keypoints);
LOGV("Picked %d (%d max) final keypoints out of %d potential.", curr_change->number_of_keypoints_, kMaxKeypoints, number_of_tmp_keypoints);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/examples/speech_commands/label_wav.cc
LOG(ERROR) << usage;
LOG(ERROR) << "Unknown argument " << argv[1] << "" << usage;
LOG(ERROR) << load_graph_status;
LOG(ERROR) << read_labels_status;
LOG(ERROR) << read_wav_status;
LOG(ERROR) << "Running model failed: " << run_status;
LOG(INFO) << labels_list[label_index] << " (" << label_index << "): " << score;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/examples/speech_commands/accuracy_utils.cc
LOG(INFO) << "No ground truth yet, " << stats.how_many_false_positives << " false positives";
LOG(INFO) << std::setprecision(1) << std::fixed << any_match_percentage << "% matched, " << correct_match_percentage << "% correctly, " << wrong_match_percentage << "% wrongly, " << false_positive_percentage << "% false positives ";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/examples/multibox_detector/main.cc
LOG(INFO) << "Saving image to " << file_path;
LOG(WARNING) << "No non-zero encodings; check log for inference errors.";
LOG(ERROR) << read_labels_status;
LOG(INFO) << original_tensor->DebugString();
LOG(INFO) << "===== Top " << how_many_labels << " Detections ======";
LOG(INFO) << "Detection " << pos << ": " << "L:" << left << " " << "T:" << top << " " << "R:" << right << " " << "B:" << bottom << " " << "(" << label_index << ") score: " << DecodeScore(score);
LOG(ERROR) << usage;
LOG(ERROR) << "Unknown argument " << argv[1] << "" << usage;
LOG(ERROR) << load_graph_status;
LOG(ERROR) << read_tensor_status;
LOG(ERROR) << "Running model failed: " << run_status;
LOG(ERROR) << "Running print failed: " << print_status;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/cc/saved_model/reader.cc
LOG(INFO) << "Reading SavedModel from: " << export_dir;
LOG(INFO) << "Reading meta graph with tags { " << absl::StrJoin(tags, " ") << " }";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/cc/saved_model/loader.cc
LOG(INFO) << "Running initialization op on SavedModel bundle at path: " << export_dir;
LOG(INFO) << "Restoring SavedModel bundle.";
LOG(INFO) << "The specified SavedModel has no variables; no checkpoints " "were restored. File does not exist: " << variables_index_path;
LOG(INFO) << "Reading SavedModel debug info (if present) from: " << export_dir;
LOG(INFO) << "SavedModel load for tags { " << absl::StrJoin(tags, " ") << " }; Status: " << status_str << ": " << status << ". Took " << GetLatencyMicroseconds(start_microseconds) << " microseconds.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/cc/saved_model/bundle_v2.cc
LOG(INFO) << "Reading SavedModel from: " << export_dir;
LOG(INFO) << "Reading SavedModel debug info (if present) from: " << export_dir;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/cc/framework/scope.cc
LOG(FATAL) << *impl()->status_;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/cc/framework/cc_op_gen.cc
LOG(FATAL) << "Not handling type " << DataType_Name(t.dtype());
LOG(FATAL) << "Unsupported Attr type: " << op << " " << attr_value.value_case();
LOG(FATAL) << "Unsupported Attr type: " << attr_type;
LOG(FATAL) << "Unsupported or non-list Attr type: " << attr_type;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/cc/training/queue_runner.cc
LOG(ERROR) << "Queue runner thread got a failure status: " << status.ToString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/c/c_api.cc
LOG(WARNING) << mutation_warning;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/c/c_api_experimental.cc
VLOG(1) << "Adding func to graph: " << fdef_to_load.DebugString();
VLOG(1) << "Dequeuing named tensor with id " << tensor_id << ", with input graph: " << session->graph->graph.ToGraphDefDebug().DebugString();
VLOG(1) << "Running the dequeue op";
VLOG(1) << "Dequeued tensor content: " << tensor.DebugString();
VLOG(1) << "Enqueuing named tensor with id " << tensor_id << ", with input graph: " << session->graph->graph.ToGraphDefDebug().DebugString();
VLOG(1) << "Enqueu'ing tensor content: " << internal_tensor.DebugString();
VLOG(1) << "Running the enqueue op";
VLOG(1) << "Enqueuing is done.";
VLOG(1) << "Dequeuing data tensor with id " << tensor_id;
VLOG(1) << "Enqueuing data tensor with id " << tensor_id;
VLOG(1) << "Enqueuing data tensor with id " << tensor_id;
VLOG(1) << "Enqueuing variant tensor with id " << tensor_id;
VLOG(1) << "Dequeuing variant tensor with id " << tensor_id;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/c/experimental/rendezvous.cc
VLOG(1) << "WARNING: CRemoteRendezvous does not support cancellation.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/c/experimental/filesystem/modular_filesystem_registration.cc
VLOG(0) << "Plugin API (" << plugin_API << ") for " << where << " operations doesn't match expected core API (" << core_API << "). Plugin will be loaded but functionality might be missing.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/c/eager/c_api_debug.cc
VLOG(3) << "Fully padded shape of [" << absl::StrJoin(shape_to_log, ", ") << "] is " << padded_shape.DebugString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/c/eager/c_api.cc
VLOG(1) << "Updating cluster with following changes";
VLOG(1) << " Added worker " << w;
VLOG(1) << " Removed worker " << w;
VLOG(1) << " Replaced worker " << w;
VLOG(1) << "Updating cluster with existing worker " << w;
LOG(WARNING) << "Device filters can only be specified when initializing " "the cluster. Any changes in device filters are ignored " "when updating the server def.";
LOG(ERROR) << "Failed to get client cache for remote workers.";
LOG(INFO) << "Remote worker " << worker_name << " is not alive: " << keep_alive_status.error_message();
VLOG(1) << "Deleting tensor handle " << this << " with internal handle " << handle_;
LOG(ERROR) << "Could not enable XLA compilation for op: " << s;
LOG(WARNING) << "This call is a no-op, as the TensorFlow library is not " "built with XLA support.";
LOG(WARNING) << "Unable to set attribute: " << attr_name;
LOG(WARNING) << "Unable to set attribute: " << attr_name;
LOG(WARNING) << "Unable to set attribute: " << attr_name;
LOG(WARNING) << "Unable to set attribute: " << attr_name;
LOG(WARNING) << "Unable to set attribute: " << attr_name;
LOG(WARNING) << "Unable to set attribute: " << attr_name;
LOG(WARNING) << "Unable to set attribute: " << attr_name;
LOG(WARNING) << "Unable to set attribute: " << attr_name;
LOG(WARNING) << "Unable to set attribute: " << attr_name;
LOG(WARNING) << "Unable to set attribute: " << attr_name;
LOG(WARNING) << "Unable to set attribute: " << attr_name;
LOG(WARNING) << "Unable to set attribute: " << attr_name;
LOG(WARNING) << "Unable to set attribute: " << attr_name;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/stream_executor/temporary_memory_manager.cc
VLOG(1) << "force-deallocating " << records_.size() << " remaining records";
LOG(FATAL) << "attempted to mark finalization for temporary " "memory that does not exist";
VLOG(1) << "deallocated " << deallocated_count << " finalized temporaries";
VLOG(1) << absl::StreamFormat( "stream %p allocated temporary device memory at %p (size %u) in " "generation %u", stream_, device_memory.opaque(), byte_size, generation);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/stream_executor/executor_cache.cc
VLOG(2) << "hit in cache";
VLOG(2) << "building executor";
VLOG(2) << "failed to get build executor: " << result.status();
VLOG(2) << "hit in cache for device ordinal " << config.ordinal;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/stream_executor/stream_executor_pimpl.cc
LOG(WARNING) << "Not all streams were deallocated at executor destruction " << "time. This may lead to unexpected/bad behavior - " << "especially if any stream is still active!";
LOG(INFO) << "Memory alloced at executor exit: addr: " << absl::StrFormat("%p", it.first) << ", bytes: " << it.second.bytes << ", trace: " << it.second.stack_trace;
VLOG(1) << "Called StreamExecutor::Deallocate(mem=" << mem->opaque() << ") mem->size()=" << mem->size() << StackTraceIfVLOG10();
LOG(ERROR) << error_msg;
LOG(WARNING) << "Not enough memory to allocate " << size << " on device " << device_ordinal_ << " within provided limit. [used=" << mem_alloc_bytes_ << ", limit=" << memory_limit_bytes_ << "]";
VLOG(1) << "Called StreamExecutor::Allocate(size=" << size << ", memory_space=" << memory_space << ") returns " << buf.opaque() << StackTraceIfVLOG10();
VLOG(1) << "Called StreamExecutor::UnifiedMemoryAllocate(size=" << bytes << ") returns " << buffer << StackTraceIfVLOG10();
VLOG(1) << "Called StreamExecutor::UnifiedMemoryDeallocate(location=" << location << ")" << StackTraceIfVLOG10();
VLOG(1) << "Called StreamExecutor::HostMemoryAllocate(size=" << size << ") returns " << buffer << StackTraceIfVLOG10();
VLOG(1) << "Called StreamExecutor::HostMemoryDeallocate(location=" << location << ")" << StackTraceIfVLOG10();
VLOG(1) << "Called StreamExecutor::HostMemoryRegister(location=" << location << ", size=" << size << ")" << StackTraceIfVLOG10();
LOG(WARNING) << "attempting to register null or zero-sized memory: " << location << "; size " << size;
VLOG(1) << "Called StreamExecutor::HostMemoryUnregister(location=" << location << ")" << StackTraceIfVLOG10();
VLOG(1) << "Called StreamExecutor::SynchronizeAllActivity()" << StackTraceIfVLOG10();
VLOG(1) << "Called StreamExecutor::SynchronousMemZero(location=" << location << ", size=" << size << ")" << StackTraceIfVLOG10();
VLOG(1) << "Called StreamExecutor::SynchronousMemSet(location=" << location << ", value=" << value << ", size=" << size << ")" << StackTraceIfVLOG10();
VLOG(1) << "Called StreamExecutor::SynchronousMemcpy(device_dst=" << device_dst->opaque() << ", host_src=" << host_src << ", size=" << size << ") H2D" << StackTraceIfVLOG10();
LOG(ERROR) << "synchronous memcpy: " << status;
VLOG(1) << "Called StreamExecutor::SynchronousMemcpy(host_dst=" << host_dst << ", device_src=" << device_src.opaque() << ", size=" << size << ") D2H" << StackTraceIfVLOG10();
LOG(ERROR) << "synchronous memcpy: " << status;
VLOG(1) << "Called StreamExecutor::SynchronousMemcpy(device_dst=" << device_dst->opaque() << ", device_src=" << device_src.opaque() << ", size=" << size << ") D2D" << StackTraceIfVLOG10();
LOG(ERROR) << "synchronous memcpy: " << status;
VLOG(1) << "Called StreamExecutor::SynchronousMemcpyD2H(device_src=" << device_src.opaque() << ", size=" << size << ", host_dst=" << host_dst << ")" << StackTraceIfVLOG10();
VLOG(1) << "Called StreamExecutor::SynchronousMemcpyH2D(host_src=" << host_src << ", size=" << size << ", device_dst=" << device_dst->opaque() << ")" << StackTraceIfVLOG10();
LOG(INFO) << "failed to allocate stream; live stream count: " << count;
LOG(ERROR) << "Deallocating unknown pointer: " << opaque;
LOG(INFO) << "Attempt to register already-registered listener, " << listener;
LOG(INFO) << "Attempt to unregister unknown listener, " << listener;
VLOG(3) << absl::StreamFormat( "Allocated %s (%uB) on device ordinal %d: %p", tensorflow::strings::HumanReadableNumBytes(size), size, device_ordinal, result.opaque());
VLOG(3) << absl::StreamFormat("Freeing %p on device ordinal %d", mem.opaque(), device_ordinal);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/stream_executor/platform.cc
LOG(INFO) << "cannot enable peer access from device ordinal " << devices.first << " to device ordinal " << devices.second;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/stream_executor/kernel_spec.cc
LOG(FATAL) << "bzip2 decompression is not supported yet.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/stream_executor/device_description.cc
VLOG(2) << "exceeded total-thread-per-block limit: " << total_threads << " vs limit " << threads_per_block_limit;
VLOG(2) << "thread dim " << thread_dim.ToString() << " exceeds limit constraints of " << limit.ToString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/stream_executor/blas.cc
LOG(FATAL) << "Unknown transpose " << static_cast<int32>(t);
LOG(FATAL) << "Unknown upperlower " << static_cast<int32>(ul);
LOG(FATAL) << "Unknown diagonal " << static_cast<int32>(d);
LOG(FATAL) << "Unknown side " << static_cast<int32>(s);
LOG(FATAL) << "Unknown ComputationType " << static_cast<int32>(ty);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/stream_executor/rng.cc
LOG(INFO) << "Insufficient RNG seed data specified: " << seed_bytes << ". At least " << RngSupport::kMinSeedBytes << " bytes are required.";
LOG(INFO) << "Too much RNG seed data specified: " << seed_bytes << ". At most " << RngSupport::kMaxSeedBytes << " bytes may be provided.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/stream_executor/stream.cc
LOG(WARNING) << "Error blocking host until done in stream destructor: " << status;
LOG(ERROR) << "failed to allocate stream during initialization";
LOG(INFO) << "did not allocate timer: " << timer;
LOG(ERROR) << "Error recording event in stream: " << status.error_message() << "; not marking stream as bad, as the Event object may be " << "at fault. Monitor for further errors.";
LOG(WARNING) << "attempting to perform DNN operation using StreamExecutor " "without DNN support";
LOG(WARNING) << "attempting to perform DNN operation using StreamExecutor " "without DNN support";
LOG(WARNING) << "attempting to perform DNN operation using StreamExecutor " "without DNN support";
LOG(WARNING) << "attempting to perform DNN operation using StreamExecutor " "without DNN support";
LOG(ERROR) << "Incompatible dimensions for depth concatenation." << "input_dimensions[0]: " << input_dimensions[0].ToString() << "input_dimensions[" << i << "]: " << input_dimensions[i].ToString();
LOG(ERROR) << "Incompatible dimensions for X concatenation." << "input_dimensions[0]: " << input_dimensions[0].ToString() << "input_dimensions[" << i << "]: " << input_dimensions[i].ToString();
LOG(ERROR) << "Incompatible dimensions for Y concatenation." << "input_dimensions[0]: " << input_dimensions[0].ToString() << "input_dimensions[" << i << "]: " << input_dimensions[i].ToString();
VLOG(1) << DebugStreamPointers() << " reusing sub_stream " << sub_stream->DebugStreamPointers();
VLOG(1) << DebugStreamPointers() << " dropped !ok sub_stream " << sub_stream->DebugStreamPointers();
LOG(ERROR) << "sub-stream failed to be initialized";
VLOG(1) << DebugStreamPointers() << " created new sub_stream " << sub_stream->DebugStreamPointers();
VLOG(1) << DebugStreamPointers() << " returned ok sub_stream " << sub_stream->DebugStreamPointers();
VLOG(1) << DebugStreamPointers() << " returned !ok sub_stream " << sub_stream->DebugStreamPointers();
LOG(FATAL) << DebugStreamPointers() << " did not create the returned sub-stream " << sub_stream->DebugStreamPointers();
LOG(INFO) << DebugStreamPointers() << " did not enqueue 'start timer': " << t;
LOG(INFO) << DebugStreamPointers() << " did not enqueue 'stop timer': " << t;
LOG(INFO) << DebugStreamPointers() << " did not wait for " << other->DebugStreamPointers();
LOG(ERROR) << "Error waiting for event in stream: " << status.error_message() << "; not marking stream as bad, as the Event object may be " << "at fault. Monitor for further errors.";
LOG(INFO) << DebugStreamPointers() << " did not wait for an event.";
LOG(WARNING) << "attempting to perform BLAS operation using StreamExecutor " "without BLAS support";
LOG(INFO) << DebugStreamPointers() << " unable to initialize RNG";
LOG(INFO) << DebugStreamPointers() << " did not set RNG seed: " << static_cast<const void *>(seed) << "; bytes: " << seed_bytes;
LOG(INFO) << DebugStreamPointers() << " attempting to perform RNG operation using StreamExecutor" " without RNG support.";
LOG(INFO) << DebugStreamPointers() << " attempting to perform RNG operation using StreamExecutor" " without RNG support.";
LOG(INFO) << DebugStreamPointers() << " attempting to perform RNG operation using StreamExecutor" " without RNG support.";
LOG(INFO) << DebugStreamPointers() << " attempting to perform RNG operation using StreamExecutor" " without RNG support.";
LOG(INFO) << DebugStreamPointers() << " attempting to perform RNG operation using StreamExecutor" " without RNG support.";
LOG(INFO) << DebugStreamPointers() << " attempting to perform RNG operation using StreamExecutor" " without RNG support.";
LOG(INFO) << DebugStreamPointers() << " did not memcpy device-to-host; source: " << gpu_src.opaque();
LOG(INFO) << DebugStreamPointers() << " did not memcpy host-to-device; source: " << host_src;
LOG(INFO) << DebugStreamPointers() << " did not memcpy gpu-to-gpu; source: " << &gpu_src;
LOG(INFO) << DebugStreamPointers() << " did not memzero GPU location; source: " << location;
LOG(INFO) << DebugStreamPointers() << " did not memset GPU location; source: " << location << "; size: " << size << "; pattern: " << std::hex << pattern;
LOG(WARNING) << "Attempting to call ThenRnnBackward without DNN support";
LOG(WARNING) << "Attempting to call ThenRnnBackward without DNN support";
LOG(WARNING) << "Attempting to call ThenRnnBackward without DNN support";
LOG(INFO) << DebugStreamPointers() << " was in error state before adding host callback";
LOG(INFO) << DebugStreamPointers() << " was in error state before adding host callback";
LOG(INFO) << DebugStreamPointers() << " was in error state before adding callback to be run after " "next block-host-until-done.";
LOG(INFO) << DebugStreamPointers() << " attempting to perform FFT operation using StreamExecutor" " without FFT support";
LOG(INFO) << DebugStreamPointers() << " attempting to perform FFT operation using StreamExecutor" " without FFT support";
LOG(INFO) << DebugStreamPointers() << " attempting to perform FFT operation using StreamExecutor" " without FFT support";
LOG(INFO) << DebugStreamPointers() << " attempting to perform FFT operation using StreamExecutor" " without FFT support";
LOG(INFO) << DebugStreamPointers() << " attempting to perform FFT operation using StreamExecutor" " without FFT support";
LOG(INFO) << DebugStreamPointers() << " attempting to perform FFT operation using StreamExecutor" " without FFT support";
LOG(INFO) << DebugStreamPointers() << " " << status;
LOG(ERROR) << status;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/stream_executor/event.cc
LOG(ERROR) << status.error_message();
LOG(ERROR) << status.error_message();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/stream_executor/dnn.cc
LOG(FATAL) << "Unknown quantized_activation_mode " << static_cast<int32>(mode);
LOG(FATAL) << "Unknown activation_mode " << static_cast<int32>(mode);
LOG(FATAL) << "Unknown elementwise op " << static_cast<int32>(op);
LOG(FATAL) << "Unknown data layout " << static_cast<int32>(layout);
LOG(FATAL) << "Unknown filter layout " << static_cast<int32>(layout);
LOG(FATAL) << "Unknown filter layout " << static_cast<int32>(mode);
LOG(FATAL) << "Unknown layout " << layout;
LOG(FATAL) << "Cannot compute full strides for batch descriptor " << ToString() << ", because its layout is kBatchDepthYX4. In fact, " "cudnnSetTensorNdDescriptor doesn't work for kBatchDepthYX4 at all. " "Use cudnnSetTensor4DDescriptor to set cudnnTensorDescriptor_t " "instead.";
LOG(FATAL) << "Unknown layout " << static_cast<int32>(layout());
LOG(FATAL) << "Unknown layout " << static_cast<int32>(layout());
LOG(ERROR) << status.error_message();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/stream_executor/plugin_registry.cc
LOG(ERROR) << "A factory must be registered for a platform before being " << "set as default! " << "Platform name: " << platform_name << ", PluginKind: " << PluginKindString(plugin_kind) << ", PluginId: " << plugin_id;
LOG(ERROR) << "Invalid plugin kind specified: " << static_cast<int>(plugin_kind);
LOG(ERROR) << "Invalid plugin kind specified: " << PluginKindString(plugin_kind);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/stream_executor/gpu/redzone_allocator.cc
LOG(WARNING) << compiled_ptx_or.status().ToString() << "Relying on driver to perform ptx compilation. " << "Modify $PATH to customize ptxas location." << "This message will be only logged once.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/stream_executor/gpu/asm_compiler.cc
LOG(WARNING) << "Couldn't invoke " << ptxas_path << " --version";
LOG(WARNING) << "Running " << ptxas_path << " --version returned " << exit_code;
LOG(WARNING) << "Couldn't parse ptxas version in output of " << ptxas_path << " --version:" << out;
LOG(ERROR) << "You are using ptxas 8.x, but TF requires ptxas 9.x (and strongly " "prefers >= 9.2.88). Compilation of XLA kernels below will likely " "fail.You do not need to update CUDA; cherry-picking the ptxas " "binary is sufficient.";
LOG(WARNING) << "*** WARNING *** You are using ptxas " << vmaj << "." << vmin << "." << vdot << ", which is older than 9.2.88. ptxas 9.x before 9.2.88 is known to " "miscompile XLA code, leading to incorrect results or " "invalid-address errors.You do not need to update to CUDA " "9.2.88; cherry-picking the ptxas binary is sufficient.";
VLOG(2) << "Looking for ptxas at " << ptxas_path;
VLOG(2) << "Using ptxas at " << ptxas_path;
VLOG(2) << "ptx written to: " << ptx_path;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/stream_executor/gpu/gpu_timer.cc
LOG(ERROR) << status;
LOG(ERROR) << status;
LOG(ERROR) << status;
LOG(ERROR) << status;
LOG(ERROR) << status;
LOG(ERROR) << status;
LOG(ERROR) << status;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/stream_executor/gpu/gpu_stream.cc
LOG(ERROR) << status.error_message();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/stream_executor/host/host_gpu_executor.cc
LOG(WARNING) << "Host callback failed: " << s;
LOG(ERROR) << "Unable to retrieve BLAS factory: " << status.status().error_message();
LOG(ERROR) << "Unable to retrieve FFT factory: " << status.status().error_message();
LOG(ERROR) << "Unable to retrieve RNG factory: " << status.status().error_message();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/stream_executor/host/host_platform.cc
LOG(FATAL) << "not yet implemented: register host trace listener";
LOG(FATAL) << "not yet implemented: unregister host trace listener";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc
LOG(FATAL) << "Unsupported Cudnn convolution forward algorithm: " << algorithm.algo_id();
LOG(FATAL) << "Unsupported Cudnn convolution backward algorithm for data: " << algorithm.algo_id();
LOG(FATAL) << "Unsupported Cudnn convolution backward algorithm for filter: " << algorithm.algo_id();
LOG(FATAL) << "Unsupported Cudnn RNN algorithm: " << algorithm->algo_id();
LOG(ERROR) << error;
LOG(ERROR) << "Could not create cudnn handle: " << ToString(status);
LOG(ERROR) << "Error retrieving driver version: " << cuda::DriverVersionStatusToString(result);
LOG(ERROR) << "Possibly insufficient driver version: " << cuda::DriverVersionToString(version);
LOG(FATAL) << "Unsupported tensor format " << DataLayoutString(batch_descriptor.layout());
LOG(FATAL) << "Unsupported filter format " << FilterLayoutString(filter_descriptor.layout());
VLOG(2) << "Requesting grouped convolution: " << convolution_descriptor.group_count();
LOG(FATAL) << "unrecognized activation mode: " << static_cast<int>(activation_mode);
LOG(FATAL) << "Invalid DNN data type: " << static_cast<int>(data_type);
LOG(FATAL) << "Invalid RNN input mode: " << static_cast<int>(input_mode);
LOG(FATAL) << "Invalid RNN direction mode: " << static_cast<int>(direction_mode);
LOG(FATAL) << "Invalid RNN Mode: " << static_cast<int>(rnn_mode);
LOG(FATAL) << "Invalid DNN data type: " << static_cast<int>(data_type);
LOG(FATAL) << "Invalid RNN Mode: " << static_cast<int>(rnn_mode);
LOG(FATAL) << "Invalid RNN data type: " << static_cast<int>(data_type);
LOG(FATAL) << "Invalid DNN data type: " << static_cast<int>(data_type);
VLOG(2) << "conv_input_scale = " << conv_input_scale << "conv_input_nd.handle() = " << conv_input_nd.handle() << "conv_input_data.opaque() = " << conv_input_data.opaque() << "filter.handle() = " << filter.handle() << "filter_data.opaque() = " << filter_data.opaque() << "conv.handle() = " << conv.handle() << "algo = " << algo_desc.algo_id() << "scratch.opaque() = " << scratch.opaque() << "scratch.size() = " << scratch.size() << "side_input_scale = " << side_input_scale << "output_nd.handle() = " << output_nd.handle() << "side_input_data_ptr = " << side_input_data_ptr << "bias_nd.handle() = " << bias_nd.handle() << "biases.opaque() = " << biases.opaque() << "activation_desc.handle() = " << activation_desc.handle() << "output_nd.handle() = " << output_nd.handle() << "output_data->opaque() = " << output_data->opaque();
LOG(WARNING) << "cudnnConvolutionBiasActivationForward() for int8 is only " "supported on GPUs with compute capability 6.1 or later.";
LOG(WARNING) << "cudnnConvolutionBiasActivationForward() for int8 is only " "supported on GPUs with compute capability 6.1 or later.";
LOG(ERROR) << "MatMul input and output dimensions are not compatible.";
LOG(ERROR) << "Unsupported MatMul input layout.";
LOG(ERROR) << "Unsupported MatMul output layout.";
LOG(ERROR) << "Unsupported MatMul output layout.";
LOG(ERROR) << "stream " << stream << " could not enqueue a tensor copy as part of bias addition.";
LOG(ERROR) << "CUDA LRN does not support cudnn-around mode";
LOG(ERROR) << "CUDA LRN does not support segmentation";
LOG(ERROR) << "CUDA LRN does not support cudnn-around mode";
LOG(ERROR) << "CUDA LRN does not support segmentation";
LOG(ERROR) << "CudnnSupport::DoDepthConcatenate currently only " "supports the kBatchDepthYX layout.";
LOG(ERROR) << "BlockHostUntilDone failed: " << block_status;
LOG(INFO) << output_dimensions.ElementCount() << ' ' << batch << ' ' << yx << ' ' << depth;
LOG(FATAL) << "not yet implemented";
LOG(FATAL) << "not yet implemented";
LOG(FATAL) << "not yet implemented";
LOG(ERROR) << "quantized memcpy not supported by cuDNN";
LOG(ERROR) << "quantized memcpy not supported by cuDNN";
LOG(ERROR) << "Attempting to initialize an instance of the cuDNN " << "support library with a non-CUDA StreamExecutor";
LOG(ERROR) << "Unable to register cuDNN factory: " << status.error_message();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/stream_executor/cuda/cuda_diagnostics.cc
VLOG(2) << "version string "" << value << "" made value " << DriverVersionToString(result);
LOG(INFO) << "kernel driver is installed, but does not appear to be " "running on this host " << "(" << port::Hostname() << ")";
LOG(INFO) << "kernel driver does not appear to be installed on this host " << "(" << port::Hostname() << ")";
LOG(INFO) << "kernel driver does not appear to be running on this host " << "(" << port::Hostname() << "): " << "/proc/driver/nvidia/version does not exist";
LOG(INFO) << "no NVIDIA GPU device is present: " << dev0_path << " does not exist";
LOG(INFO) << "retrieving CUDA diagnostic information for host: " << port::Hostname();
LOG(INFO) << "hostname: " << port::Hostname();
VLOG(1) << "LD_LIBRARY_PATH is: "" << library_path << """;
VLOG(1) << "could not open "" << piece << """;
VLOG(1) << piece << " :: " << entity->d_name;
LOG(INFO) << "libcuda reported version is: " << cuda::DriverVersionStatusToString(dso_version);
LOG(INFO) << "kernel reported version is: " << cuda::DriverVersionStatusToString(kernel_version);
VLOG(1) << "found DLL info with name: " << info->dlpi_name;
VLOG(1) << "found DLL info with resolved path: " << resolved_path;
LOG(INFO) << "kernel version seems to match DSO: " << cuda::DriverVersionToString(kernel_version.ValueOrDie());
LOG(ERROR) << "kernel version " << cuda::DriverVersionStatusToString(kernel_version) << " does not match DSO version " << cuda::DriverVersionStatusToString(dso_version) << " -- cannot find working devices in this configuration";
VLOG(1) << "driver version file contents: """" << contents.begin() << """"";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/stream_executor/cuda/cuda_kernel.cc
LOG(FATAL) << "Unknown KernelCacheConfig" << static_cast<int32>(preferred_cache_config_);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/stream_executor/cuda/cuda_rng.cc
LOG(ERROR) << "failed to create random number generator: " << ret;
LOG(ERROR) << "failed to set stream for random generation: " << ret;
LOG(ERROR) << "failed to do uniform generation of " << v->ElementCount() << " " << TypeString<T>() << "s at " << v->opaque() << ": " << ret;
LOG(ERROR) << "failed to do gaussian generation of " << v->ElementCount() << " floats at " << v->opaque() << ": " << ret;
LOG(ERROR) << "failed to set rng seed: " << ret;
LOG(ERROR) << "failed to reset rng position: " << ret;
LOG(ERROR) << "Attempting to initialize an instance of the cuRAND " << "support library with a non-CUDA StreamExecutor";
LOG(ERROR) << "Unable to register cuRAND factory: " << status.error_message();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/stream_executor/cuda/cuda_blas.cc
LOG(ERROR) << "failed to get old cublas pointer mode: " << ToString(ret);
LOG(ERROR) << "failed to set new cublas pointer mode: " << ToString(ret);
LOG(ERROR) << "failed to set former cublas pointer mode: " << ToString(ret);
LOG(ERROR) << "failed to get old cublas math mode: " << ToString(ret);
LOG(ERROR) << "failed to set new cublas math mode: " << ToString(ret);
LOG(ERROR) << "failed to set former cublas math mode: " << ToString(ret);
LOG(ERROR) << "failed to create cublas handle: " << ToString(ret);
LOG(ERROR) << "failed to set stream for cuBLAS calls: " << ToString(ret);
LOG(FATAL) << "Invalid value of blas::Transpose.";
LOG(FATAL) << "Invalid value of blas::UpperLower.";
LOG(FATAL) << "Invalid value of blas::Diagonal.";
LOG(FATAL) << "Invalid value of blas::Side.";
LOG(ERROR) << "failed to run cuBLAS routine: " << ToString(ret);
VLOG(1) << absl::StrFormat( "doing cuBLAS SGEMM: at=%d bt=%d m=%u n=%u " "k=%u alpha=%f a=%p lda=%d b=%p ldb=%d beta=%f " "c=%p ldc=%d", static_cast<int>(transa), static_cast<int>(transb), m, n, k, alpha, a.opaque(), lda, b.opaque(), ldb, beta, c->opaque(), ldc);
LOG(WARNING) << "GEMM lda was smaller than m (no transpose case); " "precondition violation";
LOG(WARNING) << "GEMM lda (" << lda << ") was smaller than k (" << k << ") (transpose case); precondition violation";
LOG(WARNING) << "GEMM ldb (" << ldb << ") was smaller than k (" << k << ") (no transpose case); precondition violation";
LOG(WARNING) << "GEMM ldb was smaller than n (transpose case); " "precondition violation";
LOG(ERROR) << "fp16 sgemm is not implemented in this cuBLAS version " << "(need at least CUDA 7.5)";
VLOG(1) << absl::StrFormat( "doing cuBLAS SGEMM: at=%d bt=%d m=%u n=%u " "k=%u alpha=%f a=%p lda=%d b=%p ldb=%d beta=%f " "c=%p ldc=%d", static_cast<int>(transa), static_cast<int>(transb), m, n, k, alpha, a.opaque(), lda, b.opaque(), ldb, beta, c->opaque(), ldc);
LOG(WARNING) << "GEMM lda was smaller than m (no transpose case); " "precondition violation";
LOG(WARNING) << "GEMM lda (" << lda << ") was smaller than k (" << k << ") (transpose case); precondition violation";
LOG(WARNING) << "GEMM ldb (" << ldb << ") was smaller than k (" << k << ") (no transpose case); precondition violation";
LOG(WARNING) << "GEMM ldb was smaller than n (transpose case); " "precondition violation";
VLOG(2) << "DoBlasGemmWithAlgorithm returning false because sm" << cc_major << cc_minor << " devices don't support explicit gemm algorithms.";
VLOG(2) << "DoBlasGemmWithAlgorithm returning false because algorithm " << algorithm << " uses tensor ops, but tensor ops are not available in sm" << cc_major << "X devices.";
VLOG(2) << "DoBlasGemmWithAlgorithm returning false because algorithm " << algorithm << " uses tensor ops, but the input data type is not fp16.";
VLOG(2) << "DoBlasGemmWithAlgorithm returning false because one of `alpha` " "and `beta` is a pointer, but the other is not.";
VLOG(2) << "DoBlasGemmWithAlgorithm returning false because " "output_profile_result was given, but we were unable to " "create a GpuTimer.";
VLOG(2) << "DoBlasGemmWithAlgorithm returning false to work around cudnn " "<9.2 bug with m, n, or k >= 2097153. See b/79126339.";
VLOG(2) << "DoBlasGemmWithAlgorithm returning false; unable to stop " "GpuTimer.";
LOG(ERROR) << status;
LOG(ERROR) << status;
LOG(ERROR) << status;
LOG(ERROR) << status;
LOG(ERROR) << status;
LOG(ERROR) << "failed BLAS call, see log for details";
LOG(ERROR) << "failed BLAS call, see log for details";
LOG(ERROR) << "Attempting to initialize an instance of the cuBLAS " << "support library with a non-CUDA StreamExecutor";
LOG(ERROR) << "Unable to register cuBLAS factory: " << status.error_message();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/stream_executor/cuda/cuda_platform.cc
LOG(QFATAL) << "Unknown option for environment variable " "TF_CUDA_PLATFORM_GPU_DEVICE_SCHEDULE " << gpu_schedule_string << " should be one of {" << kScheduleBlockingSyncString << ", " << kScheduleSpinString << ", " << kScheduleYieldString << "}";
LOG(FATAL) << "not yet implemented: register CUDA trace listener";
LOG(FATAL) << "not yet implemented: unregister CUDA trace listener";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/stream_executor/cuda/cuda_event.cc
LOG(ERROR) << "Error polling for event status: " << status.status().error_message();
LOG(INFO) << "Error condition returned for event status: " << status.ValueOrDie();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/stream_executor/cuda/cuda_gpu_executor.cc
VLOG(2) << "found compute-capability-specific file, using that: " << cc_specific;
VLOG(2) << "could not find compute-capability specific file at: " << cc_specific;
LOG(ERROR) << "Feature not supported on CUDA platform (FindOnDiskForISAVersion)";
VLOG(3) << "Loaded CUBIN " << static_cast<const void *>(cubin) << " as module " << *module;
VLOG(3) << "CUBIN " << static_cast<const void *>(cubin) << " is already loaded as module " << *module;
VLOG(3) << "Loaded PTX " << static_cast<const void *>(ptx) << " as module " << *module;
VLOG(3) << "PTX " << static_cast<const void *>(ptx) << " is already loaded as module " << module;
VLOG(3) << "GetKernel on kernel " << kernel << " : " << kernel->name();
LOG(FATAL) << "Loader spec has no ptx for kernel " << *kernelname;
VLOG(2) << "getting function " << *kernelname << " from module " << module;
VLOG(3) << "No loaded CUDA module for " << gpu_binary;
VLOG(3) << "Found CUDA module " << module << " with refcount " << refcount;
VLOG(3) << "Unloading CUDA module " << module;
VLOG(3) << "Unloading kernel " << kernel << " : " << kernel->name();
VLOG(3) << "Kernel " << kernel << " : " << kernel->name() << " has never been loaded.";
VLOG(3) << "Kernel " << kernel << " : " << kernel->name() << " has loaded GPU code " << gpu_binary_it->second;
VLOG(2) << "Computing kernel occupancy for kernel " << kernel.demangled_name();
VLOG(2) << "Thread dimensions (" << thread_dims.x << ", " << thread_dims.y << ", " << thread_dims.z << ")";
VLOG(2) << "Resident blocks per SM is " << blocks_per_sm;
VLOG(2) << "The cuda occupancy calculator recommends using " << suggested_threads << " threads per block to achieve an occupancy of " << blocks_per_sm << " blocks per SM.";
LOG(WARNING) << "attempting to register null or zero-sized memory: " << location << "; size " << size;
VLOG(2) << "registering " << location << " size " << size;
VLOG(2) << "unregistering " << location;
VLOG(2) << "enqueueing memset8 operation onto stream " << stream << " at location " << location << " with size " << size << " and pattern " << std::hex << pattern;
VLOG(2) << "enqueueing memset32 operation onto stream " << stream << " at location " << location << " with size " << size << " and pattern " << std::hex << pattern;
LOG(WARNING) << "Host callback failed: " << s;
LOG(ERROR) << "Deallocating stream with pending work";
LOG(ERROR) << "failed to record completion event; " "therefore, failed to create inter-stream dependency";
LOG(ERROR) << "Unable to retrieve BLAS factory: " << status.status().error_message();
LOG(ERROR) << "Unable to retrieve DNN factory: " << status.status().error_message();
LOG(ERROR) << "Unable to retrieve FFT factory: " << status.status().error_message();
LOG(ERROR) << "Unable to retrieve RNG factory: " << status.status().error_message();
LOG(FATAL) << "Invalid shared memory configuration returned: " << cuda_config.ValueOrDie();
LOG(FATAL) << "Invalid shared memory configuration specified: " << static_cast<int>(config);
LOG(INFO) << "Failed to find symbol in any modules: " << symbol_name;
LOG(INFO) << "OS X does not support NUMA - returning NUMA node zero";
LOG(INFO) << "ARM64 does not support NUMA - returning NUMA node zero";
VLOG(2) << "trying to read NUMA node for device ordinal: " << device_ordinal;
LOG(INFO) << "no PCI bus ID for device ordinal: " << device_ordinal;
LOG(ERROR) << "could not open file to read NUMA node: " << filename << "Your kernel may have been built without NUMA support.";
LOG(INFO) << "successful NUMA node read from SysFS had negative value (" << value << "), but there must be at least one NUMA node" ", so returning NUMA node zero";
LOG(WARNING) << "could not convert SysFS file contents to integral NUMA node value: " << content;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/stream_executor/cuda/cublas_stub.cc
LOG(FATAL) << symbol_name << " symbol not found.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/stream_executor/cuda/cuda_driver.cc
LOG(FATAL) << "current context was not created by the StreamExecutor " "cuda_driver API: " << current << "; a CUDA runtime call " "was likely performed without using a StreamExecutor context";
LOG(FATAL) << "impossible memory space";
VLOG(3) << "ScopedActivateContext switching context from " << tls->id << " to " << cuda_context->id();
LOG(ERROR) << "could not query device: " << value.status();
LOG(ERROR) << "could not query device: " << value.status();
LOG(ERROR) << "could not retrieve source pointer's context: " << from_context.status();
LOG(ERROR) << "could not retrieve destination pointer's context: " << to_context.status();
LOG(ERROR) << "injecting CUDA init error; initialization will fail";
LOG(ERROR) << "failed call to cuInit: " << ToString(res);
LOG(WARNING) << "could not convert all device options into context flags";
LOG(ERROR) << "The primary context is active and has a different flag set (" << former_primary_context_flags << ") than the desired flag set (" << flags << ").";
VLOG(2) << "The primary context " << former_context << " for device " << device << " exists before initializing the StreamExecutor.";
LOG(WARNING) << "A non-primary context " << former_context << " for device " << device << " exists before initializing the StreamExecutor. The " << "primary context is now " << new_context << ". We " << "haven't verified StreamExecutor works with that.";
LOG(ERROR) << "Failed to get the device of the current context " << former_context;
VLOG(2) << "created or reused context " << new_context << " for this thread";
LOG(ERROR) << "failed to release CUDA context; leaking: " << ToString(res);
VLOG(2) << "launching kernel: " << function << "; gdx: " << grid_dim_x << " gdy: " << grid_dim_y << " gdz: " << grid_dim_z << " bdx: " << block_dim_x << " bdy: " << block_dim_y << " bdz: " << block_dim_z;
LOG(ERROR) << "failed to load PTX text as a module: " << ToString(res);
LOG(ERROR) << "error log buffer (" << error_log_buffer_bytes << " bytes): " << error_log_buffer.data();
VLOG(3) << "PTX compilation info log (" << info_log_buffer_bytes << " bytes): " << info_log_buffer.data();
VLOG(3) << "PTX compilation error log (" << error_log_buffer_bytes << " bytes): " << error_log_buffer.data();
LOG(ERROR) << "unable to add host callback: " << ToString(res);
LOG(ERROR) << "failed to get PTX kernel "" << kernel_name << "" from module: " << ToString(res);
VLOG(2) << "failed to get symbol "" << symbol_name << "" from module: " << ToString(res);
LOG(ERROR) << "failed to unload module " << module << "; leaking: " << ToString(res);
LOG(ERROR) << "could not allocate CUDA stream for context " << context->context() << ": " << ToString(res);
VLOG(2) << "successfully created stream " << *stream << " for context " << context->context() << " on thread";
LOG(ERROR) << "failed to destroy CUDA stream for context " << context->context() << ": " << ToString(res);
VLOG(2) << "successfully destroyed stream " << *stream << " for context " << context->context();
LOG(INFO) << "failed to allocate " << port::HumanReadableNumBytes::ToString(bytes) << " (" << bytes << " bytes) from device: " << ToString(res);
VLOG(2) << "allocated " << ptr << " for context " << context->context() << " of " << bytes << " bytes";
LOG(ERROR) << "failed to free device memory at " << location << "; result: " << ToString(res);
VLOG(2) << "deallocated " << location << " for context " << context->context();
LOG(ERROR) << "failed to alloc " << bytes << " bytes unified memory; result: " << ToString(res);
VLOG(2) << "allocated " << ptr << " for context " << context->context() << " of " << bytes << " bytes in unified memory";
LOG(ERROR) << "failed to free unified memory at " << location << "; result: " << ToString(res);
VLOG(2) << "deallocated unified memory at " << location << " for context " << context->context();
LOG(ERROR) << "failed to alloc " << bytes << " bytes on host: " << ToString(res);
LOG(ERROR) << "error deallocating host memory at " << location << ": " << ToString(res);
LOG(ERROR) << "error registering host memory at " << location << ": " << ToString(res);
LOG(ERROR) << "error unregistering host memory at " << location << ": " << ToString(res);
LOG(ERROR) << "failed to synchronize the stop event: " << ToString(res);
LOG(ERROR) << "failed to get elapsed time between events: " << ToString(res);
LOG(ERROR) << "could not wait stream on event: " << ToString(res);
LOG(ERROR) << "could not synchronize on CUDA context: " << ToString(res) << " :: " << port::CurrentStackTrace();
LOG(ERROR) << "stream in bad state on status query: " << ToString(res);
VLOG(2) << "successfully sync memcpy'd d2h of " << size << " bytes to " << host_dst;
VLOG(2) << "successfully enqueued sync memcpy h2d of " << size << " bytes";
VLOG(2) << "successfully sync memcpy'd d2d of " << size << " bytes";
LOG(ERROR) << absl::StrFormat( "failed to enqueue async memcpy from device to host: %s; host dst: %p; " "GPU src: %p; size: %u=0x%x", ToString(res), host_dst, absl::bit_cast<void*>(gpu_src), size, size);
VLOG(2) << "successfully enqueued async memcpy d2h of " << size << " bytes from " << absl::bit_cast<void*>(gpu_src) << " to " << host_dst << " on stream " << stream;
LOG(ERROR) << absl::StrFormat( "failed to enqueue async memcpy from host to device: %s; GPU dst: %p; " "host src: %p; size: %u=0x%x", ToString(res), absl::bit_cast<void*>(gpu_dst), host_src, size, size);
VLOG(2) << "successfully enqueued async memcpy h2d of " << size << " bytes" << " on stream " << stream;
LOG(ERROR) << absl::StrFormat( "failed to enqueue async memcpy from device to device: %s" "; GPU dst: %p on %s %s" "; GPU src: %p on %s %s" "; can access? %s; size: %u=0x%x", ToString(result), absl::bit_cast<void*>(gpu_dst), CUDAPointerToMemorySpaceString(gpu_dst), CUDAPointerToDeviceString(gpu_dst), absl::bit_cast<void*>(gpu_src), CUDAPointerToMemorySpaceString(gpu_src), CUDAPointerToDeviceString(gpu_src), CUDAPointersToCanAccessString(gpu_src, gpu_dst), size, size);
VLOG(2) << "successfully enqueued async memcpy d2d of " << size << " bytes";
LOG(FATAL) << "impossible event flags: " << int(flags);
LOG(ERROR) << "could not retrieve CUDA device count: " << ToString(res);
LOG(ERROR) << "failed to query max grid dim x: " << ToString(res);
LOG(ERROR) << "failed to query max grid dim y: " << ToString(res);
LOG(ERROR) << "failed to query max grid dim z: " << ToString(res);
LOG(ERROR) << "failed to query driver version: " << ToString(res);
LOG(ERROR) << "failed to query device properties: " << ToString(res);
LOG(ERROR) << "failed to query ECC status: " << ToString(res);
LOG(ERROR) << "failed to query device memory info: " << ToString(res);
LOG(ERROR) << "failed to query total available memory: " << ToString(res);
LOG(ERROR) << "failed to query PCI bus id for device: " << ToString(res);
LOG(ERROR) << "failed to resolve 'from' peer access context to a device: " << from_device.status();
LOG(ERROR) << "failed to resolve 'to' peer access context to a device: " << to_device.status();
LOG(ERROR) << "failed to detect peer access capability: " << ToString(res);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/stream_executor/cuda/cuda_fft.cc
LOG(FATAL) << "Invalid value of fft::Type.";
LOG(ERROR) << "failed to run cuFFT routine cufftSetStream: " << ret;
LOG(FATAL) << "Try to repeatedly initialize.";
LOG(ERROR) << "failed to create cuFFT 1d plan:" << ret;
LOG(ERROR) << "failed to create cuFFT 2d plan:" << ret;
LOG(ERROR) << "failed to create cuFFT 3d plan:" << ret;
LOG(ERROR) << "Invalid rank value for cufftPlan. " "Requested 1, 2, or 3, given: " << rank;
LOG(ERROR) << "failed to create cuFFT plan:" << ret;
LOG(ERROR) << "failed to set auto allocation for cuFFT plan:" << ret;
LOG(ERROR) << "failed to make cuFFT 1d plan:" << ret;
LOG(ERROR) << "failed to make cuFFT 2d plan:" << ret;
LOG(ERROR) << "failed to make cuFFT 3d plan:" << ret;
LOG(ERROR) << "Invalid rank value for cufftPlan. " "Requested 1, 2, or 3, given: " << rank;
LOG(ERROR) << "failed to create cuFFT batched plan:" << ret;
LOG(ERROR) << "failed to create cuFFT batched plan:" << ret;
LOG(ERROR) << "failed to set auto allocation for cuFFT batched plan:" << ret;
LOG(ERROR) << "failed to make cuFFT batched plan:" << ret;
LOG(ERROR) << "failed to allocate work area.";
LOG(ERROR) << "failed to set work area for cuFFT plan:" << ret;
LOG(FATAL) << "Try to get fft direction before initialization.";
LOG(FATAL) << "Invalid value of fft::Type.";
LOG(ERROR) << "Plan Parameters: num_x: " << num_x;
LOG(FATAL) << "failed to initialize cufft 1d plan: " << status.error_message();
LOG(ERROR) << "Plan Parameters: num_x: " << num_x;
LOG(FATAL) << "failed to initialize cufft 1d plan with customized allocator: " << status.error_message();
LOG(ERROR) << "Plan Parameters: num_x: " << num_x << " num_y: " << num_y;
LOG(FATAL) << "failed to initialize cufft 2d plan: " << status.error_message();
LOG(ERROR) << "Plan Parameters: num_x: " << num_x << " num_y: " << num_y;
LOG(FATAL) << "failed to initialize cufft 2d plan with customized allocator: " << status.error_message();
LOG(ERROR) << "Plan Parameters: num_x: " << num_x << " num_y: " << num_y << " num_z: " << num_z;
LOG(FATAL) << "failed to initialize cufft 3d plan: " << status.error_message();
LOG(ERROR) << "Plan Parameters: num_x: " << num_x << " num_y: " << num_y << " num_z: " << num_z;
LOG(FATAL) << "failed to initialize cufft 3d plan with customized allocator: " << status.error_message();
LOG(ERROR) << "Initialize Params: rank: " << rank << " elem_count: " << *elem_count << " input_embed: " << *input_embed << " input_stride: " << input_stride << " input_distance: " << input_distance << " output_embed: " << *output_embed << " output_stride: " << output_stride << " output_distance: " << output_distance << " batch_count: " << batch_count;
LOG(FATAL) << "failed to initialize batched cufft plan: " << status.error_message();
LOG(ERROR) << "Initialize Params: rank: " << rank << " elem_count: " << *elem_count << " input_embed: " << *input_embed << " input_stride: " << input_stride << " input_distance: " << input_distance << " output_embed: " << *output_embed << " output_stride: " << output_stride << " output_distance: " << output_distance << " batch_count: " << batch_count;
LOG(FATAL) << "failed to initialize batched cufft plan with customized allocator: " << status.error_message();
LOG(FATAL) << "failed to update custom allocator for cufft plan: " << status.error_message();
LOG(ERROR) << "the passed-in plan is not a CUDAFftPlan object.";
LOG(ERROR) << "failed to run cuFFT routine: " << ret;
LOG(ERROR) << "the passed-in plan is not a CUDAFftPlan object.";
LOG(ERROR) << "failed to run cuFFT routine: " << ret;
LOG(ERROR) << "Attempting to initialize an instance of the cuFFT " << "support library with a non-CUDA StreamExecutor";
LOG(ERROR) << "Unable to register cuFFT factory: " << status.error_message();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/stream_executor/cuda/cudart_stub.cc
LOG(INFO) << "Ignore above cudart dlerror if you do not have a GPU set " "up on your machine.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/stream_executor/platform/default/dso_loader.cc
LOG(INFO) << "Successfully opened dynamic library " << filename;
LOG(WARNING) << message;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/stream_executor/platform/default/dlopen_checker.cc
LOG(INFO) << "Not built with GPU enabled. Skip GPU library dlopen check.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc
LOG(INFO) << "GPU libraries are statically linked, skip dlopen check.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/stream_executor/lib/statusor.cc
LOG(ERROR) << kMessage;
LOG(FATAL) << "Attempting to fetch value instead of handling error " << status;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/stream_executor/rocm/rocm_fft.cc
LOG(FATAL) << "Invalid value of fft::Type.";
LOG(ERROR) << "failed to run rocFFT routine hipfftSetStream: " << ret;
LOG(FATAL) << "Try to repeatedly initialize.";
LOG(ERROR) << "failed to create rocFFT 1d plan:" << ret;
LOG(ERROR) << "failed to create rocFFT 2d plan:" << ret;
LOG(ERROR) << "failed to create rocFFT 3d plan:" << ret;
LOG(ERROR) << "Invalid rank value for hipfftPlan. " "Requested 1, 2, or 3, given: " << rank;
LOG(ERROR) << "failed to create rocFFT plan:" << ret;
LOG(ERROR) << "failed to set auto allocation for rocFFT plan:" << ret;
LOG(ERROR) << "failed to make rocFFT 1d plan:" << ret;
LOG(ERROR) << "failed to make rocFFT 2d plan:" << ret;
LOG(ERROR) << "failed to make rocFFT 3d plan:" << ret;
LOG(ERROR) << "Invalid rank value for hipfftPlan. " "Requested 1, 2, or 3, given: " << rank;
LOG(ERROR) << "failed to allocate work area.";
LOG(ERROR) << "failed to set work area for rocFFT plan:" << ret;
LOG(ERROR) << "failed to create rocFFT batched plan:" << ret;
LOG(ERROR) << "failed to create rocFFT batched plan:" << ret;
LOG(ERROR) << "failed to set auto allocation for rocFFT batched plan:" << ret;
LOG(ERROR) << "failed to make rocFFT batched plan:" << ret;
LOG(ERROR) << "failed to allocate work area.";
LOG(ERROR) << "failed to set work area for rocFFT batched plan:" << ret;
LOG(FATAL) << "Try to get fft direction before initialization.";
LOG(FATAL) << "Invalid value of fft::Type.";
LOG(FATAL) << "failed to initialize hipfft 1d plan: " << status.error_message();
LOG(FATAL) << "failed to initialize hipfft 1d plan with customized allocator: " << status.error_message();
LOG(FATAL) << "failed to initialize hipfft 2d plan: " << status.error_message();
LOG(FATAL) << "failed to initialize hipfft 2d plan with customized allocator: " << status.error_message();
LOG(FATAL) << "failed to initialize hipfft 3d plan: " << status.error_message();
LOG(FATAL) << "failed to initialize hipfft 3d plan with customized allocator: " << status.error_message();
LOG(FATAL) << "failed to initialize batched hipfft plan: " << status.error_message();
LOG(FATAL) << "failed to initialize batched hipfft plan with customized " "allocator: " << status.error_message();
LOG(ERROR) << "update plan with scratch allocator not implemented";
LOG(ERROR) << "the passed-in plan is not a ROCMFftPlan object.";
LOG(ERROR) << "failed to run rocFFT routine: " << ret;
LOG(ERROR) << "the passed-in plan is not a ROCMFftPlan object.";
LOG(ERROR) << "failed to run rocFFT routine: " << ret;
LOG(ERROR) << "Attempting to initialize an instance of the rocFFT " << "support library with a non-ROCM StreamExecutor";
LOG(ERROR) << "Unable to register rocFFT factory: " << status.error_message();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/stream_executor/rocm/rocm_driver.cc
LOG(FATAL) << "impossible memory space";
LOG(FATAL) << "failed to query current device: " << ToString(result);
LOG(FATAL) << "Synchronize found " << ToString(res) << " :: " << port::CurrentStackTrace();
VLOG(3) << "ScopedActivateContext switching device from " << tls->current_device_ordinal << " to " << context->device_ordinal();
VLOG(3) << "ScopedActivateContext switching device from " << tls->current_device_ordinal << " to " << to_restore_->device_ordinal();
LOG(ERROR) << "could not query device: " << value.status();
LOG(ERROR) << "could not query device: " << value.status();
LOG(ERROR) << "could not retrieve source pointer's device: " << ToString(result);
LOG(ERROR) << "could not retrieve destination pointer's device: " << ToString(result);
LOG(ERROR) << "injecting ROCM init error; initialization will fail";
LOG(ERROR) << "failed call to hipInit: " << ToString(res);
VLOG(2) << "launching kernel: " << function << "; gdx: " << grid_dim_x << " gdy: " << grid_dim_y << " gdz: " << grid_dim_z << " bdx: " << block_dim_x << " bdy: " << block_dim_y << " bdz: " << block_dim_z << " smem: " << shared_mem_bytes;
VLOG(2) << "successfully launched kernel";
LOG(ERROR) << "Feature not supported on ROCm platform (LoadPtx)";
VLOG(2) << "successfully enqueued async memset operation";
LOG(ERROR) << "unable to add host callback: " << ToString(res);
LOG(ERROR) << "failed to get kernel "" << kernel_name << "" from module: " << ToString(res);
VLOG(2) << "failed to get symbol "" << symbol_name << "" from module: " << ToString(res);
LOG(ERROR) << "failed to unload module " << module << "; leaking: " << ToString(res);
LOG(ERROR) << "could not allocate ROCM stream for device " << context->device_ordinal() << ": " << ToString(res);
VLOG(2) << "successfully created stream " << *stream << " for device " << context->device_ordinal() << " on thread";
LOG(ERROR) << "failed to destroy ROCM stream for device " << context->device_ordinal() << ": " << ToString(res);
VLOG(2) << "successfully destroyed stream " << *stream << " for device " << context->device_ordinal();
LOG(ERROR) << "failed to allocate " << port::HumanReadableNumBytes::ToString(bytes) << " (" << bytes << " bytes) from device: " << ToString(res);
VLOG(2) << "allocated " << ptr << " for device " << context->device_ordinal() << " of " << bytes << " bytes";
LOG(ERROR) << "failed to free device memory at " << location << "; result: " << ToString(res);
VLOG(2) << "deallocated " << location << " for device " << context->device_ordinal();
LOG(ERROR) << "Feature not supported on ROCm platform (UnifiedMemoryAllocate)";
LOG(ERROR) << "Feature not supported on ROCm platform (UnifiedMemoryDeallocate)";
LOG(ERROR) << "failed to alloc " << bytes << " bytes on host: " << ToString(res);
LOG(ERROR) << "error deallocating host memory at " << location << ": " << ToString(res);
LOG(ERROR) << "error registering host memory at " << location << ": " << ToString(res);
LOG(ERROR) << "error unregistering host memory at " << location << ": " << ToString(res);
LOG(ERROR) << "failed to synchronize the stop event: " << ToString(res);
LOG(ERROR) << "failed to get elapsed time between events: " << ToString(res);
LOG(ERROR) << "could not wait stream on event: " << ToString(res);
LOG(ERROR) << "could not synchronize on ROCM device: " << ToString(res) << " :: " << port::CurrentStackTrace();
VLOG(2) << "successfully synchronized stream " << stream << " on device " << context->device_ordinal();
LOG(ERROR) << "stream in bad state on status query: " << ToString(res);
VLOG(2) << "successfully sync memcpy'd d2h of " << size << " bytes to " << host_dst;
VLOG(2) << "successfully enqueued sync memcpy h2d of " << size << " bytes";
VLOG(2) << "successfully sync memcpy'd d2d of " << size << " bytes";
LOG(ERROR) << absl::StrFormat( "failed to enqueue async memcpy from device to host: %s; host dst: %p; " "Gpu src: %p; size: %llu=0x%llx", ToString(res).c_str(), host_dst, absl::bit_cast<void*>(gpu_src), size, size);
VLOG(2) << "successfully enqueued async memcpy d2h of " << size << " bytes from " << absl::bit_cast<void*>(gpu_src) << " to " << host_dst << " on stream " << stream;
LOG(ERROR) << absl::StrFormat( "failed to enqueue async memcpy from host to device: %s; Gpu dst: %p; " "host src: %p; size: %llu=0x%llx", ToString(res).c_str(), absl::bit_cast<void*>(gpu_dst), host_src, size, size);
VLOG(2) << "successfully enqueued async memcpy h2d of " << size << " bytes" << " on stream " << stream;
LOG(ERROR) << absl::StrFormat( "failed to enqueue async memcpy from device to device: %s" "; Gpu dst: %p on %s %s" "; Gpu src: %p on %s %s" "; can access? %s; size: %llu=0x%llx", ToString(result).c_str(), absl::bit_cast<void*>(gpu_dst), ROCMPointerToMemorySpaceString(gpu_dst).c_str(), ROCMPointerToDeviceString(gpu_dst).c_str(), absl::bit_cast<void*>(gpu_src), ROCMPointerToMemorySpaceString(gpu_src).c_str(), ROCMPointerToDeviceString(gpu_src).c_str(), ROCMPointersToCanAccessString(gpu_src, gpu_dst).c_str(), size, size);
VLOG(2) << "successfully enqueued async memcpy d2d of " << size << " bytes";
LOG(FATAL) << "impossible event flags: " << int(hipflags);
LOG(ERROR) << "could not retrieve ROCM device count: " << ToString(res);
LOG(ERROR) << "failed to query max grid dim x: " << ToString(res);
LOG(ERROR) << "failed to query max grid dim y: " << ToString(res);
LOG(ERROR) << "failed to query max grid dim z: " << ToString(res);
LOG(ERROR) << "failed to query driver version: " << ToString(res);
LOG(ERROR) << "failed to query device properties: " << ToString(res);
LOG(ERROR) << "failed to query ECC status: " << ToString(res);
LOG(ERROR) << "failed to query device memory info: " << ToString(res);
LOG(ERROR) << "failed to query total available memory: " << ToString(res);
LOG(ERROR) << "failed to query PCI bus id for device: " << ToString(res);
LOG(ERROR) << "failed to detect peer access capability: " << ToString(res);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/stream_executor/rocm/rocm_platform.cc
LOG(FATAL) << "not yet implemented: register ROCM trace listener";
LOG(FATAL) << "not yet implemented: unregister ROCM trace listener";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/stream_executor/rocm/rocm_rng.cc
LOG(ERROR) << "failed to create random number generator: " << ret;
LOG(ERROR) << "failed to set stream for random generation: " << ret;
LOG(ERROR) << "failed to do uniform generation of " << v->ElementCount() << " " << TypeString<T>() << "s at " << v->opaque() << ": " << ret;
LOG(ERROR) << "failed to do gaussian generation of " << v->ElementCount() << " floats at " << v->opaque() << ": " << ret;
LOG(ERROR) << "failed to set rng seed: " << ret;
LOG(ERROR) << "failed to reset rng position: " << ret;
LOG(ERROR) << "Attempting to initialize an instance of the hipRAND " << "support library with a non-ROCM StreamExecutor";
LOG(ERROR) << "Unable to register rocRAND factory: " << status.error_message();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/stream_executor/rocm/rocm_diagnostics.cc
VLOG(2) << "version string "" << value << "" made value " << DriverVersionToString(result);
LOG(INFO) << "retrieving ROCM diagnostic information for host: " << port::Hostname();
LOG(INFO) << "hostname: " << port::Hostname();
VLOG(1) << "LD_LIBRARY_PATH is: "" << library_path << """;
VLOG(1) << "could not open "" << piece << """;
VLOG(1) << piece << " :: " << entity->d_name;
LOG(INFO) << "librocm reported version is: " << rocm::DriverVersionStatusToString(dso_version);
LOG(INFO) << "kernel reported version is: " << rocm::DriverVersionStatusToString(kernel_version);
VLOG(1) << "found DLL info with name: " << info->dlpi_name;
VLOG(1) << "found DLL info with resolved path: " << resolved_path;
LOG(INFO) << "kernel version seems to match DSO: " << rocm::DriverVersionToString(kernel_version.ValueOrDie());
LOG(ERROR) << "kernel version " << rocm::DriverVersionStatusToString(kernel_version) << " does not match DSO version " << rocm::DriverVersionStatusToString(dso_version) << " -- cannot find working devices in this configuration";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/stream_executor/rocm/rocm_kernel.cc
LOG(FATAL) << "Unknown KernelCacheConfig" << static_cast<int32>(preferred_cache_config_);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/stream_executor/rocm/rocm_gpu_executor.cc
VLOG(3) << "No loaded HSACO module for " << gpu_binary;
VLOG(3) << "Found HSACO module " << module << " with refcount " << refcount;
VLOG(3) << "Unloading HSACO module " << module;
VLOG(3) << "Unloading kernel " << kernel << " : " << kernel->name();
VLOG(3) << "Kernel " << kernel << " : " << kernel->name() << " has never been loaded.";
VLOG(3) << "Kernel " << kernel << " : " << kernel->name() << " has loaded GPU code " << gpu_binary_it->second;
LOG(FATAL) << "Feature not supported on ROCM platform " "(FindOnDiskForComputeCapability)";
VLOG(2) << "found AMDGPU ISA version-specific file, using that: " << cc_specific;
VLOG(2) << "could not find AMDGPU ISA version-specific file at: " << cc_specific;
VLOG(2) << "getting function " << *kernelname << " from module " << module;
VLOG(2) << "*(arg.address): " << reinterpret_cast<void*>( *static_cast<const uint64_t*>(arg.address));
LOG(FATAL) << "Feature not supported on ROCM platform (CalculateOccupancy)";
LOG(FATAL) << "Feature not supported on ROCM platform (CompareOccupancy)";
LOG(FATAL) << "Feature not supported on ROCM platform (LoadModuleFromCuBin)";
LOG(FATAL) << "Feature not supported on ROCM platform (LoadModuleFromPtx)";
VLOG(3) << "Loaded HSACO " << static_cast<const void*>(hsaco) << " as module " << *module;
VLOG(3) << "HSACO " << static_cast<const void*>(hsaco) << " is already loaded as module " << *module;
LOG(WARNING) << "attempting to register null or zero-sized memory: " << location << "; size " << size;
VLOG(2) << "registering " << location << " size " << size;
VLOG(2) << "unregistering " << location;
VLOG(2) << "enqueueing memset8 operation onto stream " << stream << " at location " << location << " with size " << size << " and pattern " << std::hex << pattern;
VLOG(2) << "enqueueing memset32 operation onto stream " << stream << " at location " << location << " with size " << size << " and pattern " << std::hex << pattern;
LOG(WARNING) << "Host callback failed: " << s;
LOG(ERROR) << "Deallocating stream with pending work";
LOG(ERROR) << "failed to record completion event; " "therefore, failed to create inter-stream dependency";
LOG(ERROR) << "Unable to retrieve BLAS factory: " << status.status().error_message();
LOG(ERROR) << "Unable to retrieve DNN factory: " << status.status().error_message();
LOG(ERROR) << "Unable to retrieve FFT factory: " << status.status().error_message();
LOG(ERROR) << "Unable to retrieve RNG factory: " << status.status().error_message();
LOG(FATAL) << "Invalid shared memory configuration returned: " << rocm_config.ValueOrDie();
LOG(FATAL) << "Invalid shared memory configuration specified: " << static_cast<int>(config);
LOG(INFO) << "Falied to find symbol in any modules: " << symbol_name;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/stream_executor/rocm/rocm_event.cc
LOG(ERROR) << "Error polling for event status: " << status.status().error_message();
LOG(INFO) << "Error condition returned for event status: " << status.ValueOrDie();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/stream_executor/rocm/rocm_dnn.cc
LOG(FATAL) << "call to miopenCreateFusionPlan failed: " << ToString(status);
LOG(FATAL) << "call to miopenDestroyFusionPlan failed: " << ToString(status);
LOG(FATAL) << "Unsupported MIOpen convolution forward algorithm: " << algorithm.algo_id();
LOG(FATAL) << "Unsupported MIOpen convolution backward algorithm for data: " << algorithm.algo_id();
LOG(FATAL) << "Unsupported MIOpen convolution backward algorithm for filter: " << algorithm.algo_id();
LOG(ERROR) << "could not create miopen handle: " << ToString(status);
LOG(ERROR) << "error retrieving driver version: " << rocm::DriverVersionStatusToString(result);
LOG(INFO) << "possibly insufficient driver version: " << rocm::DriverVersionToString(version);
LOG(FATAL) << "could not create miopen tensor descriptor: " << ToString(status);
LOG(FATAL) << "could not convert BatchDescriptor " << batch_descriptor.ToString() << " to miopen tensor descriptor: " << ToString(status);
LOG(FATAL) << "Unsupported tensor format " << DataLayoutString(batch_descriptor.layout());
LOG(ERROR) << "could not destroy miopen tensor descriptor: " << ToString(status);
LOG(FATAL) << "could not create miopen filter descriptor: " << ToString(status);
LOG(FATAL) << "could not set miopen filter descriptor: " << ToString(status);
LOG(ERROR) << "could not destroy miopen filter descriptor: " << ToString(status);
LOG(FATAL) << "could not create miopen convolution descriptor: " << ToString(status);
LOG(ERROR) << "TensorFlow padding alignment is not supported.";
LOG(FATAL) << "could not set miopen convolution descriptor: " << ToString(status);
VLOG(2) << "Requesting grouped convolution: " << convolution_descriptor.group_count();
LOG(FATAL) << "could not set miopen convolution group count: " << ToString(status);
LOG(ERROR) << "could not destroy miopen convolution descriptor: " << ToString(status);
LOG(FATAL) << "could not create miopen pooling descriptor: " << ToString(status);
LOG(FATAL) << "miopen requires pooling dimensions be 2" << ToString(status);
LOG(FATAL) << "could not set miopen pooling descriptor: " << ToString(status);
LOG(ERROR) << "could not destroy miopen pooling descriptor: " << ToString(status);
LOG(FATAL) << "could not create miopen LRN descriptor: " << ToString(status);
LOG(FATAL) << "could not set miopen LRN descriptor: " << ToString(status);
LOG(ERROR) << "could not destroy miopen LRN descriptor: " << ToString(status);
LOG(FATAL) << "call to miopenCreateActivationDescriptor failed: " << ToString(status);
LOG(FATAL) << "Activation mode (" << dnn::ActivationModeString(activation_mode) << ") not yet implemented";
LOG(FATAL) << "call to miopenSetActivationDescriptor failed: " << ToString(status);
LOG(FATAL) << "call to miopenDestroyActivationDescriptor failed: " << ToString(status);
LOG(FATAL) << "call to miopenCreateOperatorArgs failed: " << ToString(status);
LOG(FATAL) << "call to miopenDestroyoperatorArgs failed: " << ToString(status);
LOG(FATAL) << "call to miopenExecuteFusionPlan failed: " << ToString(status);
LOG(FATAL) << "call to miopenFusionPlanGetOp failed: " << ToString(status);
LOG(FATAL) << "call to miopenSetOpArgsConvForward failed: " << ToString(status);
LOG(FATAL) << "call to miopenFusionPlanGetOp failed: " << ToString(status);
LOG(FATAL) << "call to miopenSetOpArgsBiasForward failed: " << ToString(status);
LOG(FATAL) << "call to miopenFusionPlanGetOp failed: " << ToString(status);
LOG(FATAL) << "call to miopenSetOpArgsBatchNormInference failed: " << ToString(status);
LOG(FATAL) << "call to miopenFusionPlanGetOp failed: " << ToString(status);
LOG(FATAL) << "call to miopenSetOpArgsBatchNormForward failed: " << ToString(status);
LOG(FATAL) << "call to miopenFusionPlanGetOp failed: " << ToString(status);
LOG(FATAL) << "call to miopenSetOpArgsBatchNormBackward failed: " << ToString(status);
LOG(FATAL) << "call to miopenFusionPlanGetOp failed: " << ToString(status);
LOG(FATAL) << "call to miopenSetOpArgsActivForward failed: " << ToString(status);
LOG(FATAL) << "call to miopenFusionPlanGetOp failed: " << ToString(status);
LOG(FATAL) << "call to miopenSetOpArgsActivBackward failed: " << ToString(status);
LOG(FATAL) << "call to miopenCreateOpConvForward failed: " << ToString(status);
LOG(FATAL) << "call to miopenCreateOpBiasForward failed: " << ToString(status);
LOG(FATAL) << "call to miopenCreateOpActivationForward failed: " << ToString(status);
VLOG(2) << "call to miopenCompileFusionPlan (CBA) failed: " << ToString(status);
VLOG(2) << "Fusion Plan compile succedded (CBA) ";
LOG(FATAL) << "call to miopenCreateOpBatchNormInference failed: " << ToString(status);
LOG(FATAL) << "call to miopenCreateOpActivationForward failed: " << ToString(status);
VLOG(2) << "call to miopenCompileFusionPlan (BnA inference) failed: " << ToString(status);
VLOG(2) << "Fusion Plan compile succedded (BnA inference) ";
LOG(FATAL) << "call to miopenCreateOpBatchNormForward failed: " << ToString(status);
LOG(FATAL) << "call to miopenCreateOpActivationForward failed: " << ToString(status);
VLOG(2) << "call to miopenCompileFusionPlan (BnA forward) failed: " << ToString(status);
VLOG(2) << "Fusion Plan compile succedded (BnA forward) ";
LOG(FATAL) << "call to miopenCreateOpBatchNormBackward failed: " << ToString(status);
LOG(FATAL) << "call to miopenCreateOpActivationBackward failed: " << ToString(status);
VLOG(2) << "call to miopenCompileFusionPlan (BnA backward) failed: " << ToString(status);
VLOG(2) << "Fusion Plan compile succedded (BnA backward) ";
LOG(FATAL) << "Invalid DNN data type: " << static_cast<int>(data_type);
LOG(FATAL) << "Invalid RNN input mode: " << static_cast<int>(input_mode);
LOG(FATAL) << "Invalid RNN direction mode: " << static_cast<int>(direction_mode);
LOG(FATAL) << "Invalid RNN Mode: " << static_cast<int>(rnn_mode);
LOG(FATAL) << "Invalid DNN data type: " << static_cast<int>(data_type);
LOG(FATAL) << "Invalid DNN data type: " << static_cast<int>(data_type);
LOG(FATAL) << "Invalid RNN Mode: " << static_cast<int>(rnn_mode);
LOG(ERROR) << error_msg;
LOG(ERROR) << "Invalid input_h shape";
LOG(ERROR) << "Invalid input_c shape";
LOG(ERROR) << "Invalid output shape";
LOG(ERROR) << "Invalid output_h shape";
LOG(ERROR) << "Invalid output_h shape";
LOG(ERROR) << "Unable to check RNN param size: " << ToString(status);
LOG(ERROR) << "Unable to query workspace size: " << ToString(status);
LOG(ERROR) << "Failed to allocate RNN workspace";
LOG(ERROR) << "Invalid parameters for RNN Model";
LOG(ERROR) << "Invalid parameters";
LOG(ERROR) << "Unable to create rnn workspace";
LOG(ERROR) << "Unable to query reserve space size: " << ToString(status);
LOG(ERROR) << "Fail to allocate RNN reserve space";
LOG(ERROR) << "Failed to call miopenRNNForwardInference: " << ToString(status);
LOG(ERROR) << "Failed to call miopenRNNForwardTraining" << ToString(status);
LOG(ERROR) << "Invalid parameters for RNN Model";
LOG(ERROR) << "Invalid parameters";
LOG(ERROR) << "Unable to create rnn workspace";
LOG(ERROR) << "Failed to call miopenRNNBackwardData: " << ToString(status);
LOG(ERROR) << "Failed to call miopenRNNBackwardWeights: " << ToString(status);
LOG(FATAL) << "call to miopenCreateCTCLossDescriptor failed: " << ToString(status);
LOG(FATAL) << "call to miopenSetCTCLossDescriptor failed: " << ToString(status);
LOG(FATAL) << "call to miopenDestroyCTCLossDescriptor failed: " << ToString(status);
LOG(FATAL) << "call to miopenDestroyCTCLossDescriptor failed: " << ToString(status);
LOG(ERROR) << "Failed to allocate scratch memory - " << scratch_or.status().error_message() << "" << "You can set the env var TF_CUDNN_WORKSPACE_LIMIT_IN_MB to a " "larger number (e.g. 8192) to increase the max memory limit." << "Increasing the max memory limit might help resolve this " "error";
LOG(FATAL) << "call to miopenCTCLoss failed: " << ToString(status);
LOG(ERROR) << "miopen does not support double type RNN fwd yet";
LOG(ERROR) << "miopen does not support half type RNN bwd yet";
VLOG(2) << "miopen...GetSolutionWorkspaceSize returned " << scratch_memory_size << " for solution_id " << solution_id;
LOG(ERROR) << "Failed to allocate scratch memory - " << allocated.status().error_message() << "" << "You can set the env var TF_CUDNN_WORKSPACE_LIMIT_IN_MB to a " "larger number (e.g. 8192) to increase the max memory limit." << "Increasing the max memory limit might help resolve this " "error";
LOG(FATAL) << "Failed to transform the data layout.";
LOG(FATAL) << "call to miopenConvolutionForwardGetSolutionCount failed: " << ToString(status);
LOG(FATAL) << "call to miopenConvolutionBackwardDataGetSolutionCount " "failed: " << ToString(status);
LOG(FATAL) << "call to miopenConvolutionBackwardWeightsGetSolutionCount " "failed: " << ToString(status);
LOG(FATAL) << "Unexpected convolution kind " << static_cast<int>(kind);
VLOG(kImmediateModeVlogLevel) << "Number of conv solutions max: " << maxSolutionCount;
LOG(FATAL) << "call to miopenConvolutionForwardGetSolution failed: " << ToString(status);
VLOG(kImmediateModeVlogLevel) << "Number of conv solutions actual: " << solutionCount;
VLOG(kImmediateModeVlogLevel) << "Best Solution (id, algo) = " << best_solution.solution_id << ", " << ToString(best_solution.algorithm);
LOG(FATAL) << "call to miopenConvolutionForwardCompileSolution " "failed: " << ToString(status);
VLOG(kImmediateModeVlogLevel) << "solution " << i << " (time, mem, id, algo) = " << solution.time << ", " << solution.workspace_size << ", " << solution.solution_id << ", " << ToString(solution.algorithm);
LOG(FATAL) << "call to miopenConvolutionForwardCompileSolution failed: " << ToString(status);
LOG(FATAL) << "call to miopenConvolutionBackwardDataGetSolution failed: " << ToString(status);
VLOG(kImmediateModeVlogLevel) << "Number of conv solutions actual: " << solutionCount;
VLOG(kImmediateModeVlogLevel) << "Best Solution (id, algo) = " << best_solution.solution_id << ", " << ToString(best_solution.algorithm);
LOG(FATAL) << "call to miopenConvolutionBackwardDataCompileSolution " "failed: " << ToString(status);
VLOG(kImmediateModeVlogLevel) << "solution " << i << " (time, mem, id, algo) = " << solution.time << ", " << solution.workspace_size << ", " << solution.solution_id << ", " << ToString(solution.algorithm);
LOG(FATAL) << " call to miopenConvolutionBackwardDataCompileSolution " "failed: " << ToString(status);
LOG(FATAL) << "call to miopenConvolutionBackwardWeightsGetSolution failed: " << ToString(status);
VLOG(kImmediateModeVlogLevel) << "Number of conv solutions actual: " << solutionCount;
VLOG(kImmediateModeVlogLevel) << "Best Solution (id, algo) = " << best_solution.solution_id << ", " << ToString(best_solution.algorithm);
LOG(FATAL) << "call to miopenConvolutionBackwardWeightsCompileSolution " "failed: " << ToString(status);
VLOG(kImmediateModeVlogLevel) << "solution " << i << " (time, mem, id, algo) = " << solution.time << ", " << solution.workspace_size << ", " << solution.solution_id << ", " << ToString(solution.algorithm);
LOG(FATAL) << "call to miopenConvolutionBackwardWeightsCompileSolution " "failed: " << ToString(status);
LOG(FATAL) << "Unexpected convolution kind " << static_cast<int>(kind);
LOG(ERROR) << "failed to enqueue forward batch normalization on stream: " << ToString(status);
LOG(ERROR) << "failed to enqueue backward batch normalization on stream: " << ToString(status);
LOG(ERROR) << "fused convolve not implemented yet";
LOG(ERROR) << "fused convolve not implemented yet";
LOG(ERROR) << "fused convolve not implemented yet";
LOG(ERROR) << "fused convolve not implemented yet";
LOG(ERROR) << "transform tensor not implemented yet";
LOG(FATAL) << "failed to enqueue backward convolution on stream: " << ToString(status);
LOG(ERROR) << "miopen does not support double bwd bias yet";
LOG(ERROR) << "MatMul input and output dimensions are not compatible.";
LOG(ERROR) << "Unsupported MatMul input layout.";
LOG(ERROR) << "Unsupported MatMul output layout.";
LOG(ERROR) << "Unsupported MatMul output layout.";
LOG(ERROR) << "stream " << stream << " could not enqueue a tensor copy as part of bias addition.";
LOG(ERROR) << "stream " << stream << " could not enqueue bias addition.";
LOG(ERROR) << "miopen does not support activation yet";
LOG(ERROR) << "miopen does not support pooling for dobule type yet";
LOG(ERROR) << "failed to enqueue forward pooling on stream: " << ToString(status);
LOG(ERROR) << "failed to enqueue forward pooling on stream: " << ToString(status);
LOG(ERROR) << "miopen does not support backward pooling on double type yet";
LOG(ERROR) << "failed to obtain workspace size for backward pooling on stream: " << ToString(status);
LOG(ERROR) << "Failed to allocate backward pooling workspace";
LOG(ERROR) << "Failed to allocate backward pooling workspace";
LOG(ERROR) << "Failed to calculate tensor size to chain forward and " "backward pooling";
LOG(ERROR) << "failed to enqueue forward pooling (before backward) on stream: " << ToString(status);
LOG(ERROR) << "failed to enqueue backward pooling on stream: " << ToString(status);
LOG(ERROR) << "failed to obtain workspace size for backward pooling on stream: " << ToString(status);
LOG(ERROR) << "Failed to allocate backward pooling workspace";
LOG(ERROR) << "Failed to allocate backward pooling workspace";
LOG(ERROR) << "Failed to calculate tensor size to chain forward and " "backward pooling";
LOG(ERROR) << "failed to enqueue forward pooling (before backward) on stream: " << ToString(status);
LOG(ERROR) << "failed to enqueue backward pooling on stream: " << ToString(status);
LOG(ERROR) << "MIOpen LRN does not support wrap-around mode";
LOG(ERROR) << "MIOpen LRN does not support segmentation";
LOG(ERROR) << "failed to run miopenLRNForward";
LOG(ERROR) << "MIOpen LRN does not support wrap-around mode";
LOG(ERROR) << "MIOpen LRN does not support segmentation";
LOG(ERROR) << "failed to obtain workspace size for miopenLRNBackward";
LOG(ERROR) << "Failed to allocate backward pooling workspace";
LOG(ERROR) << "Failed to allocate tensor to chain forward and backward LRN";
LOG(ERROR) << "Failed to calculate tensor size to chain forward and " "backward LRN";
LOG(ERROR) << "failed to run miopenLRNForward";
LOG(ERROR) << "failed to run miopenLRNBackward";
LOG(ERROR) << "MIOpenSupport::DoDepthConcatenate currently only " "supports the kBatchDepthYX layout.";
LOG(ERROR) << "BlockHostUntilDone failed: " << block_status;
LOG(INFO) << output_dimensions.ElementCount() << ' ' << batch << ' ' << yx << ' ' << depth;
LOG(FATAL) << "not yet implemented";
LOG(FATAL) << "not yet implemented";
LOG(FATAL) << "not yet implemented";
LOG(ERROR) << "quantized memcpy not supported by MIOpen";
LOG(ERROR) << "quantized memcpy not supported by MIOpen";
LOG(ERROR) << "could not get output tensor for convolution: " << ToString(status);
LOG(FATAL) << "failed to enqueue fused-convolution on stream: " << ToString(status);
LOG(FATAL) << "failed to enqueue fused-convolution on stream: " << ToString(status);
LOG(FATAL) << "failed to enqueue fused-convolution on stream: " << ToString(status);
LOG(FATAL) << "failed to enqueue fused-convolution on stream: " << ToString(status);
LOG(ERROR) << "Attempting to initialize an instance of the MIOpen " << "support library with a non-ROCM StreamExecutor";
LOG(ERROR) << "Unable to register MIOpen factory: " << status.error_message();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/stream_executor/rocm/rocm_blas.cc
LOG(ERROR) << "failed to create rocBLAS handle: " << ToString(ret);
LOG(ERROR) << "failed to set stream for rocBLAS calls: " << ToString(ret);
LOG(FATAL) << "Invalid value of blas::Transpose.";
LOG(FATAL) << "Invalid value of blas::UpperLower.";
LOG(FATAL) << "Invalid value of blas::Diagonal.";
LOG(FATAL) << "Invalid value of blas::Side.";
LOG(ERROR) << "failed to run ROCBLAS routine " << rocblas_func.kName << ": " << ToString(ret);
LOG(ERROR) << "rocBLAS does not currently support the ASUM operation " << "for the "complex<float>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the ASUM operation " << "for the "complex<double>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the AXPY operation " << "for the "complex<float>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the AXPY operation " << "for the "complex<double>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the COPY operation " << "for the "complex<float>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the COPY operation " << "for the "complex<double>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the DOT operation " << "for the "complex<float>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the DOT operation " << "for the "complex<double>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the DOT operation " << "for the "complex<float>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the DOT operation " << "for the "complex<double>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the NRM2 operation " << "for the "complex<float>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the NRM2 operation " << "for the "complex<double>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the ROT operation " << "for the "float" datatype";
LOG(ERROR) << "rocBLAS does not currently support the ROT operation " << "for the "double" datatype";
LOG(ERROR) << "rocBLAS does not currently support the ROT operation " << "for the "complex<float>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the ROT operation " << "for the "complex<double>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the ROTG operation " << "for the "float" datatype";
LOG(ERROR) << "rocBLAS does not currently support the ROTG operation " << "for the "double" datatype";
LOG(ERROR) << "rocBLAS does not currently support the ROTG operation " << "for the "complex<float>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the ROTG operation " << "for the "complex<double>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the ROTM operation " << "for the "float" datatype";
LOG(ERROR) << "rocBLAS does not currently support the ROTM operation " << "for the "double" datatype";
LOG(ERROR) << "rocBLAS does not currently support the ROTMG operation " << "for the "float" datatype";
LOG(ERROR) << "rocBLAS does not currently support the ROTMG operation " << "for the "double" datatype";
LOG(ERROR) << "rocBLAS does not currently support the SCAL operation " << "for the "complex<float>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the SCAL operation " << "for the "complex<double>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the SCAL operation " << "for the "complex<float>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the SCAL operation " << "for the "complex<double>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the SWAP operation " << "for the "complex<float>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the SWAP operation " << "for the "complex<double>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the AMAX operation " << "for the "complex<float>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the AMAX operation " << "for the "complex<double>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the AMIN operation " << "for the "complex<float>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the AMIN operation " << "for the "complex<double>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the GBMV operation " << "for the "float" datatype";
LOG(ERROR) << "rocBLAS does not currently support the GBMV operation " << "for the "double" datatype";
LOG(ERROR) << "rocBLAS does not currently support the GBMV operation " << "for the "complex<float>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the GBMV operation " << "for the "complex<double>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the GEMV operation " << "for the "complex<float>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the GEMV operation " << "for the "complex<double>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the GER operation " << "for the "complex<float>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the GER operation " << "for the "complex<double>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the GERU operation " << "for the "complex<float>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the GERU operation " << "for the "complex<double>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the HBMV operation " << "for the "complex<float>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the HBMV operation " << "for the "complex<double>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the HEMV operation " << "for the "complex<float>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the HEMV operation " << "for the "complex<double>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the HER operation " << "for the "complex<float>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the HER operation " << "for the "complex<double>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the HER2 operation " << "for the "complex<float>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the HER2 operation " << "for the "complex<double>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the HPMV operation " << "for the "complex<float>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the HPMV operation " << "for the "complex<double>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the HPR operation " << "for the "complex<float>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the HPR operation " << "for the "complex<double>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the HPR2 operation " << "for the "complex<float>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the HPR2 operation " << "for the "complex<double>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the SBMV operation " << "for the "complex<float>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the SBMV operation " << "for the "complex<double>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the SPMV operation " << "for the "float" datatype";
LOG(ERROR) << "rocBLAS does not currently support the SPMV operation " << "for the "double" datatype";
LOG(ERROR) << "rocBLAS does not currently support the SPR operation " << "for the "float" datatype";
LOG(ERROR) << "rocBLAS does not currently support the SPR operation " << "for the "double" datatype";
LOG(ERROR) << "rocBLAS does not currently support the SPR2 operation " << "for the "float" datatype";
LOG(ERROR) << "rocBLAS does not currently support the SPR2 operation " << "for the "double" datatype";
LOG(ERROR) << "rocBLAS does not currently support the SYMV operation " << "for the "float" datatype";
LOG(ERROR) << "rocBLAS does not currently support the SYMV operation " << "for the "double" datatype";
LOG(ERROR) << "rocBLAS does not currently support the SYR2 operation " << "for the "float" datatype";
LOG(ERROR) << "rocBLAS does not currently support the SYR2 operation " << "for the "double" datatype";
LOG(ERROR) << "rocBLAS does not currently support the TBMV operation " << "for the "float" datatype";
LOG(ERROR) << "rocBLAS does not currently support the TBMV operation " << "for the "double" datatype";
LOG(ERROR) << "rocBLAS does not currently support the TBMV operation " << "for the "complex<float>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the TBMV operation " << "for the "complex<double>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the TBSV operation " << "for the "float" datatype";
LOG(ERROR) << "rocBLAS does not currently support the TBSV operation " << "for the "double" datatype";
LOG(ERROR) << "rocBLAS does not currently support the TBSV operation " << "for the "complex<float>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the TBSV operation " << "for the "complex<double>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the TPMV operation " << "for the "float" datatype";
LOG(ERROR) << "rocBLAS does not currently support the TPMV operation " << "for the "double" datatype";
LOG(ERROR) << "rocBLAS does not currently support the TPMV operation " << "for the "complex<float>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the TPMV operation " << "for the "complex<double>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the TPSV operation " << "for the "float" datatype";
LOG(ERROR) << "rocBLAS does not currently support the TPSV operation " << "for the "double" datatype";
LOG(ERROR) << "rocBLAS does not currently support the TPSV operation " << "for the "complex<float>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the TPSV operation " << "for the "complex<double>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the TRMV operation " << "for the "float" datatype";
LOG(ERROR) << "rocBLAS does not currently support the TRMV operation " << "for the "double" datatype";
LOG(ERROR) << "rocBLAS does not currently support the TRMV operation " << "for the "complex<float>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the TRMV operation " << "for the "complex<double>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the TRSV operation " << "for the "float" datatype";
LOG(ERROR) << "rocBLAS does not currently support the TRSV operation " << "for the "double" datatype";
LOG(ERROR) << "rocBLAS does not currently support the TRSV operation " << "for the "complex<float>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the TRSV operation " << "for the "complex<double>" datatype";
VLOG(1) << absl::StreamFormat( "doing rocBLAS SGEMM: at=%d bt=%d m=%u n=%u " "k=%llu alpha=%f a=%p lda=%d b=%p ldb=%d beta=%f " "c=%p ldc=%d", static_cast<int>(transa), static_cast<int>(transb), m, n, k, alpha, a.opaque(), lda, b.opaque(), ldb, beta, c->opaque(), ldc);
LOG(WARNING) << "GEMM lda was smaller than m (no transpose case); " "precondition violation";
LOG(WARNING) << "GEMM lda (" << lda << ") was smaller than k (" << k << ") (transpose case); precondition violation";
LOG(WARNING) << "GEMM ldb (" << ldb << ") was smaller than k (" << k << ") (no transpose case); precondition violation";
LOG(WARNING) << "GEMM ldb was smaller than n (transpose case); " "precondition violation";
VLOG(1) << absl::StreamFormat( "doing rocBLAS SGEMM: at=%d bt=%d m=%u n=%u " "k=%llu alpha=%f a=%p lda=%d b=%p ldb=%d beta=%f " "c=%p ldc=%d", static_cast<int>(transa), static_cast<int>(transb), m, n, k, alpha, a.opaque(), lda, b.opaque(), ldb, beta, c->opaque(), ldc);
LOG(WARNING) << "GEMM lda was smaller than m (no transpose case); " "precondition violation";
LOG(WARNING) << "GEMM lda (" << lda << ") was smaller than k (" << k << ") (transpose case); precondition violation";
LOG(WARNING) << "GEMM ldb (" << ldb << ") was smaller than k (" << k << ") (no transpose case); precondition violation";
LOG(WARNING) << "GEMM ldb was smaller than n (transpose case); " "precondition violation";
LOG(ERROR) << "rocBLAS does not currently support the GEMM operation " << "for the "complex<float>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the GEMM operation " << "for the "complex<double>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the GEMMwithAlgorithm operation " << "for the "int8" datatype";
LOG(ERROR) << "rocBLAS does not currently support the GEMMwithAlgorithm operation " << "for the "half" datatype";
LOG(ERROR) << "rocBLAS does not currently support the GEMMwithAlgorithm operation " << "for the "float" datatype";
LOG(ERROR) << "rocBLAS does not currently support the GEMMwithAlgorithm operation " << "for the "double" datatype";
LOG(ERROR) << "rocBLAS does not currently support the GEMMwithAlgorithm operation " << "for the "complex<float>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the GEMMwithAlgorithm operation " << "for the "complex<double>" datatype";
LOG(ERROR) << status;
LOG(ERROR) << status;
LOG(ERROR) << status;
LOG(ERROR) << "rocBLAS does not currently support the GEMMBatched operation " << "for the "complex<float>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the GEMMBatched operation " << "for the "complex<double>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the HEMM operation " << "for the "complex<float>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the HEMM operation " << "for the "complex<double>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the HERK operation " << "for the "complex<float>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the HERK operation " << "for the "complex<double>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the HER2K operation " << "for the "complex<float>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the HER2K operation " << "for the "complex<double>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the SYMM operation " << "for the "float" datatype";
LOG(ERROR) << "rocBLAS does not currently support the SYMM operation " << "for the "double" datatype";
LOG(ERROR) << "rocBLAS does not currently support the SYMM operation " << "for the "complex<float>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the SYMM operation " << "for the "complex<double>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the SYRK operation " << "for the "float" datatype";
LOG(ERROR) << "rocBLAS does not currently support the SYRK operation " << "for the "double" datatype";
LOG(ERROR) << "rocBLAS does not currently support the SYRK operation " << "for the "complex<float>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the SYRK operation " << "for the "complex<double>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the SYR2K operation " << "for the "float" datatype";
LOG(ERROR) << "rocBLAS does not currently support the SYR2K operation " << "for the "double" datatype";
LOG(ERROR) << "rocBLAS does not currently support the SYR2K operation " << "for the "complex<float>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the SYR2K operation " << "for the "complex<double>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the TRMM operation " << "for the "float" datatype";
LOG(ERROR) << "rocBLAS does not currently support the TRMM operation " << "for the "double" datatype";
LOG(ERROR) << "rocBLAS does not currently support the TRMM operation " << "for the "complex<float>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the TRMM operation " << "for the "complex<double>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the TRSM operation " << "for the "complex<float>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the TRSM operation " << "for the "complex<double>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the " "DoBlasGemmStridedBatched operation " << "for the "complex<float>" datatype";
LOG(ERROR) << "rocBLAS does not currently support the " "DoBlasGemmStridedBatched operation " << "for the "complex<double>" datatype";
LOG(ERROR) << "Attempting to initialize an instance of the " "rocBLAS " << "support library with a non-ROCM StreamExecutor";
LOG(ERROR) << "Unable to register rocBLAS factory: " << status.error_message();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/status_macros.cc
LOG(INFO) << status << stack_trace;
LOG(WARNING) << status << stack_trace;
LOG(ERROR) << status << stack_trace;
LOG(FATAL) << status << stack_trace;
LOG(FATAL) << "Unknown LOG severity " << log_severity;
LOG(ERROR) << "Cannot create error with status OK";
LOG(ERROR) << "MakeErrorStream destructed without getting Status: " << file_ << ":" << line_ << " " << stream_.str();
LOG(ERROR) << "MakeErrorStream got Status more than once: " << file_ << ":" << line_ << " " << stream_.str();
LOG(ERROR) << "MakeErrorStream shift called after getting Status: " << file_ << ":" << line_ << " " << stream_.str();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/literal_comparison.cc
LOG(FATAL) << "Unsupported primitive type: " << PrimitiveType_Name(expected.shape().element_type());
LOG(FATAL) << "Unsupported primitive type in near comparator: " << PrimitiveType_Name(expected.shape().element_type()) << ". Must be floating-point type.";
VLOG(1) << "expected:";
XLA_VLOG_LINES(1, expected.ToString());
VLOG(1) << "actual:";
XLA_VLOG_LINES(1, actual.ToString());
VLOG(1) << "Expected literal:";
XLA_VLOG_LINES(1, expected.ToString());
VLOG(1) << "Actual literal:";
XLA_VLOG_LINES(1, actual.ToString());

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/packed_literal_reader.cc
VLOG(3) << "reading shape from file: " << ShapeUtil::HumanString(shape) << " layout: " << (layout == nullptr ? "<none>" : layout->ToString());
VLOG(3) << "read shape from file: " << ShapeUtil::HumanString(shape);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/shape.cc
LOG(ERROR) << "Malformed shape proto: number of is_dynamic_dimension " "fields does not match number of dimension fields";
LOG(WARNING) << "Malformed shape proto: is_dynamic_dimension is empty";
VLOG(3) << "CompareShapes: lhs element type != rhs element type";
VLOG(3) << "CompareShapes: lhs dimensions != rhs dimensions";
VLOG(3) << "CompareShapes: lhs layout format != rhs layout format";
VLOG(3) << "CompareShapes: lhs layout != rhs layout";
VLOG(3) << "CompareShapes: lhs and rhs have different dynamic dimensions.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/layout_util.cc
LOG(FATAL) << "Tile dimension size needs to be minimum int64 value if " "it's negative. Value is " << dim;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/primitive_util.cc
LOG(FATAL) << "Not a floating data type " << type;
LOG(FATAL) << "TUPLE is an invalid type for BitWidth";
LOG(FATAL) << "OPAQUE_TYPE is an invalid type for BitWidth";
LOG(FATAL) << "Unhandled primitive type " << type;
LOG(FATAL) << "Primitive type is not complex: " << PrimitiveType_Name(complex_type);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/parse_flags_from_env.cc
VLOG(1) << "For env var " << envvar << " found arguments:";
VLOG(1) << " argv[" << i << "] = " << env_argv->argv[i];
LOG(QFATAL) << "Unknown flag" << (unknown_flags.size() > 1 ? "s" : "") << " in " << envvar << ": " << absl::StrJoin(unknown_flags, " ") << did_you_mean;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/literal_util.cc
LOG(FATAL) << "tuple element type cannot take on value of 0";
LOG(FATAL) << "opaque element type cannot take on value of 0";
LOG(FATAL) << "Unhandled primitive type " << primitive_type;
LOG(FATAL) << "tuple element type cannot take on value of 1";
LOG(FATAL) << "opaque element type cannot take on value of 1";
LOG(FATAL) << "Unhandled primitive type " << primitive_type;
LOG(FATAL) << "C64 element type has no minimum value";
LOG(FATAL) << "C128 element type has no minimum value";
LOG(FATAL) << "tuple element type has no minimum value";
LOG(FATAL) << "opaque element type has no minimum value";
LOG(FATAL) << "Unhandled primitive type " << primitive_type;
LOG(FATAL) << "tuple element type has no maximum value";
LOG(FATAL) << "opaque element type has no maximum value";
LOG(FATAL) << "Unhandled primitive type " << primitive_type;
LOG(FATAL) << "Unhandled primitive element type: " << PrimitiveType_Name(literal.shape().element_type());
LOG(FATAL) << "Unhandled primitive type " << literal.shape().element_type();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/debug_options_flags.cc
LOG(ERROR) << absl::StreamFormat( "Compiler fuel for "%s" was never consumed. This may be a typo in " "the --xla_fuel flag you passed.", pass);
LOG(ERROR) << absl::StreamFormat( "Illegal value for --xla_fuel. Saw %s, but expected token %s to " "have format X=INTEGER.", xla_fuel_value, kv);
LOG(ERROR) << absl::StreamFormat( "Illegal value for --xla_fuel. Saw %s, but expected token %s to be " "an integer.", xla_fuel_value, fuel_str);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/util.cc
VLOG(1) << status.ToString();
VLOG(2) << tensorflow::CurrentStackTrace();
LOG(INFO).AtLocation(file_, line_) << label_ << " time: " << tensorflow::strings::HumanReadableElapsedTime(secs) << " (cumulative: " << tensorflow::strings::HumanReadableElapsedTime(stats.cumulative_secs) << ", max: " << tensorflow::strings::HumanReadableElapsedTime(stats.max_secs) << ", #called: " << stats.times_called << ")";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/metric_table_report.cc
LOG(INFO) << "Writing report to log.";
LOG(INFO) << line;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/literal.cc
LOG(FATAL) << "not yet implemented: " << PrimitiveType_Name(result_shape.element_type());
LOG(FATAL) << PrimitiveType_Name(subshape.element_type());
LOG(FATAL) << "Invalid bitcast between types of different sizes.";}
LOG(FATAL) << "Unimplemented: LiteralBase::Piece::EqualElements for type " << PrimitiveType_Name(subshape().element_type());
LOG(FATAL) << "Input literal must be an array.";
LOG(FATAL) << "Unhandled primitive type " << PrimitiveType_Name(subshape().element_type());

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/shape_util.cc
VLOG(3) << "ShapeUtil::Equal differ: lhs = " << lhs.ShortDebugString() << ", rhs = " << rhs.ShortDebugString();
VLOG(3) << "ShapeUtil::EqualIgnoringElementType differ: lhs = " << lhs.ShortDebugString() << ", rhs = " << rhs.ShortDebugString();
VLOG(3) << "ShapeUtil::EqualIgnoringFpPrecision differ: lhs = " << lhs.ShortDebugString() << ", rhs = " << rhs.ShortDebugString();
LOG(FATAL) << "Unhandled element type " << shape.element_type();
LOG(FATAL) << PrimitiveType_Name(primitive_type) << " primitive type has no definitive size";
LOG(FATAL) << "Unhandled primitive type " << primitive_type;
LOG(FATAL) << PrimitiveType_Name(shape.element_type()) << " primitive type has no definitive size";
VLOG(3) << "Validating shape size: " << ShapeUtil::HumanString(shape);
VLOG(3) << "Shape size is valid: " << shape_size;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/tools/hex_floats_to_packed_literal.cc
LOG(QFATAL) << usage;
LOG(QFATAL) << "--input_file is required";
LOG(QFATAL) << "--output_file is required";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/tools/show_literal.cc
LOG(QFATAL) << "Usage: " << argv[0] << " <path-to-serialized-literal-proto>";
LOG(INFO) << "literal: " << literal_proto.ShortDebugString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/tools/replay_computation.cc
LOG(ERROR) << " " << ShapeUtil::HumanString(Shape(infeed.shape())) << " " << infeed.name();
LOG(FATAL) << "No instruction with id " << id;
LOG(INFO) << "Not generating fake " << xfeed_name << " shape; model has no " << xfeed_name << "s.";
LOG(INFO) << "Generating fake " << xfeed_name << " with inferred shape: " << ShapeUtil::HumanString(*xfeed_shape);
LOG(ERROR) << "--generate_fake_" << xfeed_name << " only works if the model has 0 or 1 " << xfeed_name << " ops, but this model has " << xfeed_instrs.size() << " of them:";
LOG(FATAL) << "Can't run model with --generate_fake_infeed.";
LOG(ERROR) << "Model contains " << xfeed_instrs.size() << " " << xfeed_name << " instruction(s), but neither --generate_fake_" << xfeed_name << " nor --fake_" << xfeed_name << "_shape was specified. Execution will likely hang.";
VLOG(1) << "Received outfeed data of shape " << ShapeUtil::HumanStringWithLayout(*outfeed_shape);
LOG(INFO) << "***** Final run below ******";
LOG(INFO) << "Done executing in " << static_cast<double>(profile.compute_time_ns()) / 1e9 << "s: " << module.hlo().hlo_module().name();
LOG(ERROR) << "Encountered bad proto";
LOG(ERROR) << module.status();
LOG(ERROR) << maybe_snapshot.status();
LOG(INFO) << "Compiling " << snapshots.size() << " modules in parallel.";
LOG(INFO) << "Done compiling; now running the modules.";
LOG(ERROR) << "Compilation failed: " << executables[i].status() << ": " << snapshots[i].ShortDebugString();
LOG(ERROR) << "Running iteration " << i;
LOG(ERROR) << "iteration complete.";
LOG(QFATAL) << usage;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/tools/run_hlo_module_main.cc
LOG(QFATAL) << kUsageString;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/tools/interactive_graphviz.cc
LOG(FATAL) << "Can only specify one and only one of '--hlo_proto', " "'--hlo_snapshot', '--hlo_text' flags.";
LOG(ERROR) << "*****************************************" << "This is an interactive tool, but stdin is not a tty." << "*****************************************";
LOG(INFO) << "Compiling module for " << platform->Name();
LOG(QFATAL) << usage;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/tools/run_hlo_module.cc
XLA_VLOG_LINES(1, module->ToString());

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/tools/convert_computation.cc
LOG(QFATAL) << "unknown mode for computation conversion: " << mode;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/tools/show_text_literal.cc
LOG(QFATAL) << "Usage: " << argv[0] << " <path-to-serialized-literal-text>";
LOG(INFO) << "literal: " << literal;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/python/local_client.cc
VLOG(2) << "PyLocalBuffer::FromLiterals: shape: " << shape.ToString() << " device: " << device->DebugString();
VLOG(1) << "PyLocalExecutable " << name() << " device_assignment:" << device_assignment_->ToString();
VLOG(3) << "Non-local device: " << device_id;
VLOG(3) << "Replica " << replica << ", partition " << partition << " mapped to device ordinal for execution: " << device_ordinal;
VLOG(4) << "Argument " << i << " buffer: " << argument_buffers.back().ToString();
VLOG(1) << "Replica " << replica << " partition " << partition << " completed; ok=" << result_buffer_or_status.ok();
LOG(ERROR) << "Execution of replica " << replica << " failed: " << result_buffer_or_status.status();
VLOG(1) << "Executing computation " << name();
VLOG(1) << "Executing computation " << name() << "; num_replicas=" << num_replicas() << " num_partitions=" << num_partitions() << " num_local_devices=" << num_local_devices;
LOG(FATAL) << "Replicated computation launch failed, but not all replicas " "terminated. Aborting process to work around deadlock. Failure " "message (there may have been multiple failures, see the " "error log for all failures): " << first_failure_status.error_message();
VLOG(1) << "Replicated execution complete.";
VLOG(2) << "PyLocalExecutable::Compile got device_assignment:" << device_assignment->ToString();
VLOG(2) << "PyLocalExecutable::Compile using default device_assignment:" << device_assignment->ToString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/python/shared_device_buffer.cc
LOG(ERROR) << "Buffer deallocation failed: " << status;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/python/bfloat16.cc
LOG(FATAL) << "Invalid op type " << op;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/python/nvidia_gpu_device.cc
LOG(INFO) << "XLA backend allocating " << allocator_memory << " bytes on device " << device_ordinal << " for BFCAllocator.";
LOG(INFO) << "XLA backend will use up to " << allocator_memory << " bytes on device " << device_ordinal << " for BFCAllocator.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/python/local_device_state.cc
LOG(ERROR) << "Error when closing device: " << status;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/python/tpu_driver/recording_tpu_driver.cc
LOG(FATAL) << "Unable to open " << recording_path_ << " for appending. Error: " << file_status.ToString();
LOG(WARNING) << "The TPU driver has been shut down before all logging " "has been written.";
LOG(WARNING) << "Unable to write data to log file. File possibly " "corrupt. Error: " << data_status.ToString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/python/tpu_driver/grpc_tpu_driver.cc
VLOG(2) << "Adding request: " << req->DebugString();
LOG(ERROR) << status;
VLOG(1) << "Shutting down stream.";
LOG(ERROR) << "Resetting: " << e.first;
VLOG(1) << "Closing stream.";
VLOG(1) << "Waiting for writer.";
VLOG(1) << "Waiting for reader.";
VLOG(1) << "Received a status update: " << status << ", but cannot find GrpcEvent " << id;
VLOG(1) << "Received a second status update: " << status.error_message() << ", for GrpcEvent " << id << " already done with status: " << it->second.status.error_message();
VLOG(1) << "Response received for GrpcEvent " << id << ". " << status.ToString() << ". Firing " << it->second.callbacks.size() << " callbacks.";
VLOG(1) << "Sending request: " << EventId::FromInt(e->operation_id());
VLOG(2) << "Sending request: " << e->DebugString();
VLOG(2) << "Received response: " << resp.DebugString();
VLOG(1) << "Received response for: " << event_id;
VLOG(1) << "Copying: " << it->second.num_bytes << " to position " << it->second.dst;
LOG(ERROR) << "QuerySystemInfo request failed: " << status.error_code() << ": " << status.error_message() << ": " << status.error_details();
LOG(ERROR) << "Failed to reset the gRPC driver: " << status.error_code() << ": " << status.error_message() << ": " << status.error_details();
LOG(ERROR) << "Failed to open the gRPC driver: " << status.error_code() << ": " << status.error_message() << ": " << status.error_details();
LOG(INFO) << "Using local credentials for localhost: connection.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/python/tpu_driver/tpu_driver.cc
LOG(FATAL) << PrimitiveType_Name(primitive_type) << " primitive type has no definitive size";
LOG(FATAL) << "Unhandled primitive type " << primitive_type;
LOG(FATAL) << "Tuples are not supported at the moment.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/python/tpu_driver/direct_tpu_driver.cc
LOG(ERROR) << "Unable to serialize shape to array.";
LOG(FATAL) << "Unimplemented.";
LOG(FATAL) << "Unimplemented.";
LOG(FATAL) << "Unimplemented.";
LOG(FATAL) << "Unable to load shared library: " << dlerror();
LOG(ERROR) << "Unable to serialize HLO to array.";
LOG(FATAL) << "Request to use compile-time linked TPU library, but did not " << "link in appropriate library at compile time.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/python/tpu_driver/client/tpu_client.cc
LOG(INFO) << "Detected local device, host id: " << host_id_ << ". device id: " << device->id();
VLOG(2) << "Other devices, device id: " << device->id();
LOG(INFO) << "Creating " << local_devices_.size() << " TPU device(s).";
VLOG(1) << "PyTpuBuffer::FromLiterals: shape: " << tuple_shape.DebugString() << " device id: " << device_id;
VLOG(1) << "CopyToHostAsync:: host shape: " << host_value->value->shape().DebugString();
VLOG(1) << "Device to host transfer finished.";
VLOG(1) << "Host value done: " << host_value->status;
VLOG(1) << "Waiting for device to host transfer " << host_value.get();
VLOG(1) << "Host copy finished, status:" << host_value->status;
VLOG(1) << "PyTpuBuffer::AllocateBuffer: shape: " << shape.DebugString() << " device ordinal: " << device_id;
VLOG(1) << "PyTpuBuffer::CreateBuffer: shape: " << non_tuple_shape.DebugString() << " device id: " << device_id;
VLOG(1) << "DeviceAssignment. " << device_assignment_.ToString();
VLOG(3) << "Non-local device: " << device_id;
VLOG(3) << "Replica " << replica << ", partition " << partition << " mapped to device id for execution: " << device_id;
VLOG(1) << "Created output buffer: " << result_shape_.DebugString();
LOG(WARNING) << "TPU Execute is taking a long time. This might be due to a " "deadlock between multiple TPU cores or a very slow program.";
LOG(ERROR) << "Failed to execute program: " << status;
VLOG(1) << "Executing computation; num_replicas=" << num_replicas() << " num_partitions=" << num_partitions() << " num_local_devices=" << num_local_devices;
VLOG(1) << "Replicated execution complete.";
VLOG(1) << "Compile: " << computation.GetProgramShape().ValueOrDie().DebugString();
VLOG(1) << "Got result shape: " << result_layout.DebugString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/tests/filecheck.cc
LOG(WARNING) << "Tried to execute FileCheck at " << file_check_path;
LOG(WARNING) << "NOTE: FileCheck binary does not exist!";
LOG(WARNING) << "FileCheck error:" << standard_error;
LOG(INFO) << "FileCheck stderr:";
XLA_LOG_LINES(tensorflow::INFO, standard_error);
LOG(INFO) << "FileCheck input was:";
XLA_LOG_LINES(tensorflow::INFO, input);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/tests/verified_hlo_module.cc
LOG(ERROR) << "Contents of bad module:";
XLA_LOG_LINES(tensorflow::ERROR, ToString());

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/multi_output_fusion.cc
VLOG(10) << "Looking at instruction: " << instruction->name();
VLOG(10) << "Operand not profitable: " << operand->name();
VLOG(10) << "Operand profitable: " << operand->name();
VLOG(10) << "User: " << user->name();
VLOG(10) << "User is not fusible, or is the instruction itself: " << user->name();
VLOG(10) << "User is connected: " << user->name();
VLOG(10) << "User ID for user: " << user->name() << " is " << user_id << " which is higher than " << instruction_id;
VLOG(10) << "User not legal to fuse: " << user->name();
VLOG(10) << "User added to candidate list: " << user->name();
VLOG(1) << "Considering candidate profit_score=" << candidate.score << "instr1 = " << instr1->ToString() << "instr2 = " << instr2->ToString();
VLOG(1) << "Fuse!";
VLOG(2) << "Before multi_output_fusion:";
VLOG(2) << "instr1: " << instr1->ToString();
VLOG(2) << "" << instr1->fused_instructions_computation()->ToString( HloPrintOptions().set_indent_amount(1));
VLOG(2) << "instr2: " << instr2->ToString();
VLOG(2) << "" << instr2->fused_instructions_computation()->ToString( HloPrintOptions().set_indent_amount(1));
VLOG(2) << "After fusion, this: " << fusion->name() << "" << fusion->fused_instructions_computation()->ToString( HloPrintOptions().set_indent_amount(1));

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/algebraic_simplifier.cc
VLOG(10) << "trying transform [Abs(A) => A] " << abs->ToString() << " Abs operand is: " << abs_operand->ToString();
VLOG(10) << "trying transform [A + 0 => A]: " << add->ToString();
VLOG(10) << "trying transform [0 + A => A]: " << add->ToString();
VLOG(10) << "trying transform [Const + A => A + Const]";
VLOG(10) << "trying transform [(A + C1) + C2 => A + (C1 + C2)]";
VLOG(10) << "trying transform [A && True => A]: " << logical_and->ToString();
VLOG(10) << "trying transform [False && A => False]: " << logical_and->ToString();
VLOG(10) << "trying to replace " << concatenate->ToString() << " with " << replacement->ToString();
VLOG(10) << "trying transform [A - 0 => A]: " << sub->ToString();
VLOG(10) << "trying transform [A/1 => A]: " << divide->ToString();
VLOG(10) << "transform [exp(A)/exp(B) => exp(A-B)]: " << divide->ToString();
VLOG(10) << "transform [A/exp(B) => A*exp(-B)]: " << divide->ToString();
VLOG(10) << "transform [A/pow(B,C) => A*pow(B,-C)]: " << divide->ToString();
VLOG(10) << "DotOfGather: Can only optimize 2D, non-batch dot operations.";
VLOG(10) << "DotOfGather: Can only optimize dot(DS(ctA), ctB)) or " "dot(ctB, DS(ctA)), where the two constants have equal " "contracting dimensions.";
VLOG(10) << " Replaced dot " << dot->ToString() << " with new dot operation: " << dot_of_reorder_optimized->ToString();
VLOG(10) << "Replaced dot(concat(...), constant) with add(dot(..., " "constant)...)";
VLOG(10) << "Replaced dot(constA, gather(i, constB)) with " "gather(i, dot*(constA, constB))";
VLOG(10) << "trying transform [LHS*1 => LHS]: " << multiply->ToString();
VLOG(10) << "trying transform [1*RHS => RHS]: " << multiply->ToString();
VLOG(10) << "trying transform [(A * C1) * C2 => A * (C1 * C2)]";
VLOG(10) << "trying transform [rsqrt(B) * rsqrt(B) => 1/B] " << multiply->ToString();
VLOG(10) << "trying transform [A || True => True]: " << logical_or->ToString();
VLOG(10) << "trying transform [False || A => A]: " << logical_or->ToString();
VLOG(10) << "trying transform [ln(exp(A)) => A]: " << log->ToString();
VLOG(10) << "trying transform " << "[get_tuple_element(make_tuple({...,A_i,...}), i)] => A_i: " << get_tuple_element->ToString();
VLOG(10) << "transform broadcast(X) -> reshape(X) where " "n(broadcast(X)) == n(X)";
VLOG(10) << "transform broadcast(X) -> transpose(X) where " "n(broadcast(X)) == n(X)";
VLOG(10) << "transform permuting/subset of a scalar broadcast into " << "a single broadcast";
VLOG(10) << "trying transform [pow(A, 0) => 1]: " << power->ToString();
VLOG(10) << "trying transform [pow(A, 2) => A*A]: " << power->ToString();
VLOG(10) << "trying transform [pow(A, -1) => 1/A]: " << power->ToString();
VLOG(10) << "trying transform [pow(pow(A, X), Y) => pow(A, X*Y)]: " << power->ToString();
VLOG(4) << " old broadcast: " << broadcast->ToString();
VLOG(4) << " old user: " << user->ToString();
VLOG(4) << " new user: " << new_user->ToString();
VLOG(4) << " new broadcast: " << new_broadcast->ToString();
VLOG(10) << "deleting no-op reshape";
VLOG(10) << "Trying to simplify scalar slice of pad";
VLOG(10) << "Not folding scalar slice of pad, pad has interior padding";
VLOG(10) << "Folding scalar slice of pad into padding value";
VLOG(10) << "Folding scalar slice of pad into padded value";
VLOG(10) << "Not folding scalar slice of pad into padded value as they " "have different shapes.";
VLOG(10) << "Trying to simplify scalar slice of concat";
VLOG(10) << "Not folding, slice is not rank 1";
VLOG(10) << "Folding scalar slice of concat into concat operand";
VLOG(10) << "Folding scalar slice of concat into slice of concat operand";
VLOG(10) << "trying transform [rsqrt(Pow(A, -2)) => |A|] " << rsqrt->ToString();
VLOG(10) << "Not folding pad into reduce-window as there is no pad.";
VLOG(10) << "Considering folding Pad: " << pad->ToString() << "into reduce-window: " << reduce_window->ToString() << (convert != nullptr ? absl::StrCat("via convert: ", convert->ToString()) : "");
VLOG(10) << "Not folding interior pad into base-dilated reduce-window.";
VLOG(10) << "Not folding pad into reduce-window due to different pad " "values.";
VLOG(10) << "Window has stride.";
VLOG(10) << "Window has uneven padding.";
VLOG(10) << "Window has interior padding.";
VLOG(10) << "Found non-trivial dimension being padded: " << i;
VLOG(10) << "Found to be padding trivial dimensions only.";
VLOG(10) << "Found window did not cover single unpadded element in " "dimension: " << i;
VLOG(10) << "Found window covers more than one element in non-trivial " "dimension: " << i;
VLOG(10) << "Found window covers a single unpadded element.";
VLOG(10) << "Replacing pad/reduce-window with broadcast.";
VLOG(10) << "Folding pad into reduce-window.";
VLOG(10) << "trying transform [sqrt(A*A) => |A|] " << sqrt->ToString();
VLOG(10) << "deleting no-op transpose";
XLA_VLOG_LINES(2, "AlgebraicSimplifier::Run(), before:" + module->ToString());

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/all_reduce_combiner.cc
VLOG(1) << "Combined " << to_combine.size() << " CRS ops";
VLOG(1) << "Combining set";
VLOG(1) << "Set element: " << hlo->ToString();
VLOG(1) << "Replacing with : " << combined->ToString();
VLOG(1) << "Skipping due to non-trivial reduction function.";
VLOG(1) << "Skipping due to non-trivial reduction function.";
VLOG(1) << "Running AllReduceCombiner with threshold of " << combine_threshold_in_bytes_ << " bytes";
VLOG(1) << "Skip AllReduceCombiner because the module contains all-reduce " "with constrained layouts";
VLOG(1) << "Considering HLO " << instructions.front()->ToString() << " with current set size of " << current_size_in_bytes << " and current operand count of " << current_operand_count;
VLOG(1) << "Skipping due to " << instructions.front()->operands().size() << " operands";
VLOG(1) << "Skipping due to size " << size_in_bytes << " above threshold";
VLOG(1) << "Starting new set due to dependency between " << previous[i]->ToString() << " AND " << instructions[i]->ToString();
VLOG(1) << "The instruction cannot be entered into the set due " "to the combined size being too large.";
VLOG(1) << "Skipping as the instruction is larger than the set.";
VLOG(1) << "Resetting the set as the set is larger than the instruction.";
VLOG(1) << "Adding instruction to set.";
VLOG(1) << "Done constructing sets. Final set size is " << current_size_in_bytes << " bytes and " << current_operand_count << " operands";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/memory_space_assignment.cc
VLOG(4) << "Keeping value " << interval.buffer->ToShortString() << " in default mem because it is a tuple.";
VLOG(4) << "Keeping value " << interval.buffer->ToShortString() << " in default mem because it is a scalar.";
VLOG(4) << "Keeping value " << interval.buffer->ToShortString() << " in default mem because it has a tuple-select or " << "add-dependency position.";
VLOG(4) << "Keeping value " << interval.buffer->ToShortString() << " in default mem because it is a request identifier for send/recv.";
VLOG(1) << "Assigning buffers to alternate memory. Max heap size = " << options_.max_size_in_bytes;
VLOG(4) << "Interval " << interval.buffer->ToShortString() << " is reserved in the alternate memory. Total reserved bytes = " << reserved_in_bytes_;
VLOG(3) << "Coloring " << position.ToString();
VLOG(4) << "Not allocating " << interval.buffer->ToShortString() << " because it aliases with another interval and " << " allocate_across_sequential_calls is false.";
VLOG(3) << "Adding an aliased allocation: (" << aliased_allocation->start_time() << ", " << aliased_allocation->end_time() << ") pos: " << aliased_allocation->defining_position() << " mem space: " << (aliased_allocation->memory_space() == MemorySpace::kDefault ? "default" : "alt");
VLOG(3) << "skip use " << use.ToString() << " because it's in a different computation.";
VLOG(3) << "Allocation for " << value_and_sequence.value->ToShortString();
VLOG(3) << " " << alloc->start_time() << "-" << alloc->end_time() << addr_str << ", " << alloc->uses().size() << " uses";
VLOG(3) << "Adding required assignment for parameter value = " << value->ToShortString() << " time = " << parameter_instruction_time << " space = " << (memory_space == MemorySpace::kDefault ? "def" : "alt");
VLOG(3) << "Adding required assignment for output value = " << value->ToShortString() << " time = " << root_instruction_time << " space = " << (memory_space == MemorySpace::kDefault ? "def" : "alt");
VLOG(3) << "Committing chunk: " << interval_and_chunk.first.start << "-" << interval_and_chunk.first.end << " : [" << interval_and_chunk.second.chunk.offset << ", " << interval_and_chunk.second.chunk.size << "]";
VLOG(2) << "Finding allocation for " << request.buffer->ToShortString() << " (" << request.start_time << ", " << request.end_time << ") latest prefetch = " << request.latest_prefetch_time << " last use = " << request.last_use_time << " use = " << request.use.ToString() << ". Size = " << request.size << ", def pos = " << defining_position.ToString();
VLOG(4) << "Not trying to prefetch because use requires buffer in default mem.";
VLOG(3) << "Copy to " << (memory_space == MemorySpaceAssignment::MemorySpace::kDefault ? "default" : "alternate") << " memory between " << start_time << " and " << copy_done_schedule_before_time << " keeping until " << end_time;
VLOG(4) << "We can eliminate copy to alternate memory. Preferred offset = " << (preferred_offset ? *preferred_offset : -1);
VLOG(3) << "Keep the buffer in alternate memory. Offset = " << chunk_candidate->chunk.offset << ", size = " << chunk_candidate->chunk.size << ", heap_size = " << chunk_candidate->heap_size << ", prefetch picker = " << options_.prefetch_interval_picker->ToNoCopyDebugString( defining_position.instruction->shape(), request.start_time, request.end_time);
VLOG(4) << "Eviction (" << eviction_start_time << ", " << eviction_end_time << ") preferred end time = " << eviction_mem_interval.end;
VLOG(3) << "Evicting buffer at " << prev_allocation->chunk().offset << " (" << eviction_start_time << ", " << eviction_end_time << ")";
VLOG(3) << "This violates the maximum async copies.";
VLOG(3) << "Eviction interval is too short (" << eviction_start_time << ", " << eviction_end_time << ").";
VLOG(3) << "Try evicting (" << time << ", " << time + 1 << ")";
VLOG(3) << "Eviction successful.";
VLOG(3) << "Bailing: Could not evict " << request.use.ToString() << " because we hit the limit of maximum asynchronous copies " << "between " << hlo_live_range_.flattened_instruction_sequence() .instructions()[eviction_start_time] << " and " << hlo_live_range_.flattened_instruction_sequence() .instructions()[eviction_end_time];
VLOG(4) << "Trying prefetch picker = " << options_.prefetch_interval_picker->ToDebugString();
VLOG(4) << "Trying alternate memory allocation (" << alternate_mem_interval.start << ", " << alternate_mem_interval.end << ")";
VLOG(4) << "This would violate the outstanding async copy limit.";
VLOG(4) << "This would violate asynchronous copy ordering.";
VLOG(3) << "Move the buffer to alternate memory at " << alternate_mem_interval.start << ". Offset = " << chunk_candidate.chunk.offset << ", size = " << chunk_candidate.chunk.size << ", heap_size = " << chunk_candidate.heap_size << ", prefetch picker = " << options_.prefetch_interval_picker->ToDebugString();
VLOG(4) << "Module before memory space assignment: ";
XLA_VLOG_LINES(4, module->ToString());
VLOG(4) << "Schedule: " << module->schedule().ToString();
VLOG(4) << "Module after memory space assignment: ";
XLA_VLOG_LINES(4, module->ToString());
VLOG(1) << "Maximum number of outstanding async copies: " << CountMaximumOutstandingAsyncCopies(*module);
VLOG(4) << "Old shape = " << subshape.ToString() << ", new shape = " << new_instruction->shape().ToString() << "; inserting a bitcast.";
VLOG(4) << "Old shape = " << operand_shape.ToString() << ", new shape = " << copy_done_->shape().ToString() << "; inserting a bitcast.";
VLOG(3) << "Exported alternate memory allocations:";
VLOG(3) << " [" << pair.second.offset << ", " << pair.second.size << "] : " << pair.first.ToString();
VLOG(3) << "Exported alternate memory sizes:";
VLOG(3) << " space: " << pair.first << ", size: " << pair.second.size;
VLOG(3) << "Coloring " << position.ToString();
VLOG(3) << "Removing instruction from preset assignments.";
VLOG(4) << "Not simplifying " << computation->name() << " because it's not in the schedule.";
VLOG(4) << "Running simplify graph loop over " << computation->name();
VLOG(4) << "Instruction removed: " << instruction->ToString();
VLOG(4) << "Replacing uses of " << instruction->ToString() << " with " << forwarded_instruction->ToString();
VLOG(4) << "inserting: " << new_instruction->ToShortString();
VLOG(4) << "Delaying CopyStart (" << copy_start_schedule_after << " to " << (copy_start_schedule_after + 1) << ") for " << copy_allocation->copy_start()->ToString() << " because it is not in the correct computation.";
VLOG(4) << "Not scheduling " << computation->name() << " because it's not in the schedule.";
VLOG(4) << "Scheduling: " << computation->ToString();
VLOG(4) << "before " << instruction_index << ": " << new_instruction->name();
VLOG(4) << "inst " << instruction_index << ": " << instruction->name();
VLOG(4) << "after " << instruction_index << ": " << new_instruction->name();
VLOG(3) << "Verifying:";
VLOG(3) << " value: " << value->ToShortString() << " (" << time_bound.start << ", " << time_bound.end << ")";
VLOG(3) << " buffer: " << buffer.ToString() << ": (" << start_time << ", " << end_time << ") off: " << position_and_chunk.second.offset << ", size: " << position_and_chunk.second.size;
VLOG(3) << "Memory usage: " << memory_usage << " at time: " << time;
VLOG(1) << "Max memory usage ignoring fragmentation: " << max_memory_usage;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/call_graph.cc
VLOG(3) << "Building call graph for:";
XLA_VLOG_LINES(3, module->ToString());
XLA_VLOG_LINES(2, call_graph->ToString());

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/dfs_hlo_visitor.cc
VLOG(3) << "marking HLO " << &instruction << " as visiting: ";
VLOG(3) << "marking HLO " << &instruction << " as visited: ";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/flatten_call_graph.cc
LOG(FATAL) << "unexpected opcode: " << HloOpcodeString(instruction->opcode());
XLA_VLOG_LINES(3, "Before flatten call graph:" + module->ToString());
XLA_VLOG_LINES(3, "After flatten call graph:" + module->ToString());

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/while_loop_invariant_code_motion.cc
VLOG(2) << "Trying to hoist from " << while_instr_name;
VLOG(2) << "Loop has a trip count of at most 1, skipping.";
VLOG(2) << "Adding " << instruction->ToString(print_no_metadata) << " to unhoisted invariant set.";
VLOG(2) << "Hoisting " << instruction->ToString(print_no_metadata);
VLOG(1) << "Hoisted " << instructions_to_replace.size() << " instructions from " << while_instr_name;
VLOG(2) << "HLO module before WhileLoopConstantSinking:";
XLA_VLOG_LINES(2, module->ToString());
VLOG(2) << "HLO module after WhileLoopConstantSinking:";
XLA_VLOG_LINES(2, module->ToString());
VLOG(2) << "HLO module unchanged after WhileLoopConstantSinking";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/hlo_cse.cc
VLOG(4) << "Combined " << combined << " constants in " << computation->name() << " computation";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/hlo_schedule.cc
VLOG(2) << "VerifySchedule()";
XLA_VLOG_LINES(2, ToString());

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/allocation_tracker.cc
VLOG(2) << "Register";
VLOG(2) << "RegisterReplicatedBuffers";
VLOG(2) << "RegisterInternal(" << "tag: "" << tag << "" with " << replicated_buffers.size() << " shaped_buffers.";
VLOG(2) << "shaped_buffer:" << shaped_buffer;
VLOG(2) << "handle: " << handle;
VLOG(2) << "Unregister(" << "handle: " << data.handle() << ")";
VLOG(2) << "resolve:" << data.handle();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/copy_insertion.cc
VLOG(2) << "Adding copies for kWhile instruction " << xla_while->name();
VLOG(2) << "No copies necessary for kWhile instruction " << xla_while->name();
VLOG(2) << "Adding copies for " << xla_while->name() << " at indices:";
VLOG(2) << " " << pair.first;
VLOG(2) << "Adding copies for kConditional instruction " << conditional->name();
XLA_VLOG_LINES(3, ToString());
VLOG(2) << "Trying to remove " << copy->name();
VLOG(2) << copy->name() << " is not removable";
VLOG(2) << copy->name() << " is not removable (shape mismatch)";
VLOG(3) << copy->name() << " copies value " << src->value->ToShortString();
VLOG(3) << "Source buffer values: " << ValueListToString(src);
VLOG(3) << "Dest buffer values: " << ValueListToString(dest);
VLOG(2) << copy->name() << " source and destination buffers are same.";
VLOG(2) << copy->name() << " defines the first value in its buffer";
VLOG(2) << "Not removing the copy: live range of " << src->value->ToShortString() << " is not before " << next_dest->value->ToShortString();
VLOG(2) << "Not removing the copy: live range of " << last_dest->value->ToShortString() << " is not before " << next_src->value->ToShortString();
VLOG(2) << copy->name() << " copies the last value (" << src->value->ToShortString() << ") in its buffer";
VLOG(2) << "Not removing the copy: live range of " << prev_dest->value->ToShortString() << " is not before " << first_src->value->ToShortString();
VLOG(2) << "Not removing the copy: live range of " << src->value->ToShortString() << " is not before " << next_dest->value->ToShortString();
VLOG(2) << copy->name() << " copies value in middle of source buffer to value in middle " "of destination buffer";
XLA_VLOG_LINES(4, ToString());
VLOG(2) << "Removing copy " << operand_node->value->ToShortString() << " => " << copy_value_node->value->ToShortString();
VLOG(3) << "Checking live range of " << *a.value << " WRT " << *b.value;
VLOG(2) << "Empty uses for " << *a.value;
VLOG(3) << "Checking use " << *use << " against " << *b.value;
VLOG(2) << "Use " << *use << " is NOT before " << *b.value;
VLOG(3) << "Use " << *use << " is before " << *b.value;
VLOG(2) << " Live range of " << a.value->ToShortString() << " is before " << b.value->ToShortString();
VLOG(2) << " Live range of " << a.value->ToShortString() << " is not before " << b.value->ToShortString();
VLOG(2) << "Value " << value->ToShortString() << " is read only, but its buffer contains more than one value. " "Copying.";
VLOG(2) << "Output indices " << index.ToString() << " and " << other_index.ToString() << " are both aliased to " << alias->parameter_number << " copying " << other_index;
VLOG(2) << "Index " << index << " of computation " << computation->name() << " (" << root->name() << ") has ambiguous or non-distinct buffer. Copying.";
VLOG(2) << "Root of (" << root->name() << ") of computation(" << computation->name() << ") has constant or parameter value at index " << index << ". Copying.";
LOG(INFO) << "Removing unnecessary copies in " << module->name();
LOG(INFO) << "Buffer values, in dependency order: ";
LOG(INFO) << " HloBuffer " << buffer.id();
VLOG(2) << "Running fixpoint iteration " << num_iterations << " of copy elision";
VLOG(1) << "Num copies before copy-insertion: " << GetNumExistingCopies(module);
VLOG(1) << "Num copies after copy-insertion: " << num_total_copies;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/conditional_to_select.cc
VLOG(1) << "Not transforming conditional; branches have side effects:" << conditional->ToString();
VLOG(1) << "Running conditional-to-select pass";
VLOG(1) << "Visiting conditional: " << callsite.ToString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/hlo_dataflow_analysis.cc
VLOG(4) << "NewHloValue = " << emplaced.first->second.ToShortString();
VLOG(4) << "MarkValueForDeletion(" << value.ToShortString() << ")";
VLOG(4) << "Phi(" << instruction->name() << ")";
VLOG(5) << "instruction value set = " << GetInstructionValueSet(instruction).ToString();
VLOG(5) << "input value set = " << input->ToString();
VLOG(5) << "current_value_defined_here: " << current_value->ToString();
VLOG(5) << "after input_value_ids.size = " << input_value_ids.size();
LOG(FATAL) << "CallContext::kSequential computations should only be " "called from call, while, or conditional instructions";
VLOG(3) << "Worklist top: " << instruction->name();
VLOG(3) << ToString();
VLOG(4) << "No change.";
VLOG(4) << "New value set for " << instruction->name() << ": " << GetInstructionValueSet(instruction);
VLOG(1) << "HloDataflowAnalysis::Run on module " << module.name();
XLA_VLOG_LINES(2, module.ToString());
XLA_VLOG_LINES(1, dataflow_analysis->ToString());

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/hlo_sharding_metadata.cc
VLOG(4) << " " << instruction->name() << " to " << sharding;
VLOG(2) << "Found passthrough domain link:";
VLOG(2) << " " << user->ToString();
VLOG(2) << " " << instruction->ToString();
VLOG(2) << "Found passthrough domain link:";
VLOG(2) << " <root>";
VLOG(2) << " " << instruction->ToString();
VLOG(4) << "Applying " << sharding << " sharding";
VLOG(4) << " " << instruction->name() << " already has sharding " << instruction->sharding();
VLOG(4) << " " << instruction->name() << " to sharding " << instruction->sharding();
VLOG(1) << "Assigning non-trivial sharding " << sharding;
LOG(WARNING) << "Unassigned instruction: " << instruction->ToString();
VLOG(4) << "Extracted sharding is " << *sharding;
VLOG(4) << "Normalizing sharding to " << sharding->ToString() << ":";
VLOG(4) << "Normalizing sharding-less domain to " << sharding->ToString();
VLOG(1) << "Unable to find common sharding";
VLOG(3) << "Creating domain:";
VLOG(3) << " Instruction: " << instruction->name();
VLOG(3) << " Operand: " << operand->name();
VLOG(3) << " User side sharding: " << (instruction_sharding != nullptr ? instruction_sharding->ToString() : "None");
VLOG(3) << " Operand side sharding: " << (root_sharding != nullptr ? root_sharding->ToString() : "None");

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/hlo_runner.cc
VLOG(1) << "Created HloRunner for platform: " << platform->Name();
VLOG(1) << "Starting infeed on device " << device;
VLOG(1) << "Infeed step " << step;
VLOG(1) << "Starting outfeed on device " << device;
VLOG(1) << "Outfeed step " << step;
LOG(INFO) << "Replicated execution started";
LOG(INFO) << "Creating thread pool for " << options.num_replicas << " replicas";
LOG(INFO) << "Replicated execution terminated";
VLOG(1) << "Executing on platform " << backend().platform()->Name();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc
LOG(ERROR) << alarm->msg();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/bfloat16_conversion_folding.cc
XLA_VLOG_LINES( 2, "BFloat16ConversionFolding::Run(), before:" + module->ToString());

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/hlo_lexer.cc
LOG(ERROR) << "Failed to parse int literal: " << slice;
LOG(ERROR) << "Failed unescaping string: " << raw << ". error: " << error;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/transfer_manager.cc
VLOG(2) << "Writing tuple index tables for " << device_buffer;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/batchnorm_expander.cc
XLA_VLOG_LINES(2, "BatchNormExpander::Run(), before:" + module->ToString());

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/layout_assignment.cc
VLOG(3) << "SetBufferLayout : " << buffer << " : " << LayoutUtil::HumanString(layout);
VLOG(3) << "Buffer" << buffer << " already has a mandatory layout constrain, skipping";
VLOG(3) << "SetOperandLayout : " << instruction->name() << ", operand " << operand_no << " : " << ShapeUtil::HumanStringWithLayout(shape_with_layout);
VLOG(3) << "SetResultLayout : " << ShapeUtil::HumanStringWithLayout(shape_with_layout);
VLOG(3) << "SetInstructionLayout : " << instruction->name() << ", " << ShapeUtil::HumanStringWithLayout(shape_with_layout);
VLOG(3) << "Adding mandatory layout constraints to computation " << computation->name();
VLOG(2) << "Reset %while body parameter layout: body=" << body->name() << " while=" << instruction->name() << " shape=" << body_layout.result_layout().ToString();
VLOG(2) << "Reset %while condition parameter layout: cond=" << condition->name() << " while=" << instruction->name() << " shape=" << body_layout.parameter_layout(0).ToString();
VLOG(5) << "Operand " << operand->ToString() << " layout matches in " << instruction->ToString();
VLOG(4) << "New copy of " << operand->ToString() << " is " << param_copy->ToString();
VLOG(4) << "New copy of " << operand->ToString() << " is " << operand_copy->ToString();
VLOG(2) << "Propagating " << layout_constraint->ToString() << " to its neighbors.";
LOG(FATAL) << "Invalid constraint type: " << *layout_constraint;
VLOG(5) << "PropagateBufferConstraintToOperands: " << buffer_constraint.ToString();
VLOG(6) << "Propagating constraint to operand " << operand_no << " of " << instruction->ToShortString();
VLOG(6) << "Operand already has a constraint " << constraint->ToString();
VLOG(3) << "Propagating layout through backedge" << buffer_constraint.layout().ToString();
VLOG(2) << "Assigning layouts to computation: " << computation->name();
XLA_VLOG_LINES(2, computation->ToString());
XLA_VLOG_LINES(2, constraints.ToString());
VLOG(2) << " Calculated ComputationLayout = " << computation_layout.ToString();
VLOG(2) << "LayoutAssignment::RunOnComputation(" << computation->name() << ")";
VLOG(2) << " New ComputationLayout = " << computation_layout->ToString();
VLOG(2) << " Existing ComputationLayout = " << computation_layout->ToString();
VLOG(2) << " No ComputationLayout specified (will be calculated)";
VLOG(4) << "Assigning layout to parameter " << i << " of computation " << computation->name() << ": " << computed_computation_layout.parameter_layout(i).ToString();
VLOG(4) << "Assigning result layout of computation " << computation->name() << ": " << computed_computation_layout.result_layout().ToString();
VLOG(2) << "Running layout assignment on module " << module->name();
VLOG(5) << "Running " << (i == 0 ? "un" : "") << "constrained pass";
VLOG(5) << "Clearing previous side effects";
VLOG(5) << "Removing added copy: " << instruction->ToString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/generic_transfer_manager.cc
VLOG(2) << "transferring literal from device ordinal " << stream->parent()->device_ordinal() << "; device buffer: " << device_buffer;
VLOG(2) << "transferring literal shape to device: " << ShapeUtil::HumanString(shape) << "; device buffer: " << device_buffer;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/dump.cc
LOG(ERROR) << "--xla_dump_to=" << opts.xla_dump_to() << ", but environment variable TEST_UNDECLARED_OUTPUTS_DIR " "is not set, so cannot dump anywhere.";
LOG(ERROR) << "Refusing to write " << filename << " to stdout. Pass --xla_dump_to=<path> to write to a file.";
VLOG(1) << "Dumping " << filename << " to " << dir;
LOG(ERROR) << "Could not create directory " << dir << " for dumping XLA debug data: " << status;
LOG(ERROR) << "Could not get matching paths for pattern " << pattern << ": " << status;
LOG(ERROR) << "Have already dumped " << matches.size() << " modules, more than the limit of " << opts.dump_max_hlo_modules;
LOG(ERROR) << "Could not write XLA debug data to " << file_path << ": " << status;
LOG(ERROR) << "Could not write XLA debug data to " << filename << ": " << status;
LOG(ERROR) << "Refusing to write HLO snapshot proto for " << filename << " to stdout. Pass --xla_dump_to=<path> to write to a file.";
LOG(ERROR) << "Failed to serialize HLO snapshot proto " << filename;
LOG(ERROR) << "Refusing to write HLO snapshot proto for " << filename << " to stdout. Pass --xla_dump_to=<path> to write to a file.";
LOG(ERROR) << "Failed to serialize HLO snapshot proto " << filename;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/convolution_group_converter.cc
VLOG(2) << "Dealing with batch_group_count " << batch_group_count << " for convolution " << convolution->ToString() << "";
VLOG(2) << "is_cost_viable_ " << is_cost_viable_(convolution);
VLOG(2) << "output_batch_dimension " << output_batch_dimension;
VLOG(2) << "New output shape of convolution " << expanded_filter_shape.ToString();
VLOG(2) << "Expanded convolution " << new_convolution->ToString();
VLOG(2) << "is_cost_viable_ " << is_cost_viable_(convolution);
XLA_VLOG_LINES( 2, "ConvolutionGroupConverter::Run(), before:" + module->ToString());
XLA_VLOG_LINES( 2, "ConvolutionGroupConverter::Run(), after:" + module->ToString());

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/hlo_ordering.cc
VLOG(4) << "UseIsBeforeValueDefinition(use=" << use << ", value=" << value.ToShortString() << ")";
VLOG(4) << " use instruction executes before value-defining instruction";
VLOG(4) << " use is value def, and instruction can share use buffer";
VLOG(4) << " use is while " << use.instruction->name() << " and def is in body";
VLOG(4) << " use is while " << use.instruction->name() << " and def is in condition and is not the parameter";
VLOG(4) << " use is while " << use.instruction->name() << " and def is in condition and is the parameter";
VLOG(4) << " value is while " << value.defining_instruction()->name() << " and use is in condition or body";
VLOG(4) << " use is call " << use.instruction->name() << " and def is in called computation";
VLOG(4) << " use is conditional " << use.instruction->name() << " and def is in " << j << "th branch computation";
VLOG(4) << " use is conditional " << use << " and def is " << value.ToShortString();
VLOG(4) << " use is not before value";
VLOG(4) << "LiveRangeStrictlyBefore(a = " << a.ToShortString() << ", b = " << b.ToShortString() << ")";
VLOG(4) << a << " not defined before " << b;
VLOG(4) << a << " is live out of module and not defined before " << b;
VLOG(4) << "use of " << a << " (" << use << ") not before " << b << " is defined";
VLOG(4) << a << " is live out of computation and defined before " << b << " which is in same computation";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/hlo_evaluator.cc
LOG(FATAL) << "unhandled direction for conversion to Comparison: " << ComparisonDirectionToString(direction);
LOG(FATAL) << "unhandled direction for conversion to Comparison: " << ComparisonDirectionToString(direction);
XLA_VLOG_LINES( 2, "HloEvaluator::Evaluate computation:" + computation.ToString());
VLOG(1) << "TryEvaluate failed:" << result_or.status();
VLOG(2) << "Parameter evaluated to: " << input_literal->ToString();
LOG(FATAL) << "HandleReal: unknown/unhandled primitive type: " << PrimitiveType_Name(operand->shape().element_type());
LOG(FATAL) << "HandleImag: unknown/unhandled primitive type: " << PrimitiveType_Name(operand->shape().element_type());
LOG(FATAL) << "HandleComplex: unknown/unhandled primitive type: " << PrimitiveType_Name(complex->shape().element_type());
LOG(FATAL) << "HandleCompare: unknown primitive type: " << PrimitiveType_Name(lhs->shape().element_type());
VLOG(3) << "Loop iteration result: " << body_val.ToString();
VLOG(3) << "HandleSort operand " << i << " literal: " << GetEvaluatedLiteralFor(sort->operand(i)).ToString();
VLOG(3) << "HandleSort result_tuple: " << result_tuple.ToString();
VLOG(3) << "HandleReduce arg_literal: " << input_args[i]->ToString();
VLOG(3) << "HandleReduce init_literal: " << init_values[i]->ToString();
VLOG(2) << "About to visit HLO: " << hlo->ToString();
VLOG(2) << "Finished visiting " << hlo->ToString() << "; evaluated value is: " << GetEvaluatedLiteralFor(hlo).ToString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/compilation_cache.cc
VLOG(2) << "inserting cache key: " << key;
VLOG(2) << "looking up cache key: " << key;
VLOG(2) << "cache key not found: " << key;
VLOG(2) << "hit executable: " << result->module().name();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/shape_inference.cc
VLOG(2) << "inferred dot shape: " << ShapeUtil::HumanString(result);
VLOG(2) << StrFormat( "inferring shape for <%s>(%s, %s) with broadcast_dimensions={%s}", HloOpcodeString(opcode), ShapeUtil::HumanStringWithLayout(lhs), ShapeUtil::HumanStringWithLayout(rhs), StrJoin(broadcast_dimensions, ", "));
LOG(FATAL) << "Unexpected fft_type: " << fft_type;
VLOG(2) << StrFormat("slicing shape %s starts={%s} limits={%s}", ShapeUtil::HumanString(arg), StrJoin(starts, ", "), StrJoin(limits, ", "));
VLOG(2) << StrFormat("starts[%d] = %d", dimension, start_index);
VLOG(2) << StrFormat("limits[%d] = %d", dimension, limit_index);
VLOG(2) << StrFormat( "slicing shape %s at dynamic start_indices %s with slice_sizes={%s}", ShapeUtil::HumanString(operand_shape), ShapeUtil::HumanString(start_indices_shape), StrJoin(slice_sizes, ", "));
VLOG(2) << StrFormat("slicing shape %s a with slice_sizes={%s}", ShapeUtil::HumanString(operand_shape), StrJoin(slice_sizes, ", "));
VLOG(2) << StrFormat("slice_sizes[%d] = %d", dim, slice_dim_size);
VLOG(2) << StrFormat( "updating slice of shape %s at dynamic start_indices %s with update " "shape %s", ShapeUtil::HumanString(operand_shape), ShapeUtil::HumanString(start_indices_shape), ShapeUtil::HumanString(update_shape));
VLOG(2) << StrFormat("updating slice of shape %s with update shape %s", ShapeUtil::HumanString(operand_shape), ShapeUtil::HumanString(update_shape));
VLOG(2) << StrFormat("update_sizes[%d] = %d", dim, update_dim_size);
VLOG(3) << "Reshape inferred shape: " << ShapeUtil::HumanString(inferred_shape);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/hlo_instructions.cc
LOG(FATAL) << "Unimplemented field type: " << field->DebugString();
LOG(FATAL) << "Not yet implemented, clone: " << HloOpcodeString(opcode());
VLOG(3) << "CloneAndFuseInternal:" << instruction_to_fuse->ToString();
VLOG(2) << "New clone:" << clone->ToString();
LOG(FATAL) << "expects 1 operand";
LOG(FATAL) << "expects 2 operand";
LOG(FATAL) << "expects 0 operand";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/hlo_dce.cc
VLOG(3) << "Before dce:";
XLA_VLOG_LINES(3, computation->ToString());
VLOG(1) << "Removing dead root " << dead_root->ToString() << " and it's unused operands";
VLOG(3) << "After dce:";
XLA_VLOG_LINES(3, computation->ToString());
VLOG(2) << "Before dce:";
XLA_VLOG_LINES(2, module->ToString());
VLOG(2) << "After dce:";
XLA_VLOG_LINES(2, module->ToString());

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/hlo_module_dce.cc
VLOG(1) << "WhileDCE SKIP while: " << xla_while->ToString();
VLOG(2) << "Before HloModuleDCE:";
XLA_VLOG_LINES(3, module->ToString());
VLOG(2) << "After HloModuleDCE:";
XLA_VLOG_LINES(3, module->ToString());

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/hlo_live_range.cc
VLOG(1) << "Moved value " << value->ToShortString() << " to while param: " << used->ToString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/hlo_memory_scheduler.cc
VLOG(2) << "Schedule instruction: " << best->ToShortString() << " Bytes freed: " << best_it->first.first;
VLOG(2) << "Computation: " << computation->name();
VLOG(2) << "Min-memory list sequence: " << HumanReadableNumBytes(list_memory);
VLOG(2) << "Min-memory dfs sequence: " << HumanReadableNumBytes(dfs_memory);
VLOG(2) << "Min-memory post order sequence: " << HumanReadableNumBytes(post_order_memory);
VLOG(2) << "Chose min-memory list sequence: " << HumanReadableNumBytes(list_memory);
VLOG(2) << "Chose min-memory dfs sequence: " << HumanReadableNumBytes(dfs_memory);
VLOG(2) << "Chose min-memory post_order sequence: " << HumanReadableNumBytes(post_order_memory);
VLOG(2) << "Min-memory list sequence: " << HumanReadableNumBytes(list_memory);
VLOG(2) << "Min-memory dfs sequence: " << HumanReadableNumBytes(dfs_memory);
VLOG(2) << "Min-memory post order sequence: " << HumanReadableNumBytes(post_order_memory);
VLOG(2) << "Chose min-memory list sequence: " << HumanReadableNumBytes(list_memory);
VLOG(2) << "Chose min-memory dfs sequence: " << HumanReadableNumBytes(dfs_memory);
VLOG(2) << "Chose min-memory post_order sequence: " << HumanReadableNumBytes(post_order_memory);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/executable.cc
VLOG(1) << "enqueueing executable on stream...";
LOG(ERROR) << "Failed to BlockHostUntilDone: " << status;
VLOG(1) << "enqueueing 'stop timer' and profiling callback...";
XLA_LOG_LINES(tensorflow::INFO, profile->ToString(*device_description));

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/optimize_input_output_buffer_alias.cc
VLOG(1) << "input_shape:" << input_shape.ToString();
VLOG(1) << "output_shape:" << output_shape.ToString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/hlo_pass_pipeline.cc
VLOG(1) << " Invariant checker " << invariant_checker->name();
VLOG(1) << " Invariant checker done " << invariant_checker->name();
VLOG(2) << "Failed invariant check:";
XLA_VLOG_LINES(2, hlo->ToString());
VLOG(1) << " HLO pass " << pass_name;
VLOG(1) << "*All* passes disabled by --xla_disable_all_hlo_passes.";
VLOG(1) << "Passes disabled by --xla_disable_hlo_passes: " << absl::StrJoin(disabled_pass_names, ", ");
VLOG(1) << "Passes enabled by --xla_enable_hlo_passes_only: " << absl::StrJoin(enabled_pass_names, ", ");
VLOG(1) << "Running HLO pass pipeline on module " << module->name() << ": " << name();
VLOG(1) << "Running HLO pass pipeline on module group " << module_group->name() << ": " << name();
VLOG(1) << "Module group is empty. Nothing to do.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/heap_simulator.cc
XLA_VLOG_LINES(1, computation.parent()->ToString());
XLA_VLOG_LINES(2, computation.ToString());
VLOG(1) << hlo_live_range->ToString();
VLOG(1) << "Program time" << hlo_live_range->schedule_end_time();
VLOG(1) << "Time step: " << i;
VLOG(1) << "Start buffer: " << value->ToShortString();
VLOG(1) << " ShareWith" << first_allocated_value[hlo_buffer]->ToShortString();
VLOG(1) << "Sharing " << value->ToShortString() << " with " << operand_value->ToShortString() << ", size:" << size_fn_(*value);
VLOG(1) << "Free Buffer: ";
VLOG(1) << " " << value->ToShortString();
VLOG(1) << "result heap_size: " << result_.heap_size;
VLOG(1) << "Finding chunks for buffer: " << buffer_interval.buffer->ToString();
VLOG(1) << "Size " << buffer_interval.size << ", start " << buffer_interval.start << ", end " << buffer_interval.end;
VLOG(1) << " Alias size " << colocation_interval.size << ", start " << colocation_interval.start << ", end " << colocation_interval.end << " " << colocation_interval.buffer->ToString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/bfloat16_propagation.cc
VLOG(2) << "Fused root " << root->ToString() << " at shape index " << index << " changed to BF16 precision for fusion " << fusion->ToString();
VLOG(2) << "While body root " << body_root->ToString() << " at shape index " << index << " changed to BF16 precision for while " << while_hlo->ToString();
VLOG(2) << "HloInstruction output at shape index " << index << " changed to BF16 precision: " << hlo->ToString();
VLOG(2) << "Called computation parameter " << parameter->ToString() << " at shape index " << index << " adjusted to " << (operand_type == BF16 ? "BF16" : "F32") << " to match operand in HLO " << hlo->ToString();
VLOG(2) << "Called computation root " << root->ToString() << " at shape index " << index << " adjusted to " << (output_type == BF16 ? "BF16" : "F32") << " to match output shape of " << hlo->ToString();
VLOG(2) << "HloInstruction output at shape index " << index << " adjusted to " << (type == BF16 ? "BF16" : "F32") << ": " << hlo->ToString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/while_loop_analysis.cc
VLOG(2) << "GetGTEOperandIndex(" << instr->ToString() << ", " << gte_operand->ToString() << ")";
VLOG(2) << "instr does not have a gte operand.";
VLOG(2) << "instr uses something other than a constant or gte(gte_operand, " << tuple_idx << "): " << operand->ToString();
VLOG(2) << "Finding induction variable for loop " << while_op->ToShortString();
VLOG(2) << "Induction variable not found in loop condition: " << while_cond->root_instruction()->ToString();
VLOG(2) << "While body's root is not a tuple instruction: " << while_body_root->ToString();
VLOG(2) << "Induction variable not found in while body increment instruction: " << while_body_inc->ToString();
VLOG(2) << "Tuple index of induction variable does not match between loop " "condition (" << *indvar_tuple_idx << ") and while body (" << *while_body_indvar_tuple_idx << ")";
VLOG(2) << "While init expected to be a tuple: " << while_init->ToString();
VLOG(2) << "Induction variable's tuple index: " << *indvar_tuple_idx;
VLOG(2) << "literal is not an effective scalar: " << l.ToString();
VLOG(2) << "uint64 literal is out of range for int64: " << v;
VLOG(2) << "literal is of non-integral type " << l.shape().ToString();
VLOG(2) << "Pattern-match failed: induction variable init is not a " "constant scalar representable as an int64: " << indvar_init.ToString();
VLOG(2) << "Pattern-match failed: induction variable does not go as i++: " << while_body_indvar_update->ToString();
VLOG(2) << "Pattern-match failed: while condition is not of the form " "op(i, N) or op(N, i).";
VLOG(2) << "Pattern-match failed: while condition induction variable is " "not a constant scalar representable as an int64.";
VLOG(2) << "Pattern-match succeeded: loop condition is i < N: " << while_cond_root->ToString();
VLOG(2) << "Pattern-match failed: Trip count exceeds INT64_MAX.";
VLOG(2) << "Pattern-match succeeded: loop condition is i <= N: " << while_cond_root->ToString();
VLOG(2) << "Pattern-match failed: Trip count exceeds INT64_MAX";
VLOG(2) << "Pattern-match failed: Trip count exceeds INT64_MAX";
VLOG(2) << "Pattern-match failed: while condition follows unknown pattern: " << while_cond_root->ToString();
VLOG(2) << "Getting trip count for loop " << while_op->ToString();
VLOG(2) << "Couldn't evaluate induction variable init, " << indvar_init_result.status() << ", " << indvar_init->ToString();
VLOG(2) << "Couldn't evaluate while cond: " << result.status();
VLOG(2) << "Loop has static trip count of " << trip_count;
VLOG(2) << "Couldn't evaluate induction variable update: " << indvar_next_result.status();
VLOG(2) << "Loop has unknown trip count.";
VLOG(2) << "Loop has exact trip count.";
VLOG(2) << "Induction variable not found in loop condition: " << while_cond->root_instruction()->ToString();
VLOG(3) << "While body's root is not a tuple instruction: " << while_body_root->ToString();
VLOG(3) << "While body does not set the IV to a constant: " << while_body_indvar->ToString();
VLOG(2) << "Couldn't evaluate while loop condition.";
VLOG(2) << "Upper bound on the trip count is 1";
VLOG(2) << "Loop has no known upper bound on the trip count.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/hlo_proto_util.cc
VLOG(4) << proto.ShortDebugString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/platform_util.cc
LOG(INFO) << "platform " << platform->Name() << " present but no " << "XLA compiler available: " << compiler_status.status().error_message();
LOG(INFO) << "StreamExecutor cuda device (" << executor->device_ordinal() << ") is of " << "insufficient compute capability: " << kMinCudaComputeCapabilityMajor << "." << kMinCudaComputeCapabilityMinor << " required, " << "device is " << major_version << "." << minor_version;
LOG(INFO) << "StreamExecutor ROCM device (" << executor->device_ordinal() << ") is of " << "obsolete AMDGPU ISA version: " << "gfx" << kMinAMDGPUISAVersion << " required, " << "device is gfx" << isa_version;
VLOG(1) << "Initializing devices";
VLOG(1) << "Not initializing StreamExecutor for device " << i << " since it is not in the visible device list";
VLOG(1) << "Started device init " << i;
LOG(WARNING) << "unable to create StreamExecutor for " << platform->Name() << ":" << i << ": " << executor_status.status().error_message();
VLOG(1) << "Finished device init " << i;
VLOG(1) << "Device initialization complete";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/hlo_element_type_converter.cc
XLA_VLOG_LINES( 3, "HloElementTypeConverter::Run(), before:" + module->ToString());
XLA_VLOG_LINES( 2, "HloElementTypeConverter::Run(), after:" + module->ToString());

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/hlo_domain_isolator.cc
VLOG(4) << "New domain: " << domain->ToString();
VLOG(3) << "Added " << added_domains << " kDomain instructions";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/human_readable_profile_builder.cc
VLOG(1) << "Total floating point ops: " << total_flops;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/instruction_fusion.cc
VLOG(1) << "There are " << fused_count << " fused bits that cause " << fuse_count << " fusion actions.";
VLOG(1) << FusionConfigToString(*fusion_config);
VLOG(1) << "Fusion count: " << fuse_count;
VLOG(2) << "Fusing " << producer->ToString() << " into " << consumer->ToString();
VLOG(2) << " created new fusion: " << fusion_instruction->ToString();
VLOG(2) << "Multi-output fusing " << producer->ToString() << " into " << consumer->ToString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/conditional_simplifier.cc
VLOG(2) << "Not attempting to remove conditional as it is not removable or " "has side effect: " << conditional->ToShortString();
VLOG(2) << "Not attempting to remove conditional as its branch_index is not a " "compile-time constant or contains expensive instructions: " << conditional->ToShortString();
VLOG(3) << "Skip RemoveUnusedTupleElements due to non-tuple result:" << conditional_op->ToShortString();
VLOG(3) << "Skip RemoveUnusedTupleElements due to non-GTE user:" << user->ToShortString();
VLOG(3) << "Skip RemoveUnusedTupleElements due to every index is in use.";
VLOG(3) << "Skip RemoveUnusedTupleElements due to some branch " << branch->name() << " has in-compatible root shape, expect " << old_shape.ToString() << ", but got " << root->shape().ToString() << "" << conditional_op->ToString();
VLOG(3) << "Skip MergeDuplicateTupleElements due not tuple shape nor root " "instruction:" << conditional->ToShortString();
VLOG(3) << "Skip MergeDuplicateTupleElements due not all users are " "kGetTupleElement:" << conditional->ToShortString();
VLOG(3) << "Skip MergeDuplicateTupleElements due not all branch roots " "are kTuple:" << conditional->ToShortString();
XLA_VLOG_LINES( 3, "ConditionalSimplifier::Run(), before:" + module->ToString());
XLA_VLOG_LINES(3, "ConditionalSimplifier::Run(), after:" + module->ToString());

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/hlo_alias_analysis.cc
VLOG(3) << "Merging colocated values, value: " << value->ToShortString();
VLOG(3) << "Compute kWhile aliases";
VLOG(3) << " value is init value to a while; must share buffer with " "while value " << while_value.ToShortString();
VLOG(3) << " value is parameter value of the body or condition of a " "while; must share buffer with while value " << while_value.ToShortString();
VLOG(3) << " value @ " << position << " is root of " << callsite.instruction()->name() << "; body root and while value root must share buffer " "among them : " << while_value.ToShortString();
VLOG(3) << " value is output of a while instruction";
VLOG(3) << "Compute kConditional aliases";
VLOG(3) << " value @ " << position << " is root of " << callsite.instruction()->name() << "; branch computation roots must share buffer among them : " << cond_value.ToShortString();
VLOG(3) << " value is output of a conditional instruction";
VLOG(2) << "Use of value " << value.ToShortString() << ": " << use;
VLOG(2) << "HloAliasAnalysis::Run on module " << module->name();
XLA_VLOG_LINES(2, module->ToString());
XLA_VLOG_LINES(2, alias_analysis->ToString());
VLOG(2) << "Merge buffer: " << from.ToString() << " into :" << to.ToString();
VLOG(1) << values[i - 1]->ToShortString() << " and " << values[i]->ToShortString() << " are not ordered";
VLOG(1) << "In buffer " << buffer.id() << " containing values: " << absl::StrJoin(values, ", ", [](string* out, const HloValue* value) { StrAppend(out, value->ToShortString()); }) << "Value " << values[i - 1]->ToShortString() << " may interfere with value " << values[i]->ToShortString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/slice_sinker.cc
VLOG(10) << "Adding operation_on_slice_sources: " << operation_on_slice_sources->ToString();
VLOG(10) << "Adding new slice: " << user_slice->ToString() << " to replace: " << user->ToString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/rng_expander.cc
VLOG(2) << "Expand rng instruction " << rng->ToString();
VLOG(2) << "Rng key " << key->ToString();
VLOG(2) << "Rng state " << state->ToString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/compilation_stats.cc
LOG(INFO) << "Total runtime (ms) of HLO passes: " << total_duration;
LOG(INFO) << "Pass name, num runs, time (ms)";
LOG(INFO) << pass_info.name << ", " << pass_info.num_runs << ", " << pass_info.duration_ms;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/hlo_domain_verifier.cc
VLOG(4) << "Running HLO Domain Verifier";
VLOG(4) << "Reach set:";
VLOG(4) << " " << instruction->name();
VLOG(4) << " Domains:";
VLOG(4) << " User side: " << instruction->name();
VLOG(4) << " " << meta.ToString();
VLOG(4) << " Operand side: " << instruction->name();
VLOG(4) << " " << meta.ToString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/while_loop_simplifier.cc
VLOG(2) << "Can't remove dead parameters from non-removable while op.";
VLOG(2) << "While op's carried value isn't tuple shaped.";
VLOG(2) << "While body's root is not a tuple(...) instruction.";
VLOG(2) << "Cowardly refusing to analyze while loop with " << instr->ToString(print_no_metadata) << " used by non-GTE instruction " << user->ToString(print_no_metadata) << " in computation " << instr->parent()->name();
VLOG(2) << "Can't remove elements from while loop's tuple -- it's already " "empty.";
VLOG(2) << "Loop " << while_op->ToString(print_no_metadata) << " uses all of its inputs; no simplification possible.";
VLOG(2) << "Tuple index " << i << " is not passed through loop body unmodified.";
VLOG(2) << "Loop " << while_op->ToString(print_no_metadata) << " uses all of its inputs; no simplification possible.";
VLOG(1) << "Eliminating " << tuple_size - used_tuple_indices.size() << " elements from tuple of " << while_op->ToString(print_no_metadata);
VLOG(2) << "Remapping tuple index " << old_idx << " to " << new_idx;
VLOG(2) << "Not attempting to remove while loop that is not removable: " << while_op->ToShortString();
VLOG(2) << "Not attempting to remove while loop whose condition contains " "side-effecting instructions: " << while_op->ToShortString();
VLOG(3) << "Found loop invariant tuple element " << i << " " << init_tuple_elem->ToString();
VLOG(3) << "tuple index " << instr->tuple_index() << " " << instr->ToString();
VLOG(3) << "Replace use of " << instr->ToString() << " with " << hlo_constant->ToString();
VLOG(10) << "Found existing trip counter at index " << i;
VLOG(10) << "Found induction variable at index " << i;
VLOG(10) << "Adding new trip counter to end of loop's tuple.";
XLA_VLOG_LINES(3, "WhileLoopSimplifier::Run(), before:" + module->ToString());
VLOG(2) << "Not attempting to simplify while loop because it contains a " "send/recv node: " << while_op->ToShortString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/hlo_graph_dumper.cc
VLOG(3) << "Generating Header";
LOG(FATAL) << from_node->name() << " was added to edges but not to nodes";
LOG(FATAL) << to_node->name() << " was added to edges but not to nodes";
VLOG(3) << "Adding css for edge " << edge_id << " from node " << from_node->name() << " to node " << to_node->name();
VLOG(3) << "Adding css for edge " << edge_id << " from node " << from_node->name() << " to root tag";
VLOG(2) << "Dumping subcomputation " << subcomp->name();
VLOG(2) << "Edge: from " << from->name() << " to " << parent_instr->name() << " as " << next_edge_id_;
VLOG(2) << "Adding root tag as node " << next_node_id_;
VLOG(2) << "Adding edge from " << from->name() << " to root tag as " << next_edge_id_;
VLOG(2) << "Adding node " << instr->name() << " as " << next_node_id_;
LOG(FATAL) << "Constants don't get their own nodes in the graph.";
LOG(WARNING) << "Multiple calls to RegisterGraphToURLRenderer. Last call " "wins, but because order of initialization in C++ is " "nondeterministic, this may not be what you want.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/hlo_parser.cc
VLOG(1) << "Error: " << error_.back();
LOG(FATAL) << "instruction " << root_name << " was marked as ROOT but the parser has not seen it before";
LOG(FATAL) << "unknown integral primitive type " << PrimitiveType_Name(shape.element_type());
LOG(FATAL) << "unknown floating point primitive type " << PrimitiveType_Name(shape.element_type());
LOG(FATAL) << PrimitiveType_Name(shape.element_type()) << " is not PRED type";
LOG(FATAL) << PrimitiveType_Name(shape.element_type()) << " is not a complex type type";
VLOG(3) << "Parsing attribute " << name;
LOG(FATAL) << "expects 3 items: lhs, rhs, and output dims, but sees " << str;
LOG(FATAL) << "expects 3 items: lhs, rhs, and output dims, but sees " << str;
LOG(INFO) << "parsed computation " << computation->name();
VLOG(3) << "ParseName";
VLOG(3) << "ParseString";
VLOG(3) << "ParseOpcode";
VLOG(3) << "ParseFftType";
VLOG(1) << "ParseComparisonDirection";
VLOG(3) << "ParseFusionKind";
VLOG(3) << "ParseRandomDistribution";
VLOG(3) << "ParseRandomAlgorithm";
VLOG(3) << "ParsePrecision";
VLOG(3) << "ParseInt64";
VLOG(3) << "ParseToken " << TokKindToString(kind) << " " << msg;
LOG(FATAL) << "Parser state is not clean. Please do not call any other " "methods before calling ParseSingleInstruction.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/depthwise_convolution_converter.cc
VLOG(2) << "Dealing with batch_group_count " << num_groups << " for convolution " << convolution->ToString() << "";
XLA_VLOG_LINES(2, "DepthwiseConvolutionConverter::Run(), before:" + module->ToString());
XLA_VLOG_LINES( 2, "DepthwiseConvolutionConverter::Run(), after:" + module->ToString());

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/hlo_computation.cc
VLOG(2) << "Removing instruction " << instruction->name() << " from computation " << name();
VLOG(10) << "transformed " << old_instruction->ToString() << " to " << new_instruction->ToString();
VLOG(3) << "Unreachable roots:" << absl::StrJoin(unreachable_roots, "", [](string* out, const HloInstruction* hlo) { absl::StrAppend(out, hlo->ToString()); });
VLOG(1) << "Cloning " << name() << " --> " << suffix << "";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/dynamic_padder.cc
VLOG(2) << "Rewriting dynamic reshape " << reshape->ToString() << " input dim: " << input_dim;
VLOG(2) << "Pre DynamicPadder HLO:";
VLOG(2) << "Has dynamic dimension of operand" << operand_num << " @" << input_dim;
VLOG(2) << "Post DynamicPadder HLO:";
XLA_VLOG_LINES(2, module->ToString());

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/hlo_instruction.cc
LOG(FATAL) << "Invalid unary instruction opcode " << HloOpcodeString(opcode);
LOG(FATAL) << "Invalid binary instruction opcode " << HloOpcodeString(opcode);
LOG(FATAL) << "Invalid ternary instruction opcode " << HloOpcodeString(opcode);
VLOG(3) << "CloneWithNewOperands: " << ToString();
VLOG(3) << " new operands:";
VLOG(3) << " %" << new_operand->name();
LOG(FATAL) << "target was not an operand: " << target->ToString();
LOG(FATAL) << "Base class impl called for opcode with subclass: " << opcode();
VLOG(3) << "Replacing uses of " << name() << " in " << user->name() << " with " << new_producer->name();
VLOG(3) << "Replacing operand " << operand_num << " of " << name() << " with " << new_operand->name() << ", was " << old_operand->name();
LOG(FATAL) << "Invalid opcode for to_apply(): " << HloOpcodeString(opcode());
LOG(FATAL) << "Invalid opcode for to_apply(): " << HloOpcodeString(opcode());
LOG(INFO) << "Directed cycle: " << absl::StrJoin( dfs, " ", [](std::string* out, const HloInstruction* instr) { out->append(instr->name()); });
VLOG(3) << "Not visiting HLO (id = " << current_id << ") as it was already visited.";
VLOG(2) << "Visiting HLO %" << current_node->name();
VLOG(3) << "HloInstruction::Accept(%" << name() << ")";
VLOG(2) << "HloInstruction::AcceptWithOperandOrder(%" << name() << ")";
VLOG(3) << "HloInstruction::AcceptWithOperandOrder BEFORE FINISH VISIT";
VLOG(3) << "HloInstruction::AcceptWithOperandOrder AFTER FINISH VISIT";
VLOG(2) << "HloInstruction::AcceptWithOperandOrder EXIT";
LOG(FATAL) << "Unimplemented method.";
LOG(FATAL) << "Unimplemented method.";
LOG(FATAL) << "Unimplemented method.";
LOG(FATAL) << "Unimplemented method.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/hlo_input_output_alias_config.cc
VLOG(4) << "Set up alias between output index " << output_index.ToString() << " and parameter " << param_index << " at index " << param_index.ToString();
LOG(FATAL) << "Unknown alias kind " << data->kind;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/dot_decomposer.cc
XLA_VLOG_LINES(2, "DotDecomposer ENTRY" + module->ToString());
XLA_VLOG_LINES(2, "DotDecompose EXIT" + module->ToString());

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/hlo_constant_folding.cc
XLA_VLOG_LINES(2, "HloConstantFolding::Run(), before:" + module->ToString());
VLOG(2) << "Constant folding failed for instruction: " << instruction->ToString();
VLOG(4) << "Constant folded: " << instruction->ToString();
XLA_VLOG_LINES(2, "HloConstantFolding::Run(), after:" + module->ToString());

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/hlo_module_group_metadata.cc
LOG(FATAL) << "opcode not supported";
VLOG(5) << "Companion path size do not match: " << path0.size() << " != " << path1.size();
VLOG(5) << "Companion instructions at path index " << i << " do not have the same opcode: " << path0[i].ToString() << " vs " << path1[i].ToString();
LOG(FATAL) << "unknown module";
VLOG(2) << "Created " << channels_.size() << " channels";
VLOG(2) << "Created " << all_reduce_map_.size() << " all-reduce groups";
VLOG(2) << "adding as companions:" << instruction1->ToString() << " and " << instruction2->ToString();
LOG(INFO) << "Channel " << channel.id << ": from_device=" << *from_device << " to_device=" << *to_device << " send=" << channel.send->name() << " send_done=" << channel.send_done->name() << " recv=" << channel.recv->name() << " recv_done=" << channel.recv_done->name();
LOG(INFO) << "From " << fromto_count.first.first << " to " << fromto_count.first.second << ": " << fromto_count.second;
LOG(INFO) << "Companion set:";
LOG(INFO) << " " << instruction->name();
LOG(INFO) << "Communicating instruction " << instruction_comm.first->name();
LOG(INFO) << " " << instruction->name() << " on device " << *device;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/batch_dot_simplification.cc
VLOG(2) << "Replaced " << batch_dot->ToString() << " with " << new_dot->ToString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/hlo_liveness_analysis.cc
VLOG(3) << "ADD instruction: " << instruction->name();
VLOG(3) << "MARK instruction: " << instruction->name() << " shape_index: " << shape_index.ToString();
VLOG(3) << "MARK instruction: " << instruction->name() << " shape_index: " << shape_index.ToString();
VLOG(1) << "VISIT instruction: " << instruction->name();
VLOG(1) << "HloLivenessAnalysis::Run on module " << module.name();
XLA_VLOG_LINES(2, module.ToString());

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/defuser.cc
VLOG(2) << "Defusing instruction: " << fusion_instruction->ToString();
VLOG(1) << "Defusing module " << module->name();
XLA_VLOG_LINES(2, "Before defusion:" + module->ToString());
XLA_VLOG_LINES(2, "After defusion:" + module->ToString());

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/buffer_assignment.cc
VLOG(4) << "Adding the following buffer to allocation #" << index() << absl::StrFormat(" (size=%d, offset=%d) %s", size, offset, buffer.ToShortString());
VLOG(3) << "Trying to find unique slice for " << instruction->name() << " [" << index << "]";
VLOG(3) << "Examining value " << *value;
VLOG(3) << "Has allocation";
VLOG(3) << "No allocation";
VLOG(3) << "HloBuffer lives out" << buffer.ToString();
VLOG(3) << "Set maybe live out: " << allocation->ToString();
VLOG(3) << "HloValue lives out: " << hlo_value.ToString();
VLOG(3) << "Set maybe live out: " << allocation->ToString();
VLOG(1) << "CombineTempAllocations()";
VLOG(1) << "Combined temp allocation for color " << color << " is: " << temp_allocation;
VLOG(1) << "Combined allocation absorbing temp allocation: " << temp_allocation;
VLOG(4) << "End of live range of " << buffer1->ToShortString() << " is equal to the start of live range of " << buffer2->ToShortString() << ", buffer cannot be shared.";
VLOG(4) << "End of live range of " << buffer2->ToShortString() << " is equal to the start of live range of " << buffer1->ToShortString() << ", buffer cannot be shared.";
VLOG(4) << "Can't assign: assignee " << *buffer1 << " may interfere with " << *buffer2;
VLOG(4) << "assigned_buffer.start: " << live_range_1.start;
VLOG(4) << "assigned_buffer.end: " << live_range_1.end;
VLOG(4) << "live_range_2.start" << live_range_2.start;
VLOG(4) << "live_range_2.end" << live_range_2.end;
VLOG(4) << "Trying to assign " << hlo_buffer << " size " << assignment->HloBufferSize(hlo_buffer) << " to allocation: " << *allocation;
VLOG(4) << "Can't assign: buffer has color " << hlo_buffer.color() << " and allocation has color " << allocation->color() << ".";
VLOG(4) << "Can't assign: buffer is larger than allocation (" << assignment->HloBufferSize(hlo_buffer) << " > " << allocation->size() << ")";
VLOG(4) << "Can't assign: allocation is readonly";
VLOG(4) << "Can't assign: " << value->instruction()->ToString() << " cannot live out of the module";
VLOG(4) << "Can't assign: " << buffer_offset_size.first->instruction() << " cannot live out of the module";
VLOG(4) << "Can't assign: allocation is not reusable";
VLOG(4) << "Can't assign: assignee " << assigned_buffer << " live range interferes with " << new_value->ToShortString();
VLOG(4) << "Can't assign: assignee " << assigned_buffer << " may interfere with " << new_value->ToShortString();
VLOG(4) << "Can't assign: assignee " << assigned_buffer << " is used at copy instruction " << new_value->ToShortString();
VLOG(4) << "Can't assign: buffer " << hlo_buffer << "is live out and size not the same as allocation";
VLOG(3) << "Merging inplace " << instruction_buffer << " and " << operand_buffer;
VLOG(3) << "New allocation #" << allocation->index() << " for constant " << *hlo_buffer << " value ptr: " << value;
VLOG(3) << "New allocation #" << allocation->index() << " marked as entry computation parameter: " << *hlo_buffer;
VLOG(3) << "New allocation #" << allocation->index() << " for thread-local: " << *hlo_buffer;
VLOG(3) << "New allocation #" << allocation->index() << " for tuple-shaped buffer: " << *hlo_buffer;
VLOG(3) << "Reusing (operand) allocation #" << allocation->index() << " for: " << *hlo_buffer;
VLOG(3) << "Reusing allocation #" << allocation->index() << " for: " << *hlo_buffer;
VLOG(3) << "Delaying assignment of temp buffer: " << *hlo_value;
VLOG(3) << "New allocation #" << allocation->index() << " for: " << *hlo_buffer;
VLOG(3) << "Skip allocation for buffer: " << buffer;
VLOG(3) << "=================================================";
VLOG(3) << "Assigning buffer for " << *buffer;
VLOG(3) << "Created preset buffer allocation " << inserted_allocation->index() << ", color: " << inserted_allocation->color() << ", size: " << inserted_allocation->size();
VLOG(3) << "Preset allocation for value: " << value.ToShortString();
VLOG(1) << "Running whole-module heap simulation";
VLOG(2) << "Simulating heap for color " << color;
VLOG(1) << "Running per-computation heap simulation";
VLOG(2) << "Simulating heap for color " << color;
VLOG(1) << "Compute peak memory logical buffers";
LOG(FATAL) << "Unknown event kind: " << event.kind();
VLOG(4) << "Peak memory buffer:";
VLOG(4) << " " << value->ToString();
VLOG(1) << "Result size from heap simulator: " << result.heap_size;
VLOG(1) << "Ran heap simulation for allocation: ";
XLA_VLOG_LINES(2, allocation->ToString());
VLOG(1) << "Assigning buffers to module " << module->name();
XLA_VLOG_LINES(3, module->ToString());
XLA_VLOG_LINES(3, alias_analysis->ToString());
XLA_VLOG_LINES(3, alias_analysis->dataflow_analysis().ToString());
VLOG(1) << "Number of buffers to assign: " << alias_analysis->buffers().size();
VLOG(3) << "After coloring:";
XLA_VLOG_LINES(3, assignment->alias_analysis().dataflow_analysis().ToString());
VLOG(2) << "Running whole module heap simulation: " << run_whole_module_heap_simulation;
VLOG(3) << "maybe_live_out LogicalBuffer: " << *buffer;
VLOG(3) << "maybe_live_out BufferAllocation: " << *alloc;
XLA_VLOG_LINES(2, assignment->ToString());
XLA_VLOG_LINES(1, assignment->GetStats().ToString());
VLOG(1) << "Buffer assignment done.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/while_loop_constant_sinking.cc
VLOG(2) << "HLO module before WhileLoopConstantSinking:";
XLA_VLOG_LINES(2, module->ToString());
VLOG(2) << "HLO module after WhileLoopConstantSinking:";
XLA_VLOG_LINES(2, module->ToString());
VLOG(2) << "HLO module unchanged after WhileLoopConstantSinking";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/service.cc
LOG(INFO) << StrFormat( "XLA service %p initialized for platform %s (this does not guarantee " "that XLA will be used). Devices:", this, execute_backend_->platform()->Name());
LOG(INFO) << StrFormat(" StreamExecutor device (%d): %s, %s", i, description.name(), description.platform_version());
VLOG(1) << "XLA compile-only service constructed";
VLOG(1) << StrFormat("BuildExecutable on service %p", this);
VLOG(1) << "Computations:";
VLOG(1) << proto->name();
VLOG(1) << "running execute-graph-parallel request";
VLOG(3) << "ExecuteGraphParallel created HloModuleConfig computation layout: " << module_config->entry_computation_layout().ToString();
VLOG(1) << "successfully completed 'execute-graph-parallel' request";
VLOG(1) << StrFormat( "BuildExecutable on service %p with serialized module proto: %s", this, module_proto.name());
VLOG(1) << "running compile request";
VLOG(3) << "Compile created HloModuleConfig computation layout: " << module_config->entry_computation_layout().ToString();
VLOG(1) << "successfully completed 'compile' request";
VLOG(1) << "running execute request";
VLOG(1) << "successfully completed 'execute' request";
VLOG(1) << "successfully completed 'wait-for-execution' request";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/ar_crs_combiner.cc
VLOG(2) << "Replaced replicated all-reduce:" << ar->ToString();
VLOG(2) << "ArCrsPair matching pattern: " << pair.ToString();
VLOG(2) << "Replacing ArCrsPair: " << prev_pair.ToString() << " with ArCrsPair: " << pair.ToString();
VLOG(2) << "KeepProvablyEqualInstructionGroups. Checking AllReduce channel id: " << channel_id << "";
VLOG(2) << "KeepProvablyEqualInstructionGroups. Erased AllReduce " "channel id: " << channel_id << "";
VLOG(2) << "KeepProvablyEqualInstructionGroups. Checking AllReduce channel id: " << channel_id << "";
VLOG(2) << "KeepProvablyEqualInstructionGroups. Erased AllReduce " "channel id: " << channel_id << "";
LOG(FATAL) << "Unexpected instruction: " << next->ToShortString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/sort_simplifier.cc
VLOG(2) << "HLO module before SortSimplifier:";
XLA_VLOG_LINES(2, module->ToString());
VLOG(2) << "HLO module after SortSimplifier:";
XLA_VLOG_LINES(2, module->ToString());
VLOG(2) << "HLO module unchanged after SortSimplifier";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/map_inliner.cc
VLOG(10) << "inlining map({X ... Y}, op) => : op(X ... Y) with function " << root.ToShortString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/reshape_mover.cc
VLOG(5) << "Found first non-trivial reshape operand of " << hlo->ToString(HloPrintOptions().set_print_metadata(false)) << ":" << operand->ToString(HloPrintOptions().set_print_metadata(false));
VLOG(5) << "Adding reshape to kConstant operand";
VLOG(5) << "Adding transpose to kConstant operand";
VLOG(5) << "Cloning kRng operand with new shape";
VLOG(5) << "Using existing operand of kReshape or kTranspose";
VLOG(5) << "Changing broadcast from " << operand->ToString() << " to " << inst->ToString();
LOG(FATAL) << "Unexpected operand opcode during update: " << operand;
VLOG(3) << "** Sinking reshape or transpose: " << instruction->ToString(print_no_metadata) << "first reshape operand: " << first_reshape_operand->ToString(print_no_metadata) << "new operand shape: " << ShapeUtil::HumanString(new_operand_shape);
VLOG(3) << "Updating operand #" << i << ": " << operands[i]->ToString(print_no_metadata);
VLOG(3) << "Creating new reshape for new elementwise op: " << new_elementwise->ToString(print_no_metadata);
LOG(FATAL) << "Bad opcode";
VLOG(5) << "** Checking instruction: " << instruction->ToString(print_no_metadata);
VLOG(5) << "Operand shape differs from output shape; so preventing " "movementoperand: " << operand->ToString(print_no_metadata) << "instruction: " << instruction->ToString(print_no_metadata);
VLOG(5) << "Operand can trivially change shape: " << operand->ToString(print_no_metadata);
VLOG(5) << "Operand can't trivially change shape: " << operand->ToString(print_no_metadata);
VLOG(5) << "First reshape operand " << operand->ToString(print_no_metadata);
VLOG(5) << "Operand is an equivalent reshape of the first reshape operand " << operand->ToString(print_no_metadata);
VLOG(5) << "Operand is a reshape but is not equivalent to the first " "Reshape operand" << operand->ToString(print_no_metadata);
VLOG(5) << "All operands have easy shape changes: " << instruction->ToString(print_no_metadata);
VLOG(5) << "candidate " << instruction->ToString();
VLOG(2) << "Pre ReshapeMover HLO:";
XLA_VLOG_LINES(2, module->ToString());
XLA_VLOG_LINES(2, module->ToString());

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/bfloat16_normalization.cc
XLA_VLOG_LINES( 2, "BFloat16Normalization::Run(), before:" + module->ToString());

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/tuple_points_to_analysis.cc
XLA_VLOG_LINES(3, ToString());

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/hlo_module.cc
VLOG(2) << "CreateFromProto()";
XLA_VLOG_LINES(3, proto.DebugString());
LOG(FATAL) << error_message;
VLOG(2) << "Outlining the following instructions";
VLOG(2) << " " << instruction_to_outline->ToString();
VLOG(2) << "as a call " << call->ToString();
VLOG(2) << "to " << nested_computation->ToString();
LOG(ERROR) << "Post Order: " << computation->name() << " (" << computation->parent()->name() << ")";
LOG(ERROR) << "Computations: " << computation->name() << " (" << computation->parent()->name() << ")";
LOG(FATAL) << "Mismatch computation count: post_order=" << post_order.size() << " computation_count=" << computations_.size();
VLOG(1) << "Cloning module :" << name_ << " --> " << suffix << "";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/dynamic_dimension_inference.cc
VLOG(2) << "Input dim start: " << input_dim_start << " Input dim end: " << input_dim_end << " output dim start: " << output_dim_start << " output dim end: " << output_dim_end;
VLOG(2) << "input_dim_size: " << input_dim_size << " output_dim_size: " << output_dim_size;
VLOG(2) << "Param Config " << module->dynamic_parameter_binding().ToString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/call_inliner.cc
VLOG(1) << "Cloning HLO and adding to caller: " << hlo->ToString();
VLOG(1) << "Replacing all uses of " << call_->ToString() << " with new root " << new_root->ToString();
VLOG(1) << "Visiting node: " << node.ToString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/hlo_domain_remover.cc
VLOG(4) << "Applying domain normalization: " << ref_metadata->ToString();
VLOG(2) << "Applying domain-less normalization";
VLOG(4) << "Processing metadata domain: '" << remover_->kind_ << "'";
VLOG(5) << "Removing " << operand->name();
VLOG(5) << "Removing " << root->name();
VLOG(3) << "Removed " << removed_domains << " kDomain instructions of '" << remover_->kind_ << "' kind";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/local_service.cc
VLOG(3) << "Computation Layout: " << module_config->entry_computation_layout().ToString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/indexed_array_analysis.cc
VLOG(3) << "ComputeArrayForGather: indices are not scalar";
VLOG(3) << "ComputeArrayForGather: gather operations must elide " "start_index_map[0] and " "start_index_map[0] only";
VLOG(3) << "ComputeArrayForGather: slice_sizes[" << i << "] != source->shape().dimensions(" << i << ") -- " << source->shape().dimensions(i) << " vs. " << slice_sizes[i] << " with dim_numbers.collapsed_slice_dims(0) = " << dim_numbers.collapsed_slice_dims(0);
VLOG(3) << "For a reshape from [" << StrJoin(operand_shape, ",") << "] to [" << StrJoin(result_shape, ",") << "] passthrough indices are [" << StrJoin(result_strings, ",") << "] (legend: `result`->`operand`)";
VLOG(3) << "FindSourcePositionForPassthroughResultDim([" << StrJoin(operand_shape, ",") << "], [" << StrJoin(result_shape, ",") << "], " << source_passthrough_dim << ")";
LOG(FATAL) << "Did not find source dim in " << ToString(operand);
VLOG(3) << "FoldReshapeOfGather(" << ToString(operand) << ")";
VLOG(3) << "FoldReshapeOfGatherNoDegenerateDims(" << ToString(scalar_indexed) << ")";
VLOG(3) << "Not all output dims are passthrough dims " << ToString(scalar_indexed);
VLOG(3) << "Could not compute the source dim for the new scalar indexed " "node: scalar_indexed_source_shape = [" << StrJoin(scalar_indexed_source_shape.dimensions(), ",") << "] and new_scalar_indexed_source_shape = [" << StrJoin(new_scalar_indexed_source_shape, ",") << "]";
VLOG(3) << tag << ": multiple or no non-contracting non-batch dimensions";
VLOG(3) << tag << ": output dims != the lhs non-contracting non-batch dim";
VLOG(3) << tag << ": source dim is not in the low two dims, won't be able to form " "a matmul";
VLOG(3) << "ComputeArrayForDotWithIndexedLhs(" << ToString(lhs) << " " << ToString(rhs);
VLOG(3) << "ComputeArrayForDotWithIndexedRhs(" << ToString(lhs) << " " << ToString(rhs);
VLOG(3) << "ComputeArrayForDot(" << ToString(lhs) << " " << ToString(rhs);
VLOG(2) << instr->ToString() << " -> " << analysis.ToString(t);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/stream_pool.cc
VLOG(1) << stream->DebugStreamPointers() << " StreamPool reusing existing stream";
VLOG(1) << stream->DebugStreamPointers() << " stream was not ok, StreamPool deleting";
VLOG(1) << stream->DebugStreamPointers() << " StreamPool created new stream";
VLOG(1) << stream->DebugStreamPointers() << " StreamPool returning ok stream";
VLOG(1) << stream->DebugStreamPointers() << " StreamPool deleting !ok stream";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/hlo_rematerialization.cc
VLOG(3) << "InsertBeforeInstructions: " << to_insert->instruction->name() << " before {" << absl::StrJoin(before_instructions, ", ", [](string* out, Item* item) { absl::StrAppend(out, item->instruction->name()); }) << "}";
VLOG(3) << "InsertAfterInstructions: " << to_insert->instruction->name() << " after {" << absl::StrJoin(after_instructions, ", ", [](string* out, Item* item) { absl::StrAppend(out, item->instruction->name()); }) << "}";
VLOG(3) << "InsertBefore: " << item->instruction->name() << " before " << before->instruction->name();
XLA_VLOG_LINES(10, ToString());
VLOG(3) << "BeginInstruction " << instruction->name();
VLOG(3) << " Buffer " << buffers_.at(buffer_id).ToString() << " is now live.";
VLOG(3) << " memory usage = " << memory_usage_;
VLOG(10) << ToString();
VLOG(3) << "EndInstruction " << in_progress_item_->instruction->name();
VLOG(3) << " " << buffer.ToString() << " is now dead.";
VLOG(3) << " " << buffer.ToString() << " is immediately dead.";
VLOG(3) << " memory usage = " << memory_usage_;
VLOG(10) << ToString();
LOG(WARNING) << "Unplaced item or in progress item being checked for " "rematerialization.";
VLOG(3) << "AddRematerializedInstruction: original_instruction = " << original_item->instruction->name() << ", remat_instruction = " << remat_item->instruction->name();
VLOG(3) << " memory usage = " << memory_usage_;
XLA_VLOG_LINES(10, ToString());
VLOG(5) << "Picking candidate block with size in [" << min_block_size << ", " << max_block_size << "]";
VLOG(3) << "candidate " << candidate->name() << "(" << candidate->ToShortString() << ")" << " now best when compressed into " << compact_shape.ToString(true);
VLOG(5) << "Candidate block of size " << block.size() << " starting from " << block[0]->instruction->name() << ", memory reduced " << memory_reduced << ", cost per byte " << cost;
VLOG(5) << "Candidate block of size " << block.size() << " starting from " << block[0]->instruction->name() << " now best";
VLOG(2) << " Replacing use of " << best->name() << " in " << user->name() << " with " << remat->name();
VLOG(2) << best->name() << " is now dead";
VLOG(5) << "Transposing instruction " << best->name() << " (saving " << HumanReadableNumBytes(memory_tracker->MemoryReducedIfCompressed( best_item, compact_shape)) << ") to" << compact_shape.ToString(true);
VLOG(5) << " Replacing use of " << best->name() << " in " << user->name() << " with " << uncompressed->name();
VLOG(1) << "Compressing instruction " << best->name() << " (saving " << HumanReadableNumBytes(memory_tracker->MemoryReducedIfCompressed( best_items[0], best_strategy.compact_shape)) << ")";
VLOG(1) << "Rematerializing computation " << computation->name() << " with limit " << HumanReadableNumBytes(memory_limit_bytes);
VLOG(1) << "peak memory usage is " << HumanReadableNumBytes(computation_peak_memory_.at(computation));
VLOG(2) << "Program point at " << instruction->name() << ", memory usage = " << memory_tracker.memory_usage() << ", callee usage = " << callee_usage << ", [" << instruction_index << "/" << instruction_list.size() << "]";
VLOG(2) << "Over memory limit at instruction " << instruction->name() << ", using " << HumanReadableNumBytes(memory_tracker.memory_usage() + callee_usage) << ", limit is " << HumanReadableNumBytes(memory_limit_bytes);
VLOG(1) << "memory_usage after rematerialization = " << HumanReadableNumBytes(memory_tracker.memory_usage());
VLOG(1) << "Memory usage still over the limit (" << (memory_tracker.memory_usage() + callee_usage) << " > " << memory_limit_bytes << "). Rematerializing computations called by " << instruction->name();
VLOG(3) << "peak memory usage = " << HumanReadableNumBytes(peak_memory);
VLOG(1) << " peak memory usage now " << HumanReadableNumBytes(peak_memory) << " (was " << HumanReadableNumBytes(computation_peak_memory_.at(computation)) << ")";
VLOG(1) << "HloRematerialization() with memory limit of " << HumanReadableNumBytes(memory_limit_bytes_);
XLA_VLOG_LINES(3, "Before HloRematerialization:" + module->ToString());
VLOG(1) << "Adjusted memory limit accounting for output (" << HumanReadableNumBytes(module_output_size) << "): " << HumanReadableNumBytes(adjusted_memory_limit_bytes);
VLOG(1) << "Peak memory usage of module (before): " << HumanReadableNumBytes(before_peak_memory);
VLOG(1) << "Rematerialized " << instructions_rematerialized_ << " instructions in module " << module->name() << "; " << net_instructions_added_ << " net instructions added";
VLOG(1) << "Peak memory usage of module now " << HumanReadableNumBytes(current_peak_memory) << " (" << current_peak_memory << " bytes), was " << HumanReadableNumBytes(before_peak_memory) << " (" << before_peak_memory << " bytes)";
VLOG(1) << "Reduced peak memory by " << HumanReadableNumBytes(reduced_peak_memory) << " (" << reduced_peak_memory << " bytes)";
XLA_VLOG_LINES(5, "After HloRematerialization:" + module->ToString());
LOG(WARNING) << absl::StrFormat( "Can't reduce memory use below %s (%d bytes) by rematerialization; " "only reduced to %s (%d bytes)", HumanReadableNumBytes(memory_limit_bytes_), memory_limit_bytes_, HumanReadableNumBytes(current_peak_memory), current_peak_memory);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/interpreter/executor.cc
LOG(WARNING) << "Host callback failed: " << s;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/interpreter/platform.cc
LOG(FATAL) << "not yet implemented: register executor trace listener";
LOG(FATAL) << "not yet implemented: unregister executor trace listener";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/interpreter/executable.cc
VLOG(1) << "Execute " << module().name();
VLOG(2) << "-- argument " << a;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/interpreter/compiler.cc
VLOG(1) << "Run hlo passes on graph " << hlo_module->name();
VLOG(1) << "Run backend " << hlo_module->name();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/gpu/multi_output_fusion.cc
VLOG(3) << "Looking at producer " << producer->name() << " and its consumer " << consumer->name();
VLOG(3) << "Consumer " << consumer->name() << " is not eligible as multi-output fusion root.";
VLOG(3) << producer->name() << " and " << consumer->name() << " are not fusible.";
VLOG(3) << producer->name() << " would introduce a cycle when fused.";
VLOG(3) << producer->name() << " and " << consumer->name() << " would be too large of a fusion.";
VLOG(3) << "Considering " << (*i)->name();
VLOG(3) << "Considering " << (*i)->name() << " and " << (*j)->name();
VLOG(2) << "Fuse siblings " << (*i)->name() << " and " << (*j)->name();
VLOG(3) << producer->name() << " is a constant.";
VLOG(2) << "Fuse producer " << producer->name() << " into its consumer " << consumer_for_fusion->name();
VLOG(2) << "Fuse producer " << producer->name() << " and its consumer " << consumer_for_fusion->name() << " into " << input_fusion->name();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/gpu/stream_assignment.cc
VLOG(2) << "Assign stream #" << stream_num << " to " << hlo->ToString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/gpu/ir_emission_utils.cc
LOG(FATAL) << "Invalid triple " << target_triple.str();
LOG(FATAL) << "Invalid triple " << target_triple.str();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/gpu/gpu_debug_info_manager.cc
LOG(ERROR) << "Cannot find debug info for module: " << module_id;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/gpu/cholesky_thunk.cc
VLOG(3) << "type=" << PrimitiveType_Name(type_) << " uplo=" << se::blas::UpperLowerString(uplo_) << " batch_size=" << batch_size_ << " n=" << n_ << " a=" << a_buffer_.ToString() << " workspace=" << workspace_buffer_.ToString() << " info=" << info_buffer_.ToString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/gpu/reduction_layout_normalizer.cc
VLOG(5) << "Input: " << reduce->ToString();
VLOG(5) << "Processing logical dimension " << logical_dim << " of size " << dim_size;
VLOG(5) << "logical_reduce_dim = " << logical_reduce_dim << ", " << "physical_reduce_dim = " << physical_reduce_dim;
VLOG(5) << "Reduction input: " << canonical_reduce_input->ToString();
VLOG(5) << "Generated new reduction: " << new_reduce->ToString();
VLOG(5) << "Generated output: " << new_reduce->ToString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/gpu/gpu_executable.cc
LOG(WARNING) << "PROFILING: profiling is enabled";
VLOG(2) << "Executing the thunk for " << thunk->hlo_instruction()->ToString() << " on stream " << stream_no;
VLOG(3) << "Resolved global " << llvm_ir::ConstantBufferAllocationToGlobalName(allocation) << " to " << global.opaque();
VLOG(3) << "H2D memcpy for constant with shape " << ShapeUtil::HumanString(literal.shape());
VLOG(4) << "Looking at: " << sources.values()[0]

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/gpu/gpu_conv_padding_legalization.cc
VLOG(1) << "Canonicalizing forward conv";
VLOG(1) << "Replacing: " << conv->ToString() << "with: " << new_conv->ToString();
VLOG(1) << "Canonicalizing backward filter conv";
VLOG(1) << "Replacing: " << backward_conv->ToString() << "with: " << new_backward_conv->ToString();
VLOG(1) << "Canonicalizing backward input conv";
VLOG(1) << "Replacing: " << backward_conv->ToString() << "with: " << new_tuple->ToString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/gpu/nccl_all_reduce_thunk.cc
VLOG(3) << absl::StreamFormat("Destroying comm %p", *comm_);
VLOG(3) << absl::StreamFormat( "Initializing nccl comms for participant devices {%s}", absl::StrJoin(devices_, ", "));
VLOG(3) << absl::StreamFormat("Device %d assigned ncclComm %p", devices_[i], raw_comms[i]);
VLOG(3) << "Primary initializing accounting data.";
VLOG(1) << "SubmitParticipant failing because clique failed to initialize: " << clique->status().ToString();
VLOG(3) << "Performing all reduce from device ordinal: " << participant.device_ordinal;
VLOG(3) << "Using stream pointer: " << cu_stream << " on device: " << participant.device_ordinal;
VLOG(3) << absl::StreamFormat( "Calling ncclAllReduce(send_buffer=%p, recv_buffer=%p, count=%d, " "comm=%p, stream=%p)", send_buffer, recv_buffer, buffer.element_count, static_cast<const void*>(comm), cu_stream);
VLOG(3) << "Done performing all reduce for ordinal: " << participant.device_ordinal;
VLOG(3) << "This thread done with all-reduce op.";
VLOG(1) << "Starting NcclAllReduceThunk.";
VLOG(2) << "Rendezvous key: " << rendezvous_key.ToString() << ", participating replicas: " << absl::StrJoin(participating_replicas, ", ");

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/gpu/horizontal_fusion.cc
VLOG(2) << "Reject unsupported fusion instr " << instr->ToString();
VLOG(2) << "Reject maybe illegal instr " << instr->ToString() << "; including it may create cycles in HLO.";
VLOG(2) << "Reject may-not-be profitable fusion instr " << instr->ToString();
VLOG(2) << "Reject non-row-major fusion instr " << instr->ToString();
VLOG(2) << "Find a fusion candidate " << instr->ToString();
XLA_VLOG_LINES(3, computation_->ToString());
VLOG(2) << "Run horizontal fusion.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/gpu/gpu_layout_assignment.cc
VLOG(2) << "Using heuristic to figure out layouts for " << instr->ToString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/gpu/collective_permute_thunk.cc
VLOG(3) << "Begin: " << desc_fn();
VLOG(3) << "Finished: " << desc_fn();
LOG(ERROR) << "This thread has been waiting for " << timeout.count() << "ms for and may be stuck: " << desc_fn();
LOG(ERROR) << "Thread is unstuck! Warning above was a false-positive. " "Perhaps the timeout is too short: " << desc_fn();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/gpu/fft_thunk.cc
LOG(FATAL) << "unsupported fft type";
LOG(FATAL) << "unknown fft type";
VLOG(3) << "FFT type: " << FftTypeToString(fft_type_);
VLOG(3) << "Input shape: " << ShapeUtil::HumanStringWithLayout(input_shape_);
VLOG(3) << "Output shape: " << ShapeUtil::HumanStringWithLayout(output_shape_);
LOG(FATAL) << "unsupported fft type";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/gpu/buffer_comparator.cc
LOG(WARNING) << compiled_ptx_or.status().ToString() << "Relying on driver to perform ptx compilation. " << "Setting XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda " << " or modifying $PATH can be used to set the location of ptxas" << "This message will only be logged once.";
LOG(ERROR) << "Difference at " << i << ": " << original_lhs << " vs " << original_rhs;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/gpu/gpu_conv_rewriter.cc
VLOG(1) << "Forward convolution's window " << conv->window().ShortDebugString() << " should have stride of 1.";
VLOG(1) << "Forward convolution's window " << conv->window().ShortDebugString() << " should have no base (LHS) dilation.";
VLOG(1) << "Padding low should be non-negative.";
VLOG(1) << "Window reversal field not supported";
VLOG(1) << conv->ToString() << " is a regular forward convolution. No need " "to fold it to a backward filter convolution.";
VLOG(1) << conv->ToString() << " is a depthwise forward convolution. No need to fold to " "backward filter.";
LOG(ERROR) << "Fusing this pattern to backward filter convolution would cause " "negative padding (" << dim->padding_high() << ") on right/bottom of the weight gradients, which is not " "supported by GpuConvPaddingLegalization (b/32744257). " "Falling back to " "unfused convolution for instruction: " << conv->ToString();
VLOG(1) << "Can't match to backwards convolution. Either filter is not " "kReverse, or it's not a base-dilated conv with a 1x1 or " "constant filter.";
VLOG(1) << "Forward convolution's window " << conv->window().ShortDebugString() << " should have stride of 1.";
VLOG(1) << "Forward convolution's window " << conv->window().ShortDebugString() << " should have no window dilation.";
VLOG(1) << "Window reversal field not supported";
LOG(ERROR) << "The low padding of the backward convolution would be negative (" << backward_padding_low << "), which isn't supported by GpuConvPaddingLegalization " "for now (b/32744257).";
LOG(ERROR) << "Fusing this pattern to backward convolution would cause " "negative padding (" << dim->padding_high() << ") on right/bottom of the activations, which is not " "supported by GpuConvPaddingLegalization (b/32744257). " "Falling back to unfused convolution for instruction: " << conv->ToString();
VLOG(1) << "Replacing convolution " << conv->ToString() << " with " << custom_call->ToString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/gpu/cudnn_pad_for_convolutions.cc
VLOG(2) << "Padded features of " << conv->ToString() << ", replaced with " << new_conv->ToString();
VLOG(3) << "Not padding convolution; doing so would change input / result " "shape from " << ShapeUtil::HumanString(old_shape) << " to " << ShapeUtil::HumanString(new_shape) << ", a size increase of " << new_bytes / static_cast<double>(old_bytes) << "x > " << kMaxBytesTouchedIncrease << "x: " << conv->ToString();
VLOG(3) << "No need to pad features of " << conv->ToString();
VLOG(3) << "Not padding convolution; doing so would change input / result " "shape from " << ShapeUtil::HumanString(old_shape) << " to " << ShapeUtil::HumanString(new_shape) << ", a size increase of " << new_bytes / static_cast<double>(old_bytes) << "x > " << kMaxBytesTouchedIncrease << "x: " << conv->ToString();
VLOG(3) << "No need to pad features of " << conv->ToString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/gpu/buffer_allocations.cc
LOG(FATAL) << "Multiple temporary buffers detected. BufferAssigner " << "must guarantee at most one temporary buffer.";
VLOG(2) << "Buffer " << i << " -> " << buf.opaque() << " (" << buf.size() << "B)";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/gpu/gpu_conv_algorithm_picker.cc
LOG(ERROR) << "Device: " << desc.name();
LOG(ERROR) << "Platform: " << desc.platform_version();
LOG(ERROR) << "Driver: " << desc.driver_version();
LOG(ERROR) << "Runtime: " << desc.runtime_version();
LOG(ERROR) << "cudnn version: " << v.major_version() << "." << v.minor_version() << "." << v.patch();
LOG(ERROR) << absl::StreamFormat( "Detected cudnn out-of-bounds write in conv %s buffer! This is likely a " "cudnn bug. We will skip this algorithm in the future, but your GPU " "state may already be corrupted, leading to incorrect results. Within " "Google, no action is needed on your part. Outside of Google, please " "ensure you're running the latest version of cudnn. If that doesn't fix " "the problem, please file a bug with this full error message and we'll " "contact nvidia.", name);
LOG(ERROR) << redzone_check.RedzoneFailureMsg();
LOG(ERROR) << "HloInstruction " << instr->ToString();
VLOG(2) << "Cache hits: " << cache_hits;
VLOG(2) << "Cache misses: " << cache_misses;
LOG(INFO) << "Omitted potentially buggy algorithm " << AlgorithmToString(alg) << " for conv " << instr->ToString();
VLOG(3) << "Trying algorithm " << AlgorithmToString(alg) << " for " << instr->ToString();
LOG(ERROR) << "To blacklist this algorithm for this convolution, " "copy-paste the following " "proto to the blacklist file pointed by XLA_FLAGS " "--xla_gpu_algorithm_blacklist_path=" << GetDebugOptionsFromFlags().xla_gpu_algorithm_blacklist_path() << " : " << proto.ShortDebugString();
LOG(ERROR) << "Unable to compare " << AlgorithmToString(first_algorithm) << " against " << AlgorithmToString(alg) << " for " << instr->ToString() << ": " << compare_result.status();
LOG(ERROR) << "Results mismatch between different convolution algorithms. " "This is likely a bug/unexpected loss of precision in cudnn." << instr->ToString() << " for " << AlgorithmToString(first_algorithm) << " vs " << AlgorithmToString(alg);
VLOG(1) << "Full module on failure: " << instr->GetModule()->ToString();
VLOG(1) << "Autotuning result: " << log.ShortDebugString();
VLOG(3) << "Trying algorithm " << AlgorithmToString(alg) << " for " << instr->ToString();
LOG(WARNING) << "Failed to determine best cudnn convolution algorithm: " << best_algo_or.status() << "Convolution performance may be suboptimal.";
VLOG(2) << "Setting cudnn conv to use algorithm " << best_algo.conv().algorithm() << " and " << NumBytesToString(best_algo.scratch_bytes()) << " of scratch memory: " << instr->ToString() << " tensor_ops_enabled: " << best_algo.conv().tensor_ops_enabled();
VLOG(2) << "Replacing convolution " << instr->ToString() << " with " << new_call->ToString();
VLOG(2) << "Convolution auto-tuning disabled, GpuConvAlgorithmPicker " "returning early.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/gpu/outfeed_thunk.cc
VLOG(2) << "Outfeeding from GPU: " << hlo_instruction()->ToString();
VLOG(2) << "Outfeeding from GPU complete";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/gpu/reduction_dimension_grouper.cc
VLOG(4) << "Input: " << reduce->ToString();
VLOG(5) << "Processing dimension " << logical_dim << " of size " << shape.dimensions(logical_dim);
VLOG(5) << "This and consecutive dimension are reduced, merging";
VLOG(5) << "Generated new reduction: " << new_reduce->ToString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc
VLOG(3) << "Starting autotune of GemmThunk " << gemm->ToString();
VLOG(2) << "cublas gemm algorithm " << algorithm << " took " << profile_result.elapsed_time_in_ms() << "ms" << std::endl;
LOG(ERROR) << "Detected cuBLAS out-of-bounds write in gemm buffer";
LOG(ERROR) << "Results mismatch between different GEMM algorithms. " << "This is likely a bug/unexpected loss of precision " << "in cuBLAS.";
VLOG(1) << "Unable to autotune cuBLAS gemm on stream " << stream << " none of the " << algorithms.size() << " ran successfully";
VLOG(2) << "Autotuning cache hits/(hits + misses): " << cache_hits << "/" << autotuning_requests;
VLOG(4) << "Autotuning cache hit, using algorithm: " << (it->second.has_value() ? absl::StrCat(it->second.value()) : "<generic>");
VLOG(4) << "Autotuning cache miss";
VLOG(2) << "Batch size is non-singular, using generic algorithm";
VLOG(2) << "GEMM auto-tuning disabled, GemmAlgorithmPicker returning early";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/gpu/gpu_transfer_manager.cc
VLOG(2) << "Transferring literal to infeed with shape: " << ShapeUtil::HumanString(shape);
VLOG(2) << "Infeed data transferred";
VLOG(2) << "Queued infeed data on stream " << stream;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/gpu/ir_emitter.cc
VLOG(2) << "HandleBitcast: " << bitcast->ToString();
VLOG(2) << "HandleAddDependency: " << add_dependency->ToString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/gpu/infeed_thunk.cc
VLOG(2) << "Infeeding to GPU: " << hlo_instruction()->ToString();
VLOG(2) << "Infeeding to GPU complete";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/gpu/tree_reduction_rewriter.cc
VLOG(3) << "Input: " << hlo->ToString();
VLOG(3) << "Input shape: " << input_shape.ToString();
VLOG(3) << "reduce_batch_dimension = " << reduce_batch_dimension;
VLOG(3) << "reduced_input_dimension: " << reduced_input_dimension;
VLOG(1) << "Splitting batched dimension reduce into a separate reduction";
VLOG(3) << "atomic_free_bound: " << atomic_free_bound;
VLOG(3) << "Base case: dimensions fit";
VLOG(3) << "reduced_dim_size = " << reduced_dim_size;
VLOG(3) << "Generated padded shape: " << padded_shape.ToString();
VLOG(1) << "Inner reduction: " << inner_reduce->ToString()
VLOG(1) << "Generated: " << out->ToString()
VLOG(5) << "Rewriter input: " << module->ToString();
VLOG(5) << "Rewriter output: " << module->ToString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/gpu/triangular_solve_thunk.cc
LOG(ERROR) << "Invalid triangular solve transpose value " << options.transpose_a();
VLOG(3) << "uplo=" << se::blas::UpperLowerString(uplo_) << " side=" << se::blas::SideString(side_) << " diagonal=" << se::blas::DiagonalString(unit_diagonal_) << " batch_size=" << batch_size_ << " m=" << m_ << " n=" << n_ << " a_batch_stride=" << a_batch_stride_ << " b_batch_stride=" << b_batch_stride_;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/gpu/cudnn_fused_conv_rewriter.cc
VLOG(1) << "Replacing convolution " << conv->ToString() << " with " << new_conv->ToString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/gpu/while_thunk.cc
VLOG(3) << "Executing condition computation";
VLOG(3) << "condition_result = " << condition_result;
VLOG(3) << "Executing body computation";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/gpu/for_thunk.cc
VLOG(2) << "Executing ForThunk with " << loop_limit_ << " iters for " << (hlo_instruction() ? hlo_instruction()->ToString() : "<null>");

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/gpu/cusolver_rewriter.cc
VLOG(1) << "Replacing " << instruction->ToString() << " with " << custom_call->ToString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/gpu/cudnn_batchnorm_rewriter.cc
VLOG(1) << "Not rewriting op with non-F32 element type: " << batch_norm->ToString();
VLOG(1) << "Not rewriting op with non-F32 element type: " << batch_norm->ToString();
VLOG(1) << "Not rewriting op with non-F32 element type: " << batch_norm->ToString();
VLOG(2) << "CudnnBatchNormRewriter::Run(), before:";
XLA_VLOG_LINES(2, module->ToString());
VLOG(2) << "CudnnBatchNormRewriter::Run(), after:";
XLA_VLOG_LINES(2, module->ToString());

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/gpu/amdgpu_compiler.cc
VLOG(2) << "Found ROCm-Device-Libs dir " << potential_rocdl_dir;
VLOG(2) << "Unable to find potential ROCm-Device-Libs dir " << potential_rocdl_dir;
LOG(WARNING) << "Couldn't get AMDGPU ISA version for device; assuming gfx803.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/gpu/stream_executor_util.cc
LOG(FATAL) << "Unexpected type";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/gpu/kernel_thunk.cc
VLOG(3) << "Launching " << kernel->name();
VLOG(3) << " Arg: alloc #" << arg->index() << ": " << buf.opaque() << " (" << buf.size() << "B)";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/gpu/gpu_compiler.cc
VLOG(1) << "HLO memory read+written: " << tensorflow::strings::HumanReadableNumBytes( cost_analysis.bytes_accessed());

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/gpu/fusion_merger.cc
VLOG(1) << "FusionInstructionMerger EXIT" << " computation: " << computation_->name() << " total_visited: " << total_visited_ << " total_merged: " << total_merged_ << " merge failures { " << " no_users: " << num_fail_no_users_ << " not_loop_fusion: " << num_fail_not_loop_fusion_ << " merge_all_users: " << num_fail_merge_all_users_ << " expensive_instruction: " << num_fail_expensive_fused_instruction_ << " net_bytes_transferred: " << num_fail_net_bytes_transferred_ratio_ << " inefficient_fusion_emitter: " << num_fail_inefficient_fusion_emitter_ << " fusion_too_large: " << num_fail_fusion_too_large_ << " }";
VLOG(3) << "Not merging " << fusion->name() << ": Has no users.";
VLOG(3) << "Not merging " << fusion->name() << ": Is not loop fusion.";
VLOG(3) << "Not merging " << fusion->name() << ": Some of its users are not loop/input fusion kernels.";
VLOG(3) << "Not merging " << fusion->name() << ": Contains one or more expensive instructions.";
VLOG(3) << "Not merging " << fusion->name() << ": merged-to-current-bytes-ratio of " << merged_to_current_bytes_ratio << " is not favorable.";
VLOG(3) << "Not merging " << fusion->name() << ": Contains one or more users where fusing would cause " "inefficiencies in the fusion emitter.";
VLOG(3) << "Not merging " << fusion->name() << ": Contains one or more users where fusing would cause " "the fusion to have too many parameters.";
VLOG(2) << "Merged fusion instruction: " << fusion->name() << " merged_to_current_bytes_ratio: " << merged_to_current_bytes_ratio << " into users { " << absl::StrJoin(users, ", ", [](string* out, HloInstruction* user) { absl::StrAppend(out, user->name()); }) << " }";
VLOG(2) << "FusionMerger for module: " << module->name();
VLOG(1) << "Before running FusionInstructionMerger for computation: " << computation->name();
XLA_VLOG_LINES(3, computation->ToString());
VLOG(1) << "After running FusionInstructionMerger for computation: " << computation->name() << " changed: " << changed;
XLA_VLOG_LINES(3, computation->ToString());

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/gpu/hlo_to_ir_bindings.cc
VLOG(2) << "Binding " << hlo.ToString();
VLOG(2) << "Marking " << hlo.name() << " as invariant within " << consumer.name();
VLOG(2) << "Unbinding " << hlo_to_unbind->ToString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/gpu/target_util.cc
LOG(FATAL) << "Unexpected type while getting device function name.";
LOG(FATAL) << "Unexpected type while getting device function name.";
LOG(FATAL) << "Invalid triple " << target_triple.str();
LOG(FATAL) << "Invalid triple " << target_triple.str();
LOG(FATAL) << "Invalid triple " << target_triple.str();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/gpu/gemm_thunk.cc
VLOG(3) << "Running GEMM thunk on instruction: " << hlo_instruction();
LOG(FATAL) << "Unsupported type.";
VLOG(2) << "Executing a GemmThunk";
LOG(FATAL) << "Unsupported type.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/gpu/cusolver_context.cc
LOG(FATAL) << "Invalid value of blas::UpperLower.";
LOG(ERROR) << "cusolverDnDestroy failed: " << status;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/gpu/cublas_gemm_pad_for_tensor_cores.cc
VLOG(3) << "old shape: " << lshape << " " << rshape << " " << result_shape;
VLOG(3) << "new shape: " << new_lshape << " " << new_rshape << " " << new_result_shape;
LOG(ERROR) << "Dot is not canonical: Expected all dimensions but 2 to be " "batch_dimensions.";
LOG(ERROR) << "Dot is not canonical: Expected batch dimensions to be all " "dimensions except for the last 2 ones.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/gpu/parallel_loop_emitter.cc
VLOG(3) << "EmitIndexAndSetExitBasicBlock unroll_factor " << unroll_factor_;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/gpu/gpu_conv_runner.cc
VLOG(3) << "Convolution Algorithm: " << params.algorithm.algorithm()->algo_id();
VLOG(3) << "tensor_ops_enabled: " << params.algorithm.algorithm()->tensor_ops_enabled();
VLOG(3) << "Convolution kind: " << CudnnConvKindToString(params.kind);
VLOG(3) << "input shape: " << ShapeUtil::HumanStringWithLayout(*input_shape);
VLOG(3) << "filter shape: " << ShapeUtil::HumanStringWithLayout(*filter_shape);
VLOG(3) << "Output shape: " << ShapeUtil::HumanStringWithLayout(*output_shape);
VLOG(3) << "Window: { " << window.ShortDebugString() << " }";
VLOG(3) << "Dim nums: { " << dnums.ShortDebugString() << " }";
LOG(FATAL) << conv->ToString();
LOG(FATAL) << conv->ToString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc
LOG(WARNING) << msg;
LOG(WARNING) << "Searched for CUDA in the following directories:";
LOG(WARNING) << " " << dir;
LOG(WARNING) << "You can choose the search directory by setting xla_gpu_cuda_data_dir " "in HloModule's DebugOptions. For most apps, setting the environment " "variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.";
VLOG(2) << "Looking for libdevice at " << libdevice_dir;
VLOG(2) << "Found libdevice dir " << libdevice_dir;
LOG(WARNING) << "Couldn't read CUDA driver version.";
LOG(WARNING) << "*** WARNING *** Invoking the PTX->SASS JIT from driver version " << se::cuda::DriverVersionToString(version) << ", which is older than 396.20.0. These versions are known to " "miscompile XLA code, leading to incorrect results or " "invalid-address errors.XLA only uses the driver JIT if it " "cannot find ptxas; you don't need to update your driver if " "you can point XLA to ptxas 9.2.88 or newer.";
VLOG(0) << "RunBackend() - Will load PTX from file: " << filename;
VLOG(0) << "RunBackend() - For module with prefix '" << prefix << "', we did not found a PTX file to load.";
LOG(WARNING) << "Couldn't get compute capability for device; assuming sm_20.";
VLOG(2) << "Libdevice dir = " << libdevice_dir << "";
VLOG(2) << "Compiled PTX size:" << ptx.size() << " CUBIN size: " << cache_value->cubin_data.size();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/gpu/alias_passthrough_params.cc
VLOG(2) << "Parameter " << root->operand(i)->parameter_number() << " with shape " << root->operand(i)->shape().ToString() << " in module " << module->name() << " is passed-through to root tuple element " << i << ": " << root->shape().ToString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/gpu/partition_assignment.cc
LOG(WARNING) << "Attempting to calculate launch dimensions for GPU " "without full information about its capabilities. " "StreamExecutor's PopulateDeviceDescription should be " "updated for this device.";
VLOG(2) << "Update # of threads per block to the element count (" << threads_per_block << ") because the latter is smaller.";
VLOG(2) << absl::StrFormat( "Initialized the block count to ceil(# of elements / threads per " "block) = ceil(%d/%d) = %d", num_elements, threads_per_block, block_count);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/gpu/ir_emitter_unnested.cc
LOG(FATAL) << "Bad opcode for input fusion: " << fusion->fused_expression_root()->opcode();
VLOG(2) << "AllReduce; replica count: " << hlo_module_config_.replica_count() << "; operand count: " << crs->operand_count() << "; NCCL is enabled: " << NcclAllReduceThunk::NcclIsEnabled();
VLOG(1) << "Couldn't find buffer for " << instr->ToString() << " at index " << index.ToString();
LOG(FATAL) << "Multiple temp buffers found, but only one is allowed!";
VLOG(3) << "Buffer for " << instr->ToString() << " at " << index.ToString() << " is found in slice " << slice.ToString() << " at GTE index " << gte_index.ToString();
LOG(FATAL) << "Opcode " << inst->opcode() << " should not need an initializer.";
VLOG(3) << bindings_.ToString();
VLOG(3) << "EmitTargetElementLoopInThunk " << ShapeUtil::HumanStringWithLayout(hlo.shape()) << " for unroll_factor " << unroll_factor;
VLOG(10) << "Emit prologue for reduction: " << unnested_hlo->ToString();
VLOG(10) << "Emit prologue for reduction: " << reduce_inst->ToString();
VLOG(10) << "Emit tile element for reduce " << unnested_hlo->ToString();
VLOG(3) << "Added shmem buffer for parameter " << id << ": " << llvm_ir::DumpToString(*param_shmem_buffers[id]);
VLOG(10) << "Input not safe for shmem transpose " << input->ToString();
VLOG(3) << "EmitHlo021Tile Emitting hlo tile 0-2-1" << hlo->ToString();
VLOG(10) << "is_row_reduction " << reduction_dimensions.is_row_reduction << " " << reduction_dimensions.dimensions[0] << " " << reduction_dimensions.dimensions[1] << " " << reduction_dimensions.dimensions[2];
VLOG(10) << "Emitting reduction to vector " << unnested_hlo->ToString();
VLOG(3) << "Emitted initializer for constant with shape " << ShapeUtil::HumanString(literal.shape());
VLOG(10) << "Emitting slice input fusion for " << unnested_hlo->ToString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/dump_ir_pass.cc
LOG(FATAL) << "Unable to open " << output_filename_ << " to dump LLVM IR: " << ec_.message();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/utils.cc
LOG(FATAL) << diagnostic->getFilename().str() << ":" << diagnostic->getLineNo() << ":" << diagnostic->getColumnNo() << ": " << diagnostic->getMessage().str();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc
LOG(WARNING) << "Unknown compute capability (" << compute_capability.first << ", " << compute_capability.second << ") ." << "Defaulting to telling LLVM that we're compiling for sm_" << sm_version;
LOG(FATAL) << "Unable to find Target for triple '" << triple.str() << "'" << " -- " << error;
LOG(FATAL) << "opening bitcode file for writing: " << error_code.message();
LOG(ERROR) << "bitcode module is required by this HLO module but was " "not found at " << bitcode_path;
LOG(WARNING) << "libdevice is required by this HLO module but was not found at " << libdevice_path;
VLOG(1) << "Linking with libdevice from: " << libdevice_path;
LOG(WARNING) << "target triple not found in the module";
LOG(ERROR) << std::string(80, '*');
LOG(ERROR) << "The XLA GPU backend doesn't support unoptimized code " "generation but ";
LOG(ERROR) << "--xla_backend_optimization_level is set to " << opt_level << "!";
LOG(ERROR) << "(Supported configuration is " "--xla_backend_optimization_level >= 2.)";
LOG(ERROR) << std::string(80, '*');
VLOG(2) << "Module '" << module->getName().str() << "' is empty. Skipping compilation.";
VLOG(1) << "Compile-time artifacts located at: " << tempdir_name;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/cpu/cpu_runtime.cc
VLOG(3) << "TracingStart " << name;
VLOG(3) << "TracingEnd " << id;
VLOG(2) << "AcquireInfeedBufferForDequeue: " << ShapeString(shape, shape_length) << " on stream executor " << device_ordinal;
VLOG(2) << "ReleaseInfeedBufferAfterDeque: " << ShapeString(shape_ptr, shape_length) << " on stream executor " << device_ordinal;
VLOG(2) << "AcquireOutfeedBufferForPopulation: " << ShapeString(shape_ptr, shape_length) << " on stream executor " << device_ordinal;
VLOG(2) << "ReleaseOutfeedBufferAfterPopulation: " << ShapeString(shape_ptr, shape_length) << " on stream executor " << device_ordinal;
LOG(FATAL) << "Unexpected datatype;";
VLOG(2) << "All-reduce input/output shape : " << shape_str;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/cpu/vector_support_library.cc
LOG(FATAL) << "Expected either " << TypeToString(scalar_type()) << " or " << TypeToString(vector_type()) << " but got " << TypeToString(type);
LOG(FATAL) << "Max for integers is unimplemented";
LOG(FATAL) << "Division for integers is unimplemented";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/cpu/dot_op_emitter.cc
VLOG(2) << "Emitting column major matrix-vector multiply with m = " << m << " and k = " << k;
VLOG(2) << "Emitting row major matrix-vector multiply with m = " << m << " and k = " << k;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/cpu/xfeed_manager.cc
VLOG(3) << "Enqueueing " << queue_name_ << " buffer (of " << buffers.size() << " buffers) with length: " << b->length();
VLOG(3) << "Waiting for an available buffer.";
VLOG(3) << "A buffer is available!";
VLOG(3) << "Releasing buffer with shape: " << (shape.ok() ? ShapeUtil::HumanString(shape.ValueOrDie()) : "<error status>");

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/cpu/cpu_transfer_manager.cc
VLOG(2) << "Transferring literal to infeed with shape: " << ShapeUtil::HumanString(shape);
VLOG(2) << "Enqueueing outfeed buffer (for the device to populate) of length " << size_32 << "B";
VLOG(2) << "Waiting for buffer to be notified as populated.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc
VLOG(1) << "CPU target: " << target_machine_->getTargetCPU().str() << " features: " << target_machine_->getTargetFeatureString().str();
LOG(ERROR) << "Unable to resolve runtime symbol: `" << name << "'. Hint: if the symbol a custom call target, make sure you've " "registered it with the JIT using " "XLA_CPU_REGISTER_CUSTOM_CALL_TARGET.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/cpu/sample_harness.cc
LOG(INFO) << absl::StrFormat("computation took %dns", profile.compute_time_ns());
LOG(INFO) << actual.ToString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/cpu/compiler_functor.cc
VLOG(2) << "IR before optimizations";
XLA_VLOG_LINES(2, llvm_ir::DumpModuleToString(module));
VLOG(2) << "IR after optimizations";
XLA_VLOG_LINES(2, llvm_ir::DumpModuleToString(module));
LOG(WARNING) << "Could convert memory buffer to object file!";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/cpu/ir_emitter.cc
VLOG(2) << "Emitting IR for CPU function [" << function_name_prefix << "]";
VLOG(2) << "HandleBitcast: " << bitcast->ToString();
VLOG(2) << "HandleConstant: " << constant->ToString();
VLOG(2) << "HandleInfeed: " << infeed->ToString();
VLOG(2) << "HandleDot: ";
VLOG(2) << " lhs operand: " << llvm_ir::DumpToString(*lhs_array.GetBasePointer());
VLOG(2) << " rhs operand: " << llvm_ir::DumpToString(*rhs_array.GetBasePointer());
VLOG(2) << " target: " << llvm_ir::DumpToString(*target_array.GetBasePointer());
LOG(WARNING) << "Using Eigen instead of MKL-DNN for single-threaded " "conv2d function.";
VLOG(3) << "operand=" << ShapeUtil::HumanStringWithLayout(operand->shape());
VLOG(3) << "fft=" << ShapeUtil::HumanStringWithLayout(fft->shape());
VLOG(2) << "HandleParameter: " << parameter->ToString();
VLOG(1) << "Successfully vectorized reduction " << reduce->ToString() << "";
VLOG(1) << "Could not vectorize reduction " << reduce->ToString() << ": " << vectorization_failure_reason;
VLOG(2) << "HandleSlice: " << slice->ToString();
VLOG(2) << " emitted copy of " << memcpy_bytes << " bytes inside " << num_outer_loops << " loops";
VLOG(3) << "HandleFusion FusedDynamicUpdateSliceInPlace";
VLOG(3) << "HandleFusion kLoop";
VLOG(3) << "HandleFusion kOutput";
VLOG(1) << "Emitted fast concatenate for " << concatenate->ToString();
VLOG(2) << "RngGetAndUpdateState: " << rng_state->ToString();
VLOG(2) << "FinishVisit root: " << root->ToString();
VLOG(2) << " outfeed with value: " << llvm_ir::DumpToString(*GetEmittedValueFor(root->operand(0)));
VLOG(2) << " value: " << llvm_ir::DumpToString(*GetEmittedValueFor(root));
VLOG(3) << "Visiting: " << hlo->ToString();
LOG(FATAL) << "could not find emitted value for: " << hlo->ToString();
VLOG(2) << "EmitTargetElementLoop: " << target_op->ToString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/cpu/cpu_compiler.cc
VLOG(2) << "backend_optimization_level: " << module_config.debug_options().xla_backend_optimization_level();
VLOG(1) << "Compiling: " << module->name();
VLOG(1) << "Compilation finished";
VLOG(1) << "Compiling ahead-of-time: " << module->name();
VLOG(1) << "Compilation finished";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/cpu/cpu_executable.cc
VLOG(1) << "compute_function_ at address " << reinterpret_cast<void*>(compute_function_);
VLOG(3) << "Allocating " << assignment_->Allocations().size() << " allocations for module " << module().name();
VLOG(3) << allocation.ToString();
VLOG(3) << "allocation #" << i << " is a parameter";
VLOG(3) << "allocation #" << i << " is a constant";
VLOG(3) << "buffer #" << i << " is thread-local";
VLOG(3) << "buffer #" << i << " is in the preallocated result ShapedBuffer";
VLOG(3) << "buffer #" << i << " allocated " << buffer_size << " bytes [" << owning_buffers[i]->opaque() << "]";
VLOG(3) << "result index: " << result_slice.index();
VLOG(3) << "Executing compute function:";
VLOG(3) << absl::StrFormat( " func(void* result, void* params[null], void* buffer_table[%u], " "uint64 profile_counters[%u])", buffer_pointers.size(), profile_counters_size);
VLOG(3) << absl::StrFormat(" result = %p", result_buffer);
VLOG(3) << " params = nullptr";
VLOG(3) << absl::StrFormat( " buffer_table = [%s]", absl::StrJoin(buffer_pointers, ", ", ptr_printer));
VLOG(3) << absl::StrFormat(" profile_counters = %p", profile_counters);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/cpu/cpu_instruction_fusion.cc
VLOG(2) << "Considering for fusion: operand " << operand_index << " of " << consumer->ToString();
VLOG(2) << "Fusion OK: Can create output fusion.";
VLOG(2) << "Bailing because producer can be output-fused into some operand.";
VLOG(2) << "Producer is not fusible.";
VLOG(2) << "Fusion is not profitable.";
VLOG(2) << "Not fusing: !ShouldFuse(consumer).";
VLOG(2) << "Not fusing: insufficient non-constant nodes.";
VLOG(2) << "Not fusing: producer is itself a fusion node.";
VLOG(2) << "Fusing small matrix-vector product.";
VLOG(2) << "Fusing small matrix-vector product.";
VLOG(2) << "Fusing: consumer is a fusion node.";
VLOG(2) << "Fusing: consumer is elementwise or fusible.";
VLOG(2) << "Not fusing.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/cpu/parallel_task_assignment.cc
VLOG(1) << "ParallelTaskAssignment max_parallelism: " << max_parallelism;
XLA_VLOG_LINES(2, "ParallelTaskAssigner ENTRY");
XLA_VLOG_LINES(3, module->ToString());
XLA_VLOG_LINES(2, "ParallelTaskAssigner EXIT");
XLA_VLOG_LINES(3, module->ToString());
VLOG(2) << "Assigned parallel task count: " << total_partition_count << " to instruction: " << new_root->name() << " parent: " << new_root->parent()->name();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/cpu/runtime_fork_join.cc
VLOG(2) << "ParallelForkJoin ENTRY" << " num_partitions: " << num_partitions << " num_partitioned_dims: " << num_partitioned_dims;
VLOG(3) << "ParallelForkJoin partition " << i << " done.";
VLOG(3) << "ParallelForkJoin partition 0 done.";
VLOG(2) << "ParallelForkJoin EXIT";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/llvm_ir/tuple_ops.cc
VLOG(2) << "HandleSelect for tuple:";
VLOG(2) << " pred_value: " << DumpToString(*pred_value);
VLOG(2) << " pred_cond: " << DumpToString(*pred_cond);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/llvm_ir/fused_ir_emitter.cc
VLOG(3) << "The cached generated value is reused.";
VLOG(3) << "The cached generated value can't be reused, because it is in " "a different BB (" << generated_value_bb->getName().str() << ") from the current insertion block (" << b_->GetInsertBlock()->getName().str() << ").";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/llvm_ir/kernel_support_library.cc
VLOG(2) << "Generating kernel for " << kernel_name;
VLOG(3) << "Re-using kernel for " << kernel_name;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/llvm_ir/dynamic_update_slice_util.cc
VLOG(2) << "EmitDynamicUpdateSliceInPlace for " << name;
VLOG(2) << "EmitFusedDynamicUpdateSliceInPlace for " << fusion->ToShortString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/llvm_ir/llvm_util.cc
VLOG(2) << "EmitBufferIndexingGEP with type=" << llvm_ir::DumpToString(*array_type) << " array=" << llvm_ir::DumpToString(*array) << " index=" << llvm_ir::DumpToString(*index);
LOG(FATAL) << "unsupported type " << element_type;
LOG(INFO) << tag << " (int64): " << value;
VLOG(2) << "Passing argv to LLVM:";
VLOG(2) << s;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/mlir_gpu/mlir_compiler.cc
LOG(WARNING) << msg;
LOG(WARNING) << "Searched for CUDA in the following directories:";
LOG(WARNING) << " " << dir;
LOG(WARNING) << "You can choose the search directory by setting xla_gpu_cuda_data_dir " "in HloModule's DebugOptions. For most apps, setting the environment " "variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.";
VLOG(2) << "Looking for libdevice at " << libdevice_dir;
VLOG(2) << "Found libdevice dir " << libdevice_dir;
LOG(WARNING) << "Couldn't get compute capability for device; assuming sm_20.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/mlir_gpu/emission_context.cc
LOG(ERROR) << module->ToString( HloPrintOptions() .set_print_instruction( [&instructions_with_error](const HloInstruction* instr) { return instructions_with_error.count(instr); }) .set_format_instruction( // Returns the string representation of `instr` in the following // format. // // ROOT? instr_name // FAILED: err_0 // FAILED: err_1 // ... [&instructions_with_error](const HloInstruction* instr, const string& instr_name, int indent, bool is_root) { const string tab(2 * indent, ' '); if (!instructions_with_error.count(instr)) { return absl::StrCat(tab, is_root ? "ROOT " : "", instr_name); } static constexpr char kStartBold[] = "033[1m"; static constexpr char kStartRed[] = "033[31m"; static constexpr char kBackToNormal[] = "033[0m"; string result = absl::StrCat(tab, kStartBold, is_root ? "ROOT " : "", instr_name, kBackToNormal); for (const string& err : instructions_with_error.at(instr)) { absl::SubstituteAndAppend( &result, "$0 $1$2FAILED:$3 $4$5$6", tab, kStartBold, kStartRed, kBackToNormal, kStartBold, err, kBackToNormal); }

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/rpc/grpc_service_main.cc
LOG(ERROR) << usage;
LOG(INFO) << "Server listening on " << server_address;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/client/local_client.cc
LOG(ERROR) << "TransferLiteralFromDevice for HLO snapshot inputs " "failed: " << status;
LOG(ERROR) << "TransferLiteralFromDevice for HLO snapshot outputs failed: " << status;
VLOG(3) << "Set device ordinal to default value of: " << updated_options.device_ordinal();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/client/xla_builder.cc
LOG(FATAL) << "error building computation: " << error;
VLOG(1) << "Non-constant: " << instr.name();
VLOG(3) << "original shape: " << ShapeUtil::HumanString(*original_shape);
VLOG(3) << "dims to collapse: " << absl::StrJoin(dimensions, ",");
VLOG(3) << "new sizes: [" << absl::StrJoin(new_sizes, ",") << "]";
LOG(FATAL) << "unhandled distribution " << distribution;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/client/client.cc
VLOG(1) << "making transfer request";
VLOG(3) << "TransferToClientRequest: {" << request.DebugString() << "}";
VLOG(1) << "done with request";
VLOG(3) << "TransferToClientResponse: {" << response.DebugString() << "}";
VLOG(1) << "making transfer to server request";
VLOG(3) << "TransferToServerRequest: {" << request.DebugString() << "}";
VLOG(1) << "done with request";
VLOG(3) << "TransferToServerResponse: {" << response.DebugString() << "}";
VLOG(1) << "making transfer to infeed request";
VLOG(3) << "TransferToInfeedRequest: {" << request.DebugString() << "}";
VLOG(1) << "done with request";
VLOG(3) << "TransferToInfeedResponse: {" << response.DebugString() << "}";
VLOG(1) << "making transfer from outfeed request";
VLOG(3) << "TransferFromOutfeedRequest: {" << request.DebugString() << "}";
VLOG(1) << "done with request";
VLOG(3) << "TransferFromOutfeedResponse: {" << response.DebugString() << "}";
VLOG(1) << "making reset device request";
VLOG(3) << "ResetDeviceRequest: {" << request.DebugString() << "}";
VLOG(1) << "done with request";
VLOG(3) << "ResetDeviceResponse: {" << response.DebugString() << "}";
VLOG(2) << "making compute-constant-graph request";
VLOG(2) << "done with request";
VLOG(3) << "ComputeConstant: {" << response.DebugString() << "}";
VLOG(1) << "making compile request: " << request.ShortDebugString();
VLOG(1) << "done with request";
VLOG(1) << "making execute request: " << request.ShortDebugString();
VLOG(1) << "done with request";
VLOG(1) << "Making ExecuteParallel request: " << execution_options->DebugString();
VLOG(1) << "ExecuteParallel request done.";
VLOG(3) << "Fetching result from device " << i << ": " << ShapeUtil::HumanString(shape);
VLOG(1) << "Defaulting to device 0 result";
VLOG(1) << "making execute-graph-parallel request: " << request.ShortDebugString();
VLOG(1) << "done with request";
VLOG(1) << "making get device request: " << request.ShortDebugString();
VLOG(1) << "done with request";
VLOG(1) << "making unregister request";
VLOG(1) << "done with request";
VLOG(1) << "making DestructTuple request";
VLOG(1) << "done with request";
VLOG(1) << "making computation graph stats request";
VLOG(1) << "done with request";
VLOG(1) << "making get shape request";
VLOG(1) << "done with request";
VLOG(1) << "making create channel handle request";
VLOG(1) << "done with request";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/client/global_data.cc
VLOG(1) << "Requesting to unregister " << handle.ShortDebugString();
VLOG(1) << "Done with request";
LOG(WARNING) << "Failed to unregister handles: " << status << "; continuing anyway...";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/client/lib/math.cc
LOG(FATAL) << "Expected real fp type.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/client/lib/conv_grad_size_util.cc
VLOG(2) << "expanded_out = " << dim.output_size << ", effective_filter_size = " << effective_filter_size << ", padded_out = " << padded_out_size << ", pad_before = " << dim.pad_before << ", pad_after = " << dim.pad_after << ", dilation = " << dilation << ", strides = " << stride;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/tf2tensorrt/kernels/trt_engine_resource_ops.cc
VLOG(1) << "Creating TRT engine cache resource handle for op " << resource_name_ << " on device " << ctx->device()->name();
VLOG(1) << "Loaded " << num_loaded_engine << " TRT engines for op " << handle.name() << " on device " << ctx->device()->name() << " from file " << filename;
VLOG(1) << "Serialized " << num_serialized_engines << " TRT engines for op " << resource_name << " on device " << ctx->device()->name() << " to file " << filename;
VLOG(1) << "Destroying TRT engine cache resource for op " << resource_name << " on device " << ctx->device()->name();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc
LOG(ERROR) << "Unsupported Data type " << DataTypeString(tensor_type);
VLOG(1) << "Constructing function handle";
VLOG(1) << "Constructing " << name();
VLOG(2) << "Not found _allow_build_at_runtime in " << context->device()->name() << ", thus setting _allow_build_at_runtime=true";
VLOG(2) << "Not found _use_implicit_batch in " << context->device()->name() << ", thus setting _use_implicit_batch=true";
VLOG(2) << "Need at least TensorRT 6.0 for explicit batch mode. Setting " << "_use_implicit_batch=true";
VLOG(1) << "Attribute input_shapes is not set. This happens probably " << "because you are using a model that is already converted " << "to TensorRT with a previous version of TF-TRT (i.e. includes " << "TRTEngineOp in graph). This is not an error. If you convert " << "the original model again to TensorRT, the attributes " << "input_shapes will be set automatically.";
VLOG(1) << "Executing native segment: " << name();
VLOG(1) << "Native Segment completed";
VLOG(1) << "Executing TRT calibration: " << name();
VLOG(2) << "Filled map for sending";
VLOG(2) << "Passed calibration data";
VLOG(1) << "Engine retrieval for input shapes: " << TensorShapeUtils::ShapeListString(input_concrete_shapes) << " failed. Running native segment for " << name();
LOG(WARNING) << "Failed to execute engine, " << "retrying with native segment for " << name();
LOG(ERROR) << msg;
VLOG(1) << "Executing TRT engine: " << name();
VLOG(2) << " Network name: " << cuda_engine->getName();
VLOG(2) << " Activation size: " << cuda_engine->getDeviceMemorySize() << " bytes";
VLOG(2) << " Workspace size: " << cuda_engine->getWorkspaceSize() << " bytes";
VLOG(2) << " Datatype of " << cuda_engine->getNbBindings() << " inputs/outputs";
VLOG(2) << binding_types;
LOG(ERROR) << "Requested engine context with index " << trt_context_idx << ", but only 1 context is present.";
LOG(ERROR) << "Input data has inconsistent batch size: " << num_batch << " vs " << input_shape.dim_size(0);
LOG(ERROR) << "INT8 inputs are not supported yet!";
LOG(ERROR) << "Unknown TRT data type: " << static_cast<int>(dtype);
LOG(WARNING) << "Failed to set dimensions for all dynamic input tensors.";
LOG(WARNING) << "Failed to set dimensions for all shape input tensors.";
LOG(ERROR) << "Explicit batch mode is only supported with TensorRT 6 and above.";
LOG(ERROR) << "Failed to get output shape: " << status;
LOG(ERROR) << "Allocating output failed with " << status;
LOG(WARNING) << "int8 is not supported yet!";
LOG(WARNING) << "Unknown TRT data type: " << static_cast<int>(dtype);
VLOG(1) << "Called IExecutionContext::enqueue";
VLOG(1) << "Called IExecutionContext::enqueueV2";
LOG(ERROR) << "Explicit batch mode is only supported with TensorRT 6 and above.";
LOG(WARNING) << "Failed to enqueue batch for TRT engine: " << name();
VLOG(1) << "Size of serialized TRT engine: " << serialized_segment_.capacity();
LOG(WARNING) << "Found no engine in cache matching input shapes. " << "Not building a new engine because " << "allow_build_at_runtime=False. " << "The native segment will be used instead.";
LOG(INFO) << "Building a new TensorRT engine for " << name() << " with input shapes: " << TensorShapeUtils::ShapeListString(input_concrete_shapes);
LOG(WARNING) << "Engine creation for " << name() << " failed. " << "The native segment will be used instead. " << "Reason: " << status;
VLOG(1) << "Added new engine to cache of " << name() << ". Cache size: " << cache.size();
VLOG(1) << "Constructing calibrator";
LOG(ERROR) << "Can't get gpu_device_info from context->device()";
VLOG(1) << "Starting calibration thread on device " << platform_gpu_id << ", Calibration Resource @ " << cres;
LOG(ERROR) << "Couldn't set cuda device to " << platform_gpu_id << " in calibration thread";
LOG(ERROR) << "Calibration failed: " << s;
VLOG(1) << "Calibration loop terminated " << this->name();
VLOG(1) << "initialized calibrator resource";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/tf2tensorrt/segment/segment.cc
VLOG(1) << " edge to sink node " << src->name() << " -> " << e->dst()->name();
VLOG(1) << "Not a TF-TRT candidate, " << "(Op type: " << node->tf_node()->type_string() << "), " << "(Op name: " << node->name() << "), " << "(Reason: excluded by segmenter option)";
VLOG(1) << "Not a TF-TRT candidate, " << "(Op type: " << node->tf_node()->type_string() << "), " << "(Op name: " << node->name() << "), " << "(Reason: " << status << ")";
VLOG(2) << "Accepted as a TF-TRT candidate, " << "(Op type: " << node->tf_node()->type_string() << "), " << "(Op name: " << node->name();
LOG(INFO) << msg << "(For more information see " << "https://docs.nvidia.com/deeplearning" << "/frameworks/tf-trt-user-guide/index.html#supported-ops).";
VLOG(3) << "Trying node " << node->name() << " id=" << node->id();
VLOG(3) << "... not a TRT candidate";
VLOG(3) << "... out node " << out_edge->dst()->name() << " ( " << out_edge->dst()->id() << " <- " << node->id() << " )";
VLOG(3) << "... ... Control Edge, Skipping";
VLOG(3) << "... ... not a TRT candidate";
VLOG(3) << "... ... can contract";
VLOG(3) << "... ... cannot contract, would form cycle";
VLOG(3) << "Merge " << src->name() << " <- " << dst->name() << " (" << src->id() << " <- " << dst->id();
VLOG(2) << "Node " << tf_node->name() << " has no device assigned requested device is: " << tf_node->requested_device();
VLOG(1) << "Segment original size: " << segment_nodes.size();
VLOG(2) << "----> Need to remove node " << in->name() << " because one of its " << (is_input_nodes ? "output" : "input") << " nodes in the graph was removed: " << node->name();
VLOG(1) << "Segment new size: " << segment_nodes.size();
VLOG(1) << "Nodes in segment " << segments->size() << " with parent=" << segment_root << ":" << s;
VLOG(1) << "Segment " << segments->size() << " has only " << num_effective_nodes << " effective nodes, dropping";
VLOG(1) << "No device assigned to segment " << segments->size();
LOG(WARNING) << s;
VLOG(1) << "Devices " << s;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/tf2tensorrt/utils/trt_lru_cache.cc
LOG(ERROR) << "Can't find device allocator for gpu device " << device->name();
VLOG(1) << "Destroying TRTEngineCacheResource...";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/tf2tensorrt/utils/trt_int8_calibrator.cc
VLOG(1) << "Set Batch Waiting finished";
LOG(FATAL) << "FATAL " << engine_name_ << " input name '" << it.first << "' does not match with the buffer names";
LOG(FATAL) << "cudaMemcpy " << engine_name_ << " for '" << it.first << "' failed with " << status;
LOG(FATAL) << "Calibration engine asked for unknown tensor name '" << names[i] << "' at position " << i;
VLOG(1) << "Got calibration data for " << engine_name_ << " @" << ptr << " length=" << length;
VLOG(1) << "Destroying calibrator for " << engine_name_;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/tf2tensorrt/utils/py_utils.cc
LOG(WARNING) << "Cannot dlopen some TensorRT libraries. If you would like " "to use Nvidia GPU with TensorRT, please make sure the " "missing libraries mentioned above are installed properly.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/tf2tensorrt/utils/trt_shape_optimization_profiles.cc
VLOG(1) << "Not creating profiles without input_shapes. " "You have to enable profile generation mode first (build).";
VLOG(1) << "Creating profiles with startegy of one profile " << "for each input (min=opt=max).";
VLOG(1) << "Created profile " << profiles_.back().DebugString();
VLOG(1) << "Added optimization profile " << profiles_[i].DebugString() << " to builder config.";
LOG(ERROR) << "Failed to add optimization profile " << profiles_[i].DebugString() << ". This usually happens when profile is invalid.";
VLOG(1) << "Profile not found for input shapes " << DebugString(shapes) << ".";
VLOG(1) << "Creating execution context " << i;
VLOG(2) << "Attempting to restore " << n_profiles << " profiles, each with " << n_inputs << " inputs";
VLOG(2) << "Restored profile " << cfg.DebugString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/tf2tensorrt/utils/trt_allocator.cc
VLOG(2) << "Allocated " << total_size << " bytes memory @" << alloc_mem << "; aligned to " << size << " bytes @" << mem << " with alignment " << alignment;
VLOG(1) << "Using " << allocator->Name() << " allocator from TensorFlow";
VLOG(2) << "Deallocating @ " << memory;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc
VLOG(2) << name_ << " " << msg;
LOG(WARNING) << name_ << " " << msg;
LOG(ERROR) << name_ << " " << msg;
LOG(FATAL) << name_ << " " << msg;
LOG(FATAL) << name_ << "Got unknown severity level " << int(severity) << " from TensorRT: " << msg;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/tf2tensorrt/stub/nvinfer_stub.cc
LOG(FATAL) << symbol_name << " symbol not found.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/tf2tensorrt/stub/nvinfer_plugin_stub.cc
LOG(FATAL) << symbol_name << " symbol not found.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/tf2tensorrt/convert/trt_optimization_pass.cc
VLOG(1) << "Called INIT for " << name_ << " with config = " << config;
LOG(INFO) << "Cluster = " << cluster;
LOG(INFO) << offset << "type = " << cluster->type();
LOG(INFO) << offset << "num warmup steps = " << cluster->NumWarmupSteps();
LOG(INFO) << offset << " Device names:";
LOG(INFO) << offset2 << s;
LOG(INFO) << offset << "Peak Memory Usage :";
LOG(INFO) << offset2 << s.first << " = " << s.second;
LOG(INFO) << offset << "Device properties:";
LOG(INFO) << offset2 << k.first;
LOG(INFO) << offset3 << "type = " << dt.type();
LOG(INFO) << offset3 << "vendor = " << dt.vendor();
LOG(INFO) << offset3 << "model = " << dt.model();
LOG(INFO) << offset3 << "frequency = " << dt.frequency();
LOG(INFO) << offset3 << "num cores = " << dt.num_cores();
LOG(INFO) << offset3 << "num registers = " << dt.num_registers();
LOG(INFO) << offset3 << "L1 cache size = " << dt.l1_cache_size();
LOG(INFO) << offset3 << "L2 cache size = " << dt.l2_cache_size();
LOG(INFO) << offset3 << "L3 cache size = " << dt.l3_cache_size();
LOG(INFO) << offset3 << "SHMem per SMP = " << dt.shared_memory_size_per_multiprocessor();
LOG(INFO) << offset3 << "memory size = " << dt.memory_size();
LOG(INFO) << offset3 << "bandwidth = " << dt.bandwidth();
LOG(INFO) << offset3 << "environment :";
LOG(INFO) << offset4 << e.first << " = " << e.second;
LOG(INFO) << "item: " << item.id;
LOG(INFO) << offset << "Feeds :";
LOG(INFO) << offset2 << f.first << " = shaped " << shape.DebugString();
LOG(INFO) << offset << "No Feeds";
LOG(INFO) << offset << "Fetches :";
LOG(INFO) << offset2 << f;
LOG(INFO) << offset << "No Fetches";
LOG(INFO) << offset << "init ops :";
LOG(INFO) << offset2 << f;
LOG(INFO) << offset << "No init ops";
LOG(INFO) << "Save Op = " << item.save_op;
LOG(INFO) << "Restore Op = " << item.restore_op;
LOG(INFO) << "save_restore_loc_tensor = " << item.save_restore_loc_tensor;
LOG(INFO) << offset << "keep ops :";
LOG(INFO) << offset2 << f;
LOG(INFO) << offset << "No keep ops";
LOG(INFO) << "Device name= " << dev->name() << " parsedname job= " << pname.job << " id= " << pname.id << " has_id: " << pname.has_id << " has_job: " << pname.has_job << "has_type: " << pname.has_type << " type =" << pname.type;
VLOG(1) << "Called TRTOptimization Pass " << name_;
VLOG(1) << "Called TRTOptimization Pass " << name_ << " on a grappler item with id=" << item.id << ", which is probably a function object (funcdef). " << "Skipping optimization because TensorRTOptimizer " << "should not be called on function objects.";
LOG(INFO) << CurrentStackTrace();
VLOG(2) << "Setting max_batch_dim to " << max_batch_dim << " using batch dimension of " << f.first << " with shape " << shape;
LOG(INFO) << "Specified max_batch_size=" << maximum_batch_size_ << " is larger than maximum batch dimension of inputs (" << max_batch_dim << "). " << "This can result in poor performance.";
VLOG(1) << "Calibration with FP32 or FP16 is not implemented. " << "Falling back to use_calibration = False." << "Note that the default value of use_calibration is True.";
VLOG(1) << "Returning from " << name_;
VLOG(1) << "Constructing a CustomOptimizationPass registration object for " << name;
VLOG(1) << "Instantiating CustomOptimizationPass object TensorRTOptimizer";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/tf2tensorrt/convert/convert_nodes.cc
LOG(FATAL) << "Attribute not found: " << key;
LOG(INFO) << "Unknown output shape" << node->name();
VLOG(2) << "PADDING_" << i << " pre: " << left << ", post: " << right << "paras: " << input_dims[i] << ", " << stride.d[i] << ", " << "kernel: " << kernel.d[i];
LOG(FATAL) << "Unsupported type in reorder expected fp32 or fp16 but got " << DebugString(iweights.TrtDType());
VLOG(2) << "num_groups: " << num_groups << "c" << iweights.shape_.d[2] << " then " << c << "k" << iweights.shape_.d[3] << " then " << k << "r" << iweights.shape_.d[0] << " then " << r << "s" << iweights.shape_.d[1] << " then " << s;
LOG(FATAL) << "Unsupported type, expected fp32 or fp16 but got " << DebugString(iweights.TrtDType());
VLOG(2) << "num_groups: " << num_groups << ", c: " << iweights.shape_.d[3] << " becomes " << c << ", k: " << iweights.shape_.d[4] << " becomes " << k << ", d: " << d << ", r: " << r << ", s: " << s;
LOG(FATAL) << "Unsupported type, expected fp32 or fp16 but got " << DebugString(iweights.TrtDType());
LOG(INFO) << "Linked TensorRT version: " << GetLinkedTensorRTVersion();
LOG(INFO) << "Loaded TensorRT version: " << GetLoadedTensorRTVersion();
LOG(ERROR) << "Failed to initialize TensorRT plugins, and conversion may " "fail later.";
LOG(WARNING) << "Can not find any TensorRT plugins in registry.";
VLOG(1) << "Found the following " << num_trt_plugins << " TensorRT plugins in registry:";
LOG(WARNING) << "TensorRT plugin at index " << i << " is not accessible (null pointer returned by " "getPluginCreatorList for this plugin)";
VLOG(1) << " " << trt_plugin_creator_list[i]->getPluginName();
VLOG(1) << "Creating TensorRT builder";
VLOG(1) << "Creating TensorRT network";
VLOG(2) << "Adding out tensor " << output_name << ": " << output.DebugString();
VLOG(1) << "Marking output TRT tensor " << output.source_tensor_name << " with data type " << DebugString(output.trt_dtype) << ", which feeds TF node " << output.dest_node_name;
VLOG(2) << "Created TensorRT network with the following layers:";
VLOG(2) << " " << layer->getName() << " (" << "type: " << static_cast<int>(layer->getType()) << ", precision: " << static_cast<int>(layer->getPrecision()) << ")";
VLOG(1) << "Configuring TensorRT builder";
VLOG(1) << "Building TensorRT engine";
VLOG(1) << "Setting TensorRT network name to " << trt_network_name;
VLOG(1) << "Building TensorRT engine";
VLOG(1) << "TransposeTensor permutation: " << DebugString(permutation, dims.nbDims);
VLOG(1) << "Setting range for: " << tensor->getName() << ": " << range;
VLOG(1) << pattern.first;
VLOG(1) << " Fused output tensor:" << fused_layer->getOutput(i)->getName();
LOG(WARNING) << "Quantization range was not found for " << tensor->getName() << ". " << "Setting invalid quantization range.";
VLOG(1) << "And setting layer " << layer->second->getName() << " precision to fp16.";
VLOG(1) << "Copy quantization range: " << it->first->getName() << " -> " << it->second->getName();
VLOG(2) << "Retrieved input " << name << ": " << input.DebugString();
LOG(ERROR) << msg;
VLOG(2) << "RSQRT GETS DONE";
LOG(ERROR) << "Not supported op for unary: " << static_cast<int>(op);
VLOG(2) << "RSQRT GETS DONE";
LOG(ERROR) << "Not supported op for unary: " << static_cast<int>(op);
VLOG(1) << "input_batch_dim=" << input_batch_dim << ", input_dims=" << DebugString(input_dims) << "eshape_batch_dim=" << reshape_batch_dim << ", reshape_dims=" << DebugString(reshape_dims);
VLOG(2) << "Using SAME padding";
VLOG(2) << "Using SAME padding";
VLOG(1) << "ConvertBiasAdd permutation: " << DebugString(permutation, original_dims.nbDims);
VLOG(2) << "Bias shape adjusted to " << DebugString(bias_shape);
LOG(WARNING) << node_def.op() << " only supports is_training=false. If you " << "are using Keras, please call " << "keras.backend.set_learning_phase(0) before constructing " << "your model. At " << node_def.name();
VLOG(1) << "Starting to convert TensorFlow ops to TensorRT layers";
VLOG(2) << "Converting node " << node_name << ", op=" << node_def.op();
LOG(WARNING) << error_message;
VLOG(2) << "Adding engine input tensor " << node_name << " with shape " << DebugString(trt_dims);
VLOG(1) << "Finished conversion";
VLOG(1) << "Reusing input " << node_name << " for the edge " << connection.outside_node_name << ":" << connection.outside_port << " -> " << connection.inside_node_name << ":" << connection.inside_port;
VLOG(1) << "Constructing input " << node_name << " for the edge " << connection.outside_node_name << ":" << connection.outside_port << " -> " << connection.inside_node_name << ":" << connection.inside_port;
VLOG(1) << "Reusing output " << node_name << " for the edge " << connection.inside_node_name << ":" << connection.inside_port << " -> " << connection.outside_node_name << ":" << connection.outside_port;
VLOG(1) << "Constructing output " << node_name << " for the edge " << connection.inside_node_name << ":" << connection.inside_port << " -> " << connection.outside_node_name << ":" << connection.outside_port;
VLOG(2) << "Copying " << snode->name() << " to subgraph";
VLOG(1) << "Updating " << snode->name() << ":" << connection.inside_port << " from " << snode->input(connection.inside_port) << " to " << arg_name;
VLOG(1) << "... removing control inputs " << input.first << " from subgraph.";
VLOG(1) << "--> Need to remove output node " << out_edge->src()->name() << " which is a Const.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc
VLOG(1) << "Found TF GPU " << tf_gpu_id.value() << " at cuda device " << platform_gpu_id.value();
LOG(ERROR) << "Could not find any TF GPUs";
VLOG(2) << "Node " << node->name() << " neither have requested device nor assigned device";
VLOG(1) << "Failed to parse " << (node->requested_device().empty() ? "assigned" : "requested") << " device " << device_name << " of node " << node->name();
VLOG(1) << "Node " << node->name() << " was assigned to a non-GPU device " << device_name;
VLOG(1) << "Adding const node " << input_node->name();
VLOG(1) << "Input edge = " << s;
VLOG(1) << "Output edge = " << s;
VLOG(1) << "Converted TensorRT candidate segment '" << info->engine_name << "' to a GraphDef";
LOG(WARNING) << "Detected multiple (" << segment_devices.size() << ") devices for the segment. Picking first one to continue.";
VLOG(1) << "No device is assigned to the segment. A device will be " "assigned during graph execution (inference).";
LOG(FATAL) << "Node " << node_name << " not found in any engine.";
VLOG(1) << "Processing " << info.engine_name;
VLOG(1) << "Engine Control Input " << input_node->name() << " -> " << info.engine_name;
VLOG(1) << "Engine Input " << input_node->name() << ":" << port << " -> " << info.engine_name << ":" << inputs.size() - 1;
VLOG(1) << ins;
LOG(ERROR) << "Node construction failed with" << status;
VLOG(1) << "Adding TRTEngine " << info.engine_name << " to graph";
LOG(ERROR) << "Adding node failed " << status;
VLOG(1) << "Connecting control edge from " << in->name() << " to " << engine_node->name();
VLOG(1) << "input_nodes size = " << input_nodes.size();
VLOG(1) << "Connecting data edge from " << n->name() << ":" << in.index << " to " << engine_node->name() << ":" << i;
VLOG(1) << "Updating control edge from " << engine_node->name() << " to " << output_node->name();
VLOG(1) << "Updating data edge from " << engine_node->name() << ":" << conn.port_number << " to " << output_node->name() << ":" << port;
VLOG(7) << engine_name << " Function_Def ";
VLOG(7) << segment_func->DebugString();
VLOG(1) << "Adding funcdef " << segment_func->signature().name() << " to graphlib";
LOG(WARNING) << msg;
VLOG(1) << "Using allocator " << dev_allocator->Name() << " and cuda_device_id " << cuda_device_id;
LOG(WARNING) << "Cluster is set but device '" << engine.device << "' is not found in the cluster";
LOG(INFO) << "Number of TensorRT candidate segments: " << initial_segments.size();
LOG(WARNING) << "Failed to get engine info for segment " << t << ": " << status;
LOG(WARNING) << "Failed to register segment graphdef to the library " << t << ": " << status;
LOG(ERROR) << "Couldn't get current device: " << cudaGetErrorString(err);
VLOG(1) << "Current cuda device is " << old_cuda_device;
VLOG(1) << "Assigned " << engine.max_workspace_size_bytes << " bytes to " << engine.engine_name;
LOG(WARNING) << "Can't identify the cuda device. Running on device 0 ";
LOG(INFO) << "Replaced " << msg << ".";
LOG(WARNING) << "Cannot replace " << msg << " (keeping original segment).";
VLOG(1) << msg;
VLOG(1) << "Returning from conversion";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/tf2xla/graph_compiler_util.cc
VLOG(2) << "Post rewrite: " << DumpGraphToFile("tf2xla_post_rewrite", *graph);
VLOG(2) << "Post prune: " << DumpGraphToFile("tfcompile_post_prune", *graph);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/tf2xla/functionalize_while.cc
VLOG(3) << "Stack: " << NodesToString(stack);
VLOG(5) << "Copying node " << n->name();
VLOG(2) << "Building loop condition for " << frame->name;
VLOG(2) << "Building loop body for " << frame->name;
VLOG(2) << "Frame " << frame->name << " before: " << DumpGraphToFile("functionalize_before", *graph, library);
VLOG(2) << "Frame " << frame->name << " condition: " << DumpGraphToFile("loop_condition", *cond_graph, library) << " body: " << DumpGraphToFile("loop_body", *body_graph);
VLOG(2) << "Frame " << frame->name << " after: " << DumpGraphToFile("functionalize_after", *graph, library);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/tf2xla/functionalize_cond.cc
VLOG(5) << "Adding switch " << s->DebugString();
VLOG(1) << "Build function arguments";
VLOG(5) << "CondArg nodes created: " << DebugString(cond_arg_nodes_);
VLOG(2) << "Extracting bodies for " << name();
VLOG(5) << "Merges: " << NodesToString(merges_);
VLOG(5) << "For merge: " << m->DebugString() << " " << state_map_->CondStateToString(m);
VLOG(5) << "In branch: " << Branch_Name(branch) << " " << NodesToString(stack);
LOG(WARNING) << errors::InvalidArgument( "Graph contains node ", FormatNodeForError(*src), " that feeds into node ", FormatNodeForError(*dst), " but these nodes are in different control contexts (", DebugString(src_id), " vs ", DebugString(dst_id), " (detected during out edge testing)");
VLOG(2) << "Build cond function for " << name();
VLOG(3) << "FunctionalizeControlFlow (" << branch_name[branch_index] << "): " << DumpGraphToFile( "functionalize_cond_body_" + branch_name[branch_index], *bodies_[branch_index], nullptr);
VLOG(3) << "Build input type";
VLOG(3) << "Build output type: " << DataTypeVectorString(out_type);
VLOG(3) << "Build output shapes: " << PartialTensorShapeUtils::PartialShapeListString(output_shapes);
VLOG(3) << "Build If node";
VLOG(2) << "AddInputEdges for " << if_node_->name();
VLOG(2) << "AddOutputEdges for " << if_node_->name();
VLOG(1) << "Build If and replace merge nodes " << NodesToString(this->merges_);
LOG(INFO) << "Extracted bodies:";
LOG(INFO) << Branch_Name(branch) << ": " << DebugString(output->ToGraphDefDebug());
VLOG(1) << "Adding If for " << replacee->name();
VLOG(2) << "Propagating update state for " << replacee->name() << " " << state_map_.CondStateToString(replacee);
VLOG(5) << "Joining src=" << DebugString(src) << " [" << src << "] and dst=" << DebugString(dst) << " [" << dst << "]";
VLOG(4) << "Joining (for merge) " << DebugString(src) << " and " << DebugString(dst);
VLOG(5) << "Processing forward flow for merge: " << e->DebugString() << " " << state_map_.CondStateToString(src);
VLOG(4) << "Processing forward flow for: " << e->DebugString() << " " << state_map_.CondStateToString(dst);
VLOG(5) << "removing redundant merge: " << node->name();
VLOG(5) << "Redundant switch " << node->name() << " " << Branch_Name(b) << " " << DebugString(dst_id);
VLOG(5) << dst->name() << " :: " << state_map_.CondStateToString(dst) << " @ " << state_map_.AncestorStateToString(dst);
LOG(WARNING) << "Couldn't deduce shape for " << node->name();
LOG(INFO) << "FunctionalizeControlFlow (" << name << "): " << DumpGraphToFile(absl::StrCat("functionalize_cond_", name), *graph_, library_);
VLOG(1) << "FunctionalizeCond::Functionalize";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/tf2xla/xla_context.cc
VLOG(1) << "Building Max() for " << type_string;
VLOG(1) << "Building Min() for " << type_string;
VLOG(1) << "Building Add() for " << type_string;
VLOG(1) << "Building Mul() for " << type_string;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/tf2xla/xla_compilation_device.cc
VLOG(4) << "XlaCompilationDevice::Compute " << FormatNodeDefForError(op_kernel->def());
VLOG(4) << "Done";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/tf2xla/graph_compiler.cc
LOG(ERROR) << "Executor failed to create kernel. " << s;
VLOG(3) << "Translating " << params.op_kernel->name();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/tf2xla/mlir_bridge_pass.cc
LOG(WARNING) << "cannot create directory '" + prefix + "': " + status.error_message();
LOG(WARNING) << "cannot create unique filename, won't dump MLIR module.";
LOG(WARNING) << "cannot open file '" + prefix + "': " + status.error_message();
LOG(WARNING) << "error writing to file '" + prefix + "': " + status.error_message();
VLOG(1) << "Dumped MLIR module to " << prefix;
VLOG(1) << "Skipping MLIR Bridge Pass, session flag not enabled";
VLOG(1) << "Running MLIR Bridge Pass";
VLOG(1) << "Skipping MLIR Bridge V1 Compat Pass, session flag not enabled";
VLOG(1) << "Running MLIR Bridge V1 Compat Pass";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/tf2xla/tf2xla_util.cc
VLOG(2) << "Found function attr for node " << node.name() << ": " << iter.first << " = " << iter.second.func().name();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/tf2xla/xla_resource.cc
LOG(FATAL) << "Invalid resource type";
VLOG(2) << "Gradient lookup for resource: " << name_ << " gradient: " << source;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/tf2xla/tf2xla.cc
LOG(ERROR) << "ConstRetVal index:" << i << " value:" << result.outputs[i].constant_value.DebugString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/tf2xla/functionalize_control_flow_util.cc
VLOG(2) << "node: " << node->name() << " (" << node->id() << ") frame_name: " << cf.frame_name << " frame: " << (cf.frame ? cf.frame->name() : "---") << " parent_frame: " << (cf.parent_frame ? cf.parent_frame->name() : "---");

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/tf2xla/xla_compiler.cc
VLOG(1) << "Building new computation";
VLOG(4) << "Function " << function.name() << " in flib_runtime_";
VLOG(4) << "Function " << function.name() << " in local_flib_runtime_";
VLOG(1) << "XlaCompiler::CompileFunction " << function_id;
VLOG(2) << "XlaCompiler::CompileFunction: " << DumpGraphToFile( absl::StrCat("xla_compile_function_", function_id), *graph);
VLOG(1) << "====================================================";
VLOG(1) << "====================================================";
LOG(FATAL) << "Unreachable case";
VLOG(1) << "Setting dynamic binding " << i << " -> " << dynamic_size_param_index;
VLOG(2) << "XLA computation inputs:";
VLOG(2) << " XLA arg " << i << " shape: " << xla::ShapeUtil::HumanString(arg_shapes[i]) << " name: " << arg.name << " TF arg " << input_to_args->at(i) << " node name: " << arg.node_name << (arg_shardings.find(i) == arg_shardings.end() ? "" : absl::StrCat(" sharding: ", arg_shardings.at(i).DebugString()));
VLOG(2) << " resource: num_gradients: " << arg.tensor_array_gradients.size();
VLOG(1) << "Executing graph symbolically to populate XlaBuilder.: " << name;
VLOG(2) << "XlaCompiler::CompileGraph: " << DumpGraphToFile(absl::StrCat("xla_compile_graph_", name), *graph, flib_runtime_->GetFunctionLibraryDefinition());
VLOG(2) << "Outputs: total: " << context->retvals().size() << " nonconstant: " << num_nonconst_outputs;
VLOG(2) << "XLA output shape: " << xla::ShapeUtil::HumanStringWithLayout(result->xla_output_shape);
VLOG(1) << "Channel: " << key << " " << channel->DebugString();
VLOG(1) << "Host to device channel: " << key << " " << channel->DebugString();
VLOG(1) << "Device to host channel: " << key << " " << channel->DebugString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/tf2xla/functionalize_control_flow.cc
VLOG(2) << "FunctionalizeControlFlow (initial): " << DumpGraphToFile("functionalize_initial", *graph, library);
VLOG(2) << "FunctionalizeControlFlow (final): " << DumpGraphToFile("functionalize_final", *graph, library);
VLOG(2) << "Replacing function " << func_name;
VLOG(2) << "Adding function " << new_func_name;
VLOG(2) << "Graph has node " << n->type_string() << ". Corresponding function: " << func.name();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/tf2xla/shape_util.cc
VLOG(4) << "Shape[" << i << "] = " << xla::ShapeUtil::HumanStringWithLayout(shapes.back());
VLOG(4) << "Shape[] = " << xla::ShapeUtil::HumanStringWithLayout(*output_shape);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/tf2xla/xla_op_registry.cc
VLOG(1) << "LaunchOpHasKernelForDevice" << " kernel_class_name: " << kernel_class_name;
LOG(WARNING) << "Registrations of " << x.name << " have incompatible compilation_only settings.";
LOG(WARNING) << "Registrations of " << x.name << " have incompatible allow_resource_types settings.";
LOG(WARNING) << "Registrations of " << x.name << " have incompatible allow_variant_types settings.";
LOG(WARNING) << "Registrations of " << x.name << " have incompatible allow_string_type settings.";
LOG(WARNING) << "Duplicate registrations of " << x.name << "with no device whitelists.";
LOG(WARNING) << "Multiple registrations of " << x.name << " on device " << device;
LOG(WARNING) << "Registrations of " << x.name << " have incompatible compile time constant inputs.";
LOG(WARNING) << "Registrations of " << x.name << " have incompatible values for is_metadata_op.";
VLOG(2) << "tf_xla_cpu_global_jit = " << cpu_global_jit;
LOG(ERROR) << lookup_status.error_message();
XLA_LOG_LINES( ERROR, "Ops registered: " + dynamic_cast<OpRegistry*>(op_registry)->DebugString(true));
LOG(FATAL) << "Unknown type attribute " << constraint.first << " in XLA op registration for " << op_name;
VLOG(2) << "XLA op registration: device: " << backend.first << " op: " << op_name;
LOG(FATAL) << "XLA op registration " << registration->name << " is incompatible with existing registration of the same name.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/tf2xla/kernels/reduction_ops_common.cc
VLOG(1) << "ReductionOp: " << ctx->op_kernel().name();
VLOG(1) << "data shape: " << data_shape.DebugString();
VLOG(1) << "axes : " << absl::StrJoin(axes, ",");

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/tf2xla/kernels/random_ops.cc
LOG_FIRST_N(WARNING, 1) << "Warning: Using tf.random.uniform with XLA compilation will ignore " "seeds; consider using tf.random.stateless_uniform instead if " "reproducible behavior is desired.";
LOG_FIRST_N(WARNING, 1) << "Warning: Using tf.random.shuffle with XLA compilation " "will ignore seeds.";
LOG_FIRST_N(WARNING, 1) << "Warning: Using tf.random.uniform with XLA compilation will ignore " "seeds; consider using tf.random.stateless_uniform instead if " "reproducible behavior is desired.";
LOG_FIRST_N(WARNING, 1) << "Warning: Using tf.random.truncated_normal with XLA " "compilation will ignore seeds; consider using " "tf.random.stateless_truncated_normal instead if " "reproducible behavior is desired.";
LOG_FIRST_N(WARNING, 1) << "Warning: Using tf.random.truncated_normal with XLA " "compilation will ignore seeds; consider using " "tf.random.stateless_truncated_normal instead if " "reproducible behavior is desired.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/tf2xla/kernels/while_op.cc
VLOG(2) << "Num inputs " << ctx->num_inputs();
VLOG(2) << " Input " << i << " type: " << DataTypeString(ctx->input_type(i)) << " shape: " << ctx->InputShape(i).DebugString();
VLOG(2) << " resource " << resource->name() << " type: " << DataTypeString(arg.type) << " shape: " << arg.ShapeHumanString() << " initialized: " << arg.initialized;
VLOG(1) << "WhileOp::Compile";
VLOG(1) << "Compiling body";
VLOG(2) << "Recompiling loop body: has_uninitialized_vars: " << has_uninitialized_vars << " has_tensor_arrays: " << has_tensor_arrays << " has_uninitialized_tensor_lists: " << has_uninitialized_tensor_lists;
VLOG(2) << "Update shape for argument " << update.input_index << " " << update.shape.DebugString();
VLOG(4) << "TensorArray " << resource->name() << " accessed gradient " << grad_source;
VLOG(1) << "Recompiling body with corrected resource shapes";
VLOG(1) << "Compiling condition";
VLOG(2) << "Body shape: " << xla::ShapeUtil::HumanString(body_input_shape) << " -> " << xla::ShapeUtil::HumanString(body.xla_output_shape);
VLOG(2) << "Cond shape: " << xla::ShapeUtil::HumanString(cond_input_shape) << " -> " << xla::ShapeUtil::HumanString(cond.xla_output_shape);
VLOG(1) << "Building while loop";
VLOG(2) << "Loop-carried variable: pos: " << update.input_index << " name: " << resource->name() << " modified: " << update.modified << " type: " << DataTypeString(update.type) << " shape: " << update.shape.DebugString();
VLOG(1) << "Done building while loop";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/tf2xla/kernels/reshape_op.cc
VLOG(2) << "Reshape from " << input_shape.DebugString() << " to " << shape.DebugString() << ", unknown_index=" << unknown_index;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/tf2xla/kernels/if_op.cc
VLOG(1) << "Building If: " << input_types_.size() << " inputs";
VLOG(2) << "Resource " << resource->name() << " type: " << DataTypeString(arg.type) << " shape: " << arg.HumanString() << " initialized: " << arg.initialized;
VLOG(2) << "Arg type: " << DataTypeString(arg.type) << " shape: " << arg.HumanString();
VLOG(5) << "TensorArray " << resource->name() << " accessed gradient " << grad_source;
VLOG(2) << "Input shape: " << xla::ShapeUtil::HumanString(then_input_shape);
VLOG(2) << "Output shape: " << xla::ShapeUtil::HumanString(then_result.xla_output_shape);
LOG(INFO) << "Setting output " << i;
LOG(INFO) << "Shape for output " << i << ": " << xla::ShapeUtil::HumanString(shape_or.ValueOrDie());
LOG(INFO) << "Shape unknown for output " << i;
VLOG(2) << "If variable: pos: " << update.input_index << " name: " << resource->name() << " modified: " << update.modified << " type: " << DataTypeString(update.type) << " shape: " << update.shape.DebugString();
VLOG(1) << "Done building If";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/tf2xla/kernels/check_numerics_op.cc
LOG(WARNING) << "Ignoring CheckNumerics operator " << name();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/tf2xla/kernels/concat_op.cc
VLOG(1) << "ConcatOffset " << cdim << "," << inp0_rank;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/tf2xla/kernels/assert_op.cc
LOG(WARNING) << "Ignoring Assert operator " << name();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/tf2xla/kernels/categorical_op.cc
LOG_FIRST_N(WARNING, 1) << "Warning: Using tf.random.categorical with XLA" " compilation will ignore seeds."

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/tf2xla/kernels/case_op.cc
VLOG(1) << "Building Case: " << input_types_.size() << " inputs";
VLOG(2) << "Resource " << resource->name() << " type: " << DataTypeString(arg.type) << " shape: " << arg.HumanString() << " initialized: " << arg.initialized;
VLOG(2) << "Arg type: " << DataTypeString(arg.type) << " shape: " << arg.HumanString();
VLOG(5) << "TensorArray " << resource->name() << " accessed gradient " << grad_source;
VLOG(2) << "Input shape: " << xla::ShapeUtil::HumanString(branch0_input_shape);
VLOG(2) << "Output shape: " << xla::ShapeUtil::HumanString( branch_results[0].xla_output_shape);
LOG(INFO) << "Setting output " << i;
LOG(INFO) << "Shape for output " << i << ": " << xla::ShapeUtil::HumanString(shape_or.ValueOrDie());
LOG(INFO) << "Shape unknown for output " << i;
VLOG(2) << "Case variable: pos: " << update.input_index << " name: " << resource->name() << " modified: " << update.modified << " type: " << DataTypeString(update.type) << " shape: " << update.shape.DebugString();
VLOG(1) << "Done building Case";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/tf2xla/lib/scatter.cc
VLOG(3) << "Scatter op:";
VLOG(3) << " Input: " << xla::ShapeUtil::HumanString(buffer_shape);
VLOG(3) << " Indices: " << xla::ShapeUtil::HumanString(indices_shape);
VLOG(3) << " Updates: " << xla::ShapeUtil::HumanString(updates_shape);
VLOG(3) << " Scatter Dimension Numbers: ";
VLOG(3) << " index_vector_dim: " << dim_numbers.index_vector_dim();
VLOG(3) << " update_window_dims: [" << absl::StrJoin(dim_numbers.update_window_dims(), ",") << "]";
VLOG(3) << " inserted_window_dims: [" << absl::StrJoin(dim_numbers.inserted_window_dims(), ",") << "]";
VLOG(3) << " scatter_dims_to_operand_dims: [" << absl::StrJoin(dim_numbers.scatter_dims_to_operand_dims(), ",") << "]";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/tf2xla/lib/util.cc
LOG(FATAL) << "unhandled element type " << type;
LOG(FATAL) << "pred element type is not integral";
LOG(FATAL) << "tuple element type is not integral";
LOG(FATAL) << "opaque element type is not integral";
LOG(FATAL) << "unhandled element type " << type;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/mlir/xla/xla_mlir_translate.cc
LOG(ERROR) << "Failed to load proto";
LOG(ERROR) << "Hlo module import failed: " << status;
LOG(ERROR) << "HLO Module loading failed: " << hlo_module_error.status();
LOG(ERROR) << "HLO Module import failed: " << status;
LOG(ERROR) << "Module conversion failed: " << status;
LOG(ERROR) << "Module conversion failed: " << status;
LOG(ERROR) << "Conversion to HLO module failed: " << statusOrHloModule.status();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/mlir/lite/mlir_tflite_runner.cc
LOG(QFATAL) << "Unsupported type: " << TfLiteTypeGetName(tensor.type);
LOG(ERROR) << argv[0] << ": could not open input file '" << input_filename << "': " << error.message() << "";
LOG(QFATAL) << "NYI: Only nullary functions supported.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/mlir/lite/tf_to_tfl_flatbuffer.cc
LOG(ERROR) << "OpDef parsing failed for: " << tf_opdefs_string;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc
LOG(WARNING) << "Ignored output_format.";
LOG(WARNING) << "Ignored drop_control_dependency.";
LOG(WARNING) << "Ignored reorder_across_fake_quant.";
LOG(WARNING) << "Ignored change_concat_input_ranges.";
LOG(WARNING) << "Ignored dump_graphviz_video.";
LOG(WARNING) << "Allow allow_nonexistent_arrays.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/mlir/tensorflow/translate/import_model.cc
VLOG(1) << "Found " << (back_edge_helper_.RemovedEdges().size()) << " backedges.";
VLOG(1) << "Pruned unused nodes in graphdef";
VLOG(1) << "No unused nodes in graphdef to prune";
VLOG(1) << "No output nodes specified, skipping pruning";
VLOG(1) << "Pruning unused nodes in graphdef is disabled";
VLOG(1) << "Inferring graph shapes to fixpoint";
LOG(WARNING) << "Graph shapes did not converge to a fixpoint within " << kMaxIterationCount << " iterations. Graph shapes may be conservative.";
VLOG(1) << "Graph shapes were inferred with " << (i - 1) << " extra rounds of analysis to reach a fixpoint.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/mlir/tensorflow/translate/tf_mlir_translate.cc
LOG(ERROR) << "Graph import failed: " << module_or.status();
LOG(ERROR) << "Failed to load saved model '" << saved_model_dir << "': " << load_status;
LOG(ERROR) << "SavedModel import failed: " << module_or.status();
LOG(ERROR) << "Failed to load saved model v1 '" << saved_model_dir << "': " << load_status;
LOG(ERROR) << "SavedModel V1 import failed: " << module_or.status();
LOG(ERROR) << "Graph import failed: " << module_or.status();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/mlir/tensorflow/translate/mlir_roundtrip_pass.cc
VLOG(1) << "Roundtripping: " << it.first;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/mlir/tensorflow/translate/tf_mlir_translate_registration.cc
LOG(ERROR) << "Graph export failed: " << graphdef_or.status();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/mlir/tensorflow/utils/import_utils.cc
LOG(ERROR) << "Error parsing Protobuf";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/mlir/tensorflow/utils/error_util.cc
VLOG(1) << diag_str_.substr(current_diag_str_size_);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/mlir/tensorflow/utils/eval_util.cc
VLOG(2) << TF_Message(s);
VLOG(2) << s.error_message();
VLOG(1) << "Can't evaluate with null context.";
VLOG(1) << "Can't evaluate since not all operands are constant.";
VLOG(1) << "Start to evaluate node: " << node_def->DebugString();
VLOG(1) << "Evaluate node " << node_name << " successfully!";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc
LOG(INFO) << absl::string_view(ptr, size);
LOG(WARNING) << "Failed to create '" << dir << "' directory for dumping: " << status;
LOG(WARNING) << "Failed to create file '" << filepath << "': " << status;
LOG(INFO) << "Dumped MLIR operation '" << op->getName().getStringRef().str() << "' to '" << filepath << "'";
LOG(WARNING) << "Failed to dump MLIR module because dump location is not " << " specified through TF_DUMP_GRAPH_PREFIX environment variable.";
LOG(WARNING) << "TF_DUMP_GRAPH_PREFIX=sponge but " "TEST_UNDECLARED_OUTPUT_DIRS is not set";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/mlir/tensorflow/transforms/dialect_hooks.cc
VLOG(2) << decoded_attr_or.status().error_message();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/jit/xla_kernel_creator_util.cc
VLOG(3) << "Attempting to create XlaLaunchOp for " << node_def.DebugString();
VLOG(1) << message;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/jit/clone_constants_for_better_clustering.cc
VLOG(2) << "Cloning small host constant " << input->name();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/jit/encapsulate_xla_computations_pass.cc
VLOG(4) << "variable_start_index: " << variable_start_index;
VLOG(1) << "Subgraph fingerprint:" << fingerprint;
VLOG(4) << "Launch node '" << launch->name() << "'" << " input edges: " << in_edges.size() << " num_args: " << num_args << " num_variables: " << num_variables;
VLOG(2) << "Replacing with XlaLaunch";
VLOG(2) << "Device is " << launch->requested_device();
VLOG(2) << "Deleting node " << node->DebugString();
VLOG(1) << "EncapsulateXlaComputations(): " << DumpGraphToFile("encapsulate_xla_computations_before", **options.graph, options.flib_def);
VLOG(1) << "EncapsulateXlaComputations() half-way: " << DumpGraphToFile("encapsulate_xla_computations_halfway", **options.graph, options.flib_def);
VLOG(1) << "EncapsulateXlaComputations() finished: " << DumpGraphToFile("encapsulate_xla_computations_after", **options.graph, options.flib_def);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/jit/xla_compilation_cache.cc
LOG(ERROR) << "Error synchronizing activity while waiting for all " "programs to complete";
VLOG(2) << "Compiling to local executable";
LOG(INFO) << "Compiled cluster using XLA! This line is logged at most " "once for the lifetime of the process.";
VLOG(2) << "XlaCompilationCache::Compile " << DebugString();
VLOG(2) << "num_inputs=" << args.size();
VLOG(3) << i << ": " << args[i].HumanString();
VLOG(2) << "Signature: " << signature.HumanString();
VLOG(1) << "Marking " << function.name() << " as megamorphic, compile_count=" << it->second.compile_count << " execution_count=" << it->second.execution_count;
VLOG(2) << "Compilation cache entry hit: " << entry->compiled << " signature: " << signature.HumanString() << " with request count " << current_request_count << " and compile threshold " << compile_threshold.value_or(0);
VLOG(3) << "Not compiling cluster " << function.name() << " because it is megamorphic.";
VLOG(3) << "Not compiling cluster " << function.name() << " because it has not reached compile threshold; threshold is " << *compile_threshold << " execution count " << current_request_count << ".";
VLOG(2) << "Not compiling for signature: " << signature.HumanString();
VLOG(1) << "compiled " << function.name() << " " << it->second.compile_count << " times, compile time: " << compile_time_us << " us, cumulative: " << it->second.cumulative_compile_time_us << " us (" << tensorflow::strings::HumanReadableElapsedTime(compile_time_us / 1.0e6) << " / " << tensorflow::strings::HumanReadableElapsedTime( it->second.cumulative_compile_time_us / 1.0e6) << ")";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/jit/xla_tensor.cc
VLOG(4) << shaped_buffer.ToString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/jit/xla_compile_on_demand_op.cc
VLOG(2) << "Executing computation: " << name();
VLOG(2) << name() << ": " << *arg;
LOG(ERROR) << "Copying tensor of shape " << device_tensor.shape().DebugString() << " from " << ctx->device()->name() << "to CPU failed with " << status.ToString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/jit/xla_device_context.cc
VLOG(2) << "CopyCPUTensorToDevice empty tensor";
VLOG(2) << "CopyCPUTensorToDevice use_fast_mem " << use_fast_mem_ << " " << this << " " << reinterpret_cast<const void*>(cpu_tensor->tensor_data().data()) << " " << reinterpret_cast<const void*>(device_tensor->tensor_data().data()) << " " << cpu_tensor->NumElements() << " " << cpu_tensor->shape().DebugString() << " " << device_tensor->shape().DebugString();
VLOG(2) << "Transfer to device as literal: " << literal.ToString() << " " << xla_tensor->shaped_buffer().ToString();
VLOG(2) << "CopyDeviceTensorToCPU empty tensor";
VLOG(2) << "CopyDeviceTensorToCPU " << reinterpret_cast<const void*>(device_tensor->tensor_data().data()) << " " << reinterpret_cast<const void*>(cpu_tensor->tensor_data().data()) << " " << device_tensor->NumElements() << " " << cpu_tensor->shape().DebugString() << " " << device_tensor->shape().DebugString();
VLOG(2) << "Transfer from device as literal: " << xla_tensor->shaped_buffer().ToString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/jit/deadness_analysis.cc
VLOG(4) << "For " << n->name() << ":" << output_idx << " from " << insert_result.first->second->ToString() << " " << insert_result.first->second << " to " << pred->ToString() << " " << pred;
VLOG(4) << "Visiting " << curr_node->name();
VLOG(2) << "Done populating frame " << cur_frame_name << " using the " << (success ? "optimistic" : "pessimistic") << " mode.";
VLOG(4) << "Visiting " << n->name();
VLOG(4) << "Revisiting " << n->name();
VLOG(2) << "Running the optimistic mode on frame " << frame_name << " does not converge because node " << merge->name() << " cannot be mapped into the AndRecurrence form.";
VLOG(2) << "Running the optimistic mode on frame " << frame_name << " does not converge. Seeing different Merge predicates: " << curr_andrec->ToString() << " and " << prev_andrec->ToString();
VLOG(2) << tensor_id.ToString() << " -> " << it->second->ToString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/jit/encapsulate_subgraphs_pass.cc
VLOG(1) << "Guaranteed const found: " << src_arg.first->DebugString();
VLOG(2) << "ConnectSequencerToCallNode";
VLOG(2) << "Build function def " << name;
VLOG(2) << "Replace function def " << name;
VLOG(2) << "Inlining function " << node->name();
VLOG(1) << "EncapsulateSubgraphsPass::Run";
VLOG(3) << "Has ref vars = " << has_ref_vars << ", node: " << node->def().SerializeAsString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/jit/resource_operation_safety_analysis.cc
VLOG(2) << "Unsafe edge: " << NodeToString(*g.FindNodeId(incoming_op.first), incoming_op.second) << " -> " << NodeToString(*n, *op_kind);
VLOG(3) << n->name() << " -> " << ResourceOpSetToString(*resource_op_set);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/jit/mark_for_compilation_pass.cc
VLOG(3) << "Could not contract " << cluster_from->DebugString(*graph_) << " -> " << cluster_to->DebugString(*graph_) << " because contracting the edge would create a cycle via " << DescribePotentialCycle(from, to) << ".";
VLOG(2) << "No compilable candidates";
VLOG(2) << "Could not form cycle detection graph";
VLOG(4) << "Running phase 0";
VLOG(4) << "Running phase 1";
VLOG(4) << "Running phase 2";
VLOG(2) << "Checking idempotence";
VLOG(3) << "Assigning node " << n->name() << " to cluster " << name;
VLOG(2) << "XLA clustering will only consider the following TF operations: " << absl::StrJoin(vwhitelist, " ");
VLOG(2) << "Starting fuel: infinity";
VLOG(2) << "Starting fuel: " << *debug_options_.fuel;
VLOG(2) << "sorted_nodes.size() = " << sorted_nodes.size();
VLOG(1) << "Hit fuel limit; not marking any remaining ops as clusterable.";
VLOG(4) << "Device type for " << node->name() << ": " << device_type.type_string();
VLOG(2) << "Not clustering " << node->name() << ": disallowed by _XlaCompile attribute";
VLOG(2) << "Rejecting " << node->name() << ": could not find JIT device for " << device_type.type();
VLOG(1) << "Rejecting TF operation " << node->def().op() << " as it is not listed in --tf_xla_ops_to_cluster.";
VLOG(2) << "Isolating " << node->name() << ": must-be-constant stateful op";
VLOG(2) << "Rejecting " << node->name() << ": including it can create dependencies between while loop " "condition and body computations with runtime overhead.";
VLOG(2) << "compilation_candidates_.size() = " << compilation_candidates_.size();
VLOG(2) << "Rejecting " << node->name() << ": kXlaCompileAttr(" << kXlaCompileAttr << ") is false.";
VLOG(2) << "Rejecting " << node->name() << ": kXlaCompileAttr(" << kXlaCompileAttr << ") on callee is false.";
VLOG(3) << EdgeContractionFailureMsg(from, to, reason);
VLOG(3) << EdgeContractionFailureMsg( from, to, absl::StrCat( "the two nodes have mismatching deadness: ", deadness_analysis_->DebugString(*from->deadness_predicate()), " and ", deadness_analysis_->DebugString(*to->deadness_predicate())));
VLOG(2) << "*** Clustering info for graph of size " << graph_->num_nodes();
VLOG(2) << " Built " << auto_clustering_info.clusters_size() << " clusters, size " << RatioToString(auto_clustering_info.clustered_node_count(), graph_->num_nodes());
VLOG(2) << " " << cluster_name << " " << RatioToString(size, graph_->num_nodes());
VLOG(3) << " " << op_count.op() << ": " << op_count.count() << " instances";
VLOG(2) << " Unclustered nodes: " << RatioToString(auto_clustering_info.unclustered_node_count(), graph_->num_nodes());
VLOG(3) << " " << op_count.op() << ": " << op_count.count() << " instances";
VLOG(4) << "*** Inter-Cluster edges:";
VLOG(4) << " [none]";
VLOG(4) << " " << it->second.size() << " " << desc << " edges";
VLOG(4) << " " << edge_info_count_pair.first.GetClusterName() << " " << edge_info_count_pair.first.node_name << " # " << edge_info_count_pair.second;
VLOG(4) << " No " << desc << " edges.";
VLOG(4) << " ** Cluster " << cluster_name;
LOG(WARNING) << "(One-time warning): Not using XLA:CPU for cluster because envvar " "TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set. If you want " "XLA:CPU, either set that envvar, or use experimental_jit_scope " "to enable XLA:CPU. To confirm that XLA is active, pass " "--vmodule=xla_compilation_cache=1 (as a proper command-line " "flag, not via TF_XLA_FLAGS) or set the envvar " "XLA_FLAGS=--xla_hlo_profile.";
LOG(WARNING) << "(Although the tf_xla_cpu_global_jit flag is currently enabled, " "perhaps it wasn't enabled at process startup?)";
VLOG(3) << (should_compile ? "Compiling" : "Not compiling") << " cluster with device " << device_info_cache_.GetNameFor(chosen_device);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/jit/xla_gpu_device.cc
LOG(INFO) << "Not creating XLA devices, tf_xla_enable_xla_devices not set";
VLOG(1) << "Failed to create XLA_GPU device: " << platform.status();
LOG(INFO) << "Not creating XLA devices, tf_xla_enable_xla_devices not set";
VLOG(1) << "Failed to create XLA_GPU device: " << platform.status();
LOG(INFO) << "Ignoring visible " << DEVICE_GPU_XLA_JIT << " device. Device number is " << i << ", reason: " << status;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/jit/shape_inference.cc
VLOG(1) << "Shape inference failed for node " << n->name() << ": " << status;
VLOG(4) << "Output " << i << " for node " << n->name() << ": " << context->DebugString(handle);
VLOG(4) << node->name() << " output " << i << " shape" << output.shape.DebugString() << " handle_type " << DataTypeString(output.handle_type) << " handle_shape " << output.handle_shape.DebugString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/jit/xla_activity_listener.cc
VLOG(2) << "OptimizationRemark: " << optimization_remark.DebugString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/jit/introduce_floating_point_jitter_pass.cc
VLOG(1) << "No users for " << TensorId(n->name(), oidx).ToString();
VLOG(1) << "Updating " << edges_to_update.size() << " users for " << TensorId(n->name(), oidx).ToString();
VLOG(3) << "Updating " << e->dst()->name();
VLOG(3) << "Nothing to do";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/jit/xla_activity_logging_listener.cc
VLOG(3) << "Logging XlaAutoClusteringActivity disabled";
VLOG(2) << "Logging XlaAutoClusteringActivity";
VLOG(3) << auto_clustering_activity.DebugString();
VLOG(2) << "Not logging: logger not ready yet.";
VLOG(3) << "Logging XlaJitCompilationActivity disabled";
VLOG(2) << "Logging XlaJitCompilationActivity";
VLOG(3) << jit_compilation_activity.DebugString();
VLOG(2) << "Not logging: logger not ready yet.";
VLOG(3) << "Logging XlaJitCompilationActivity disabled";
VLOG(2) << "Logging XlaJitCompilationActivity";
VLOG(3) << optimization_remark.DebugString();
VLOG(2) << "Not logging: logger not ready yet.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/jit/partially_decluster_pass.cc
VLOG(3) << n->DebugString();
VLOG(3) << "Declustering must-be-constant node " << n->name();
VLOG(2) << "Declustering " << n->name() << " because it is a root shape consumer";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/jit/node_matchers.cc
LOG(FATAL) << "Unsupported dtype " // Crash ok: testonly. << DataType_Name(tensor.dtype());

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/jit/xla_launch_util.cc
VLOG(4) << "Acquiring lock for variable " << reinterpret_cast<void*>(variable);
VLOG(4) << "Finished acquiring variable locks.";
VLOG(2) << "Result tuple shape: " << output.on_host_shape().DebugString();
VLOG(2) << "Result tuple shape (on device): " << output.on_device_shape().DebugString();
VLOG(1) << "Constant output tensor on device";
VLOG(2) << "Retval " << i << " shape " << shape.DebugString() << " type " << DataTypeString(type);
VLOG(3) << ctx->mutable_output(i)->DeviceSafeDebugString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/jit/increase_dynamism_for_auto_jit_pass.cc
VLOG(3) << "Rewriting slice " << slice->name() << " to a "static shaped" Slice";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/jit/extract_outside_compilation_pass.cc
VLOG(4) << "Added HostCompute node: " << host_compute_node->DebugString();
VLOG(4) << "Expanding host graph " << host_func;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/jit/build_xla_ops_pass.cc
VLOG(3) << ndef.DebugString();
VLOG(2) << "For " << function_name << " PickDeviceForXla(" << device_info_cache->DebugString(device_set) << ") -> " << device_info_cache->GetNameFor(result);
VLOG(1) << "print_outputs = " << debugging_opts.print_outputs;
VLOG(1) << "check_input_numerics = " << debugging_opts.check_input_numerics;
VLOG(1) << "check_output_numerics = " << debugging_opts.check_output_numerics;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/jit/xla_cpu_device.cc
LOG(INFO) << "Not creating XLA devices, tf_xla_enable_xla_devices not set";
LOG(INFO) << "Not creating XLA devices, tf_xla_enable_xla_devices not set";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/jit/xla_device_ops.cc
LOG(FATAL) << "Attempted to execute Op " << name() << " type " << type_string() << " on an XLA device. This should never happen.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/jit/compilability_check_util.cc
VLOG(3) << "Found uncompilable node " << node.name() << " (op " << node.type_string() << ")" << (reason.empty() ? "" : ": ") << reason;
VLOG(2) << "Rejecting " << node.name() << ": Identity with unsafe cast.";
VLOG(2) << "Rejecting node " << node.name() << ": " << uncompilable_reason << ".";
VLOG(2) << "Rejecting node " << node.name() << ": can't compile : " << call.op();
VLOG(2) << "Rejecting " << call_def.op() << ": " << uncompilable_reason << ".";
VLOG(2) << "Rejecting " << call_def.DebugString() << ": " << uncompilable_reason << " : " << s;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/jit/xla_device.cc
VLOG(1) << "Created XLA device " << options.compilation_device_name << " " << this;
VLOG(1) << "Destroying XLA device " << jit_device_name_ << " " << this;
VLOG(1) << "XlaDevice " << this << " new " << name << " " << (*stream)->DebugStreamPointers();
VLOG(1) << "XlaDevice " << this << " new XlaDeviceContext(fast_mem=false) " << device_context_;
VLOG(1) << "XlaDevice " << this << " new XlaDeviceContext(fast_mem=true) " << fast_mem_device_context_;
VLOG(1) << "XlaDevice " << this << " new GpuDeviceInfo " << gpu_device_info_.get();
LOG(WARNING) << "XLA_GPU and XLA_CPU devices are deprecated and will be " "removed in subsequent releases. Instead, use either " "@tf.function(experimental_compile=True) for must-compile " "semantics, or run with TF_XLA_FLAGS=--tf_xla_auto_jit=2 " "for auto-clustering best-effort compilation.";
VLOG(2) << "XlaDevice::Compute " << op_kernel->name() << ":" << op_kernel->type_string();
VLOG(2) << "XlaDevice::ComputeAsync " << op_kernel->name() << ":" << op_kernel->type_string();
VLOG(1) << "XlaDevice::Sync";
VLOG(1) << "XlaDevice::Sync completed";
VLOG(1) << "XlaDevice::Sync (asynchronous)";
VLOG(2) << "Allocated tensor at " << DMAHelper::base(tensor);
VLOG(1) << "XlaDevice::MakeTensorFromProto";
VLOG(1) << "XlaDevice::MakeFastMemTensorFromProto";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/jit/xla_cluster_util.cc
VLOG(2) << "Node " << node.def().ShortDebugString() << " has ref input " << incoming_node->name() << " " << incoming_node->type_string();
VLOG(1) << "Cycle detected when adding " << src_type << "->" << dst_type << " edge: " << DescribeCycle(cycles, *graph, src, dst);
VLOG(4) << "GetGlobalJitLevelForGraph returning " << xla_global_jit_level.single_gpu;
VLOG(4) << "GetGlobalJitLevelForGraph returning " << result;
VLOG(2) << "Could not find " << call_target.name() << " in the function library.";
VLOG(2) << "# iterations = " << iterations;
VLOG(1) << "GetNodesRelatedToRefVariables() found " << result.size() << " nodes";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/jit/encapsulate_util.cc
VLOG(4) << "Oc -> oc edge: " << e->DebugString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/jit/kernels/xla_ops.cc
VLOG(1) << "XlaLocalLaunchOpBase::Compute " << Canonicalize(function_.name(), AttrSlice(&function_.attr()));
VLOG(1) << "Executing XLA Computation...";
VLOG(2) << "Executing computation.";
LOG(ERROR) << "ThenExecute failed " << status;
VLOG(2) << "Elapsed time: " << elapsed << "us";
VLOG(1) << "Done";
VLOG(1) << "XlaLocalLaunchOp destroyed";
VLOG(3) << "XlaCompileOp " << def().name() << (must_compile_ ? "(must-compile)" : "");
LOG(WARNING) << "Compilation failed:" << status.ToString() << ". Falling back to TF function call.";
VLOG(3) << "XlaRunOp " << def().name();
LOG(ERROR) << "ThenExecute failed " << status;
VLOG(2) << "Elapsed time in computation: " << elapsed << "us";
VLOG(3) << "XlaMergeOp " << def().name();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/jit/graphcycles/graphcycles.cc
LOG(FATAL) << "Did not clear visited marker on node " << x;
LOG(FATAL) << "Duplicate occurrence of rank " << nx->rank;
LOG(FATAL) << "Edge " << x << "->" << y << " has bad rank assignment " << nx->rank << "->" << ny->rank;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xrt/xrt_state.cc
VLOG(3) << "Allocating literal buffer: host_shape=" << xla::ShapeUtil::HumanStringWithLayout(shape) << " device_shape=" << xla::ShapeUtil::HumanStringWithLayout(on_device_shape);
VLOG(2) << "Allocated buffer at " << index_to_buffer.second.opaque() << " index " << index_to_buffer.first.ToString() << " (" << size << " bytes)";
LOG(INFO) << "XRT Allocation Stats: device=" << device_ordinal_ << " count=" << stats.count << " size=" << stats.size;
VLOG(2) << "Freed buffer at " << allocation_.opaque() << " (" << allocation_.size() << " bytes)";
VLOG(2) << "Allocated buffer at " << buffer->opaque() << " index " << index.ToString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xrt/xrt_compilation_cache.cc
VLOG(1) << "Created compilation cache max " << max_cache_entries_ << " entries.";
VLOG(1) << "XRTCompilationCache::~XRTCompilationCache()";
VLOG(1) << "After releasing entry " << uid << " refs cache is " << cache_.size() << " entries (" << cache_entries_ + marked_for_eviction_entries_ << "), marked for eviction " << (cache_.size() - entries_by_last_use_.size()) << " entries (" << marked_for_eviction_entries_ << ").";
LOG(FATAL) << "Tried to discard nonexistent cache entry";
VLOG(1) << "Marking " << entry_to_mark->key << " for eviction";
VLOG(1) << "Before adding new entry for key " << key << " cache is " << cache_.size() << " entries (" << cache_entries_ + marked_for_eviction_entries_ << "), " << " marked for eviction " << (cache_.size() - entries_by_last_use_.size()) << " entries (" << marked_for_eviction_entries_ << ").";
VLOG(1) << "Before refreshing entry for key " << key << " cache is " << cache_.size() << " entries (" << cache_entries_ + marked_for_eviction_entries_ << "), " << " marked for eviction " << (cache_.size() - entries_by_last_use_.size()) << " entries (" << marked_for_eviction_entries_ << ").";
VLOG(1) << "After refreshing entry for key " << key << " cache is " << cache_.size() << " entries (" << cache_entries_ + marked_for_eviction_entries_ << "), " << " marked for eviction " << (cache_.size() - entries_by_last_use_.size()) << " entries (" << marked_for_eviction_entries_ << ").";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xrt/xrt_util.cc
LOG(WARNING) << "Passing through XLA debug options!";
LOG(WARNING) << "TF_XLA_DEBUG_OPTIONS_PASSTHROUGH not set, not all options " "will be retained";
LOG(WARNING) << "Invalid config path (will be dropped): " << path;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xrt/xrt_memory_manager.cc
VLOG(4) << "CompactAllocations started";
VLOG(4) << "CompactAllocations finished: " << status;
VLOG(3) << "Swapped out " << swapped_size << " bytes";
VLOG(4) << "Allocate of " << size << " bytes failed on device " << device_ordinal;
VLOG(4) << "Allocate of " << size << " bytes on device " << device_ordinal << ": " << status;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xrt/kernels/xrt_execute_op.cc
VLOG(2) << "Released allocation handle " << input_coords[i].handle;
VLOG(2) << "Executing computation.";
VLOG(2) << "Elapsed time: " << elapsed << "us";
VLOG(1) << "XRTExecuteOp::Compute";
VLOG(2) << "Released compilation handle " << compilation_handle;
VLOG(1) << "XRTExecuteChainedOp::Compute";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xrt/kernels/xrt_state_ops.cc
VLOG(1) << "XRTMetricsCollectOp::Compute";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xrt/kernels/xrt_compile_ops.cc
VLOG(1) << "Building executable";
VLOG(1) << "XRTCompileOp::Compute";
VLOG(1) << "Compiling XLA executable";
VLOG(1) << "XRTReleaseCompilationRefOp::Compute";
VLOG(2) << "Released computation handle " << key;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/debug_ops.h
LOG(ERROR) << "Debug node of watch key " << debug_watch_key_->debug_node_name << " failed to allocate empty tensor under gated-off state.";
LOG(ERROR) << "Debug node of watch key " << debug_watch_key_->debug_node_name << " failed to publish debug tensor data to all URLs " << str_util::Join(debug_urls_, ", ") << ", due to: " << status.error_message();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/range_sampler.h
LOG(FATAL) << "Update not supported for this sampler type.";
LOG(FATAL) << "Should not be called";
LOG(FATAL) << "Should not be called";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/conv_2d_gpu.h
LOG(ERROR) << "Unsupported filter format: " << ToString(dst_filter_format);
LOG(FATAL) << "Unsupported filter format: " << ToString(src_filter_format);
LOG(FATAL) << "Invalid data format: " << format;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/reduction_gpu_kernels.cu.h
LOG(FATAL) << ss.str();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/gpu_utils.h
VLOG(1) << GetActionSummary("creates", params, config);
VLOG(1) << GetActionSummary("demotes", params, config);
VLOG(1) << GetActionSummary("erases", params, config);
VLOG(1) << GetActionSummary("promotes", params, config);
VLOG(1) << GetActionSummary("accepts", params, config);
VLOG(1) << GetActionSummary("creates", params, config);
VLOG(1) << GetActionSummary("promotes", params, config);
VLOG(1) << GetActionSummary("promotes", params, config);
VLOG(1) << GetActionSummary("accepts", params, config);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/reduction_ops_common.h
VLOG(1) << "data shape: " << data.shape().DebugString();
VLOG(1) << "axes : " << axes.SummarizeValue(10);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/mkl_tfconv_op.h
VLOG(1) << "MKLToTFConversion complete successfully."; }
VLOG(1) << "MKLToTFConversion: No conversion needed, " << "copying input to output";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/conv_ops_fused_impl.h
LOG(FATAL) << "Unsupported fusion type";
VLOG(2) << "FusedConv2D: in_depth = " << dimensions.in_depth << ", patch_depth = " << dimensions.patch_depth << ", input_cols = " << dimensions.input_cols << ", filter_cols = " << dimensions.filter_cols << ", input_rows = " << dimensions.input_rows << ", filter_rows = " << dimensions.filter_rows << ", stride_rows = " << dimensions.stride_rows << ", stride_cols = " << dimensions.stride_cols << ", dilation_rows = " << dimensions.dilation_rows << ", dilation_cols = " << dimensions.dilation_cols << ", out_depth = " << dimensions.out_depth;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/fuzzing/fuzz_session.h
LOG(FATAL) << "Could not create session: " << status.error_message();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/neon/depthwiseconv_float.h
VLOG(1) << "DepthwiseConv2d using slow path with " << "stride = " << stride << ", " << "input_depth = " << input_depth << ", " << "depth_multiplier = " << depth_multiplier << ".";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/sparse/sparse_matrix.h
VLOG(2) << "DeviceCopy from type: " << DataTypeString(from.dtype()) << " and shape: " << from.dense_shape().DebugString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/data/iterator_ops.h
VLOG(2) << "constructor"; }
VLOG(2) << "destructor"; }

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/kernels/data/dataset_utils.h
VLOG(2) << s.ToString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/util/mkl_util.h
LOG(FATAL) << "Invalid dimension: " << dimension;
LOG(FATAL) << "Invalid dimension: " << dimension;
LOG(FATAL) << "Operation received an exception: " << error_msg;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/util/presized_cuckoo_map.h
LOG(WARNING) << "Cuckoo path finding failed: Table too small?";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/util/tensor_slice_reader.h
VLOG(1) << "Did not find slice in preferred shard, loading all shards." << name << ": " << slice.DebugString();
VLOG(1) << "Failed to seek to the record for tensor " << name << ", slice " << slice_s.DebugString() << ": computed key = " << key;
VLOG(1) << "Failed to parse the record for tensor " << name << ", slice " << slice_s.DebugString() << ": computed key = " << key;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/util/tensor_format.h
LOG(FATAL) << "Unknown format " << format;
LOG(FATAL) << "Unknown format " << format;
LOG(FATAL) << "Unknown format " << format;
LOG(FATAL) << "Unknown format " << format;
LOG(FATAL) << "Unknown format " << format;
LOG(FATAL) << "Unknown format " << format;
LOG(FATAL) << "Unknown format " << format;
LOG(FATAL) << "Unknown format " << format;
LOG(FATAL) << "Invalid dimension: " << dimension;
LOG(FATAL) << "Invalid dimension: " << dimension;
LOG(FATAL) << "Invalid dimension: " << dimension;
LOG(FATAL) << "Invalid dimension: " << dimension;
LOG(FATAL) << "Invalid format: " << static_cast<int>(format);
LOG(FATAL) << "Invalid dimension: " << dimension;
LOG(FATAL) << "Invalid dimension: " << dimension;
LOG(FATAL) << "Invalid format: " << static_cast<int>(filter_tensor_format);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/util/tensor_slice_util.h
LOG(WARNING) << s;
LOG(WARNING) << s;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/util/ctc/ctc_loss_calculator.h
VLOG(1) << "The sequence length is either zero or shorter than the " "target output (CTC works only with shorter target sequence " "than input sequence). You can turn this into a warning by " "using the flag ignore_longer_outputs_than_inputs - " << b << ": " << str_util::Join(labels[b], " ");
VLOG(2) << "label for batch: " << b << ": " << str_util::Join(label, " ");
LOG(WARNING) << "No valid path found.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/framework/resource_mgr.h
VLOG(3) << "ResourceDeleter move constructor called."; }
VLOG(3) << "ResourceDeleter copy constructor called."; }
VLOG(3) << "ResourceDeleter destructor called."; }
LOG(ERROR) << "The Encode() method is not implemented for ResourceDeleter " "objects."; }
LOG(ERROR) << "The Decode() method is not implemented for ResourceDeleter " "objects";
VLOG(3) << "Deleting Resource: " << handle.DebugString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/framework/tensor_slice.h
LOG(FATAL) << "Could not parse TensorSlice";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/framework/tensor_util.h
LOG(ERROR) << "Shape and number of values (" << values.size() << ") are incompatible.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/framework/dataset.h
LOG(WARNING) << "Dataset contains external state: " << s.ToString();
LOG(ERROR) << name << " should not contain " << kColon;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/framework/model.h
VLOG(1) << "Encountered a stop event that was not preceded by a start event.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/framework/device_base.h
LOG(FATAL) << "GetAllocator() is not implemented.";
LOG(FATAL) << "Device does not implement GetScopedAllocator()";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/framework/shared_ptr_variant.h
VLOG(3) << "Creating shared_ptr of " << shared_ptr.get() << " count is: " << shared_ptr.use_count(); }
VLOG(3) << "Moving SharedPtrVariant of " << shared_ptr.get() << " count is: " << shared_ptr.use_count(); }
VLOG(3) << "Move-assign of SharedPtrVariant of " << shared_ptr.get() << " count is: " << shared_ptr.use_count();
VLOG(3) << "Copying SharedPtrVariant of " << shared_ptr.get() << " count is: " << shared_ptr.use_count(); }
VLOG(3) << "Destroying SharedPtrVariant of " << shared_ptr.get() << " count is: " << shared_ptr.use_count(); }

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/platform/profile_utils/clock_cycle_profiler.h
LOG(WARNING) << "GetCurrentClockCycle is not implemented." << " Return 1 instead.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/platform/cloud/file_block_cache.h
LOG(ERROR) << "Attempted to monitor a NULL stats object. This may prevent the " "corresponding monitoring data from being exported";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/common_runtime/scoped_allocator.h
VLOG(1) << "~ScopedAllocatorInstance " << this; }

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/common_runtime/mkl_cpu_allocator.h
LOG(ERROR) << "tried to deallocate nullptr";
VLOG(2) << "MklCPUAllocator: In MklCPUAllocator";
LOG(WARNING) << "The user specified a memory limit " << kMaxLimitStr << "=" << user_val << " greater than available physical memory: " << max_mem_bytes << ". This could significantly reduce performance!";
LOG(ERROR) << "tried to deallocate invalid pointer";
VLOG(3) << "MklCPUAllocator: In MallocHook";
VLOG(3) << "MklCPUAllocator: In FreeHook";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/common_runtime/bfc_allocator.h
LOG(FATAL) << "Could not find Region for " << p;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/common_runtime/executor.h
VLOG(1) << "ExecutorBarrier finished with bad status: " << status;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.h
VLOG(1) << "num_pending_=" << num_pending_ << " cap=" << cap;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/common_runtime/gpu/gpu_host_allocator.h
LOG(WARNING) << "could not allocate pinned host memory of size: " << num_bytes;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/common_runtime/sycl/sycl_device.h
LOG(WARNING) << "No OpenCL GPU found that is supported by " << "ComputeCpp/triSYCL, trying OpenCL CPU";
LOG(WARNING) << "No OpenCL CPU found that is supported by " << "ComputeCpp/triSYCL, checking for host sycl device";
LOG(WARNING) << "Found SYCL host device";
LOG(FATAL) << "No SYCL host and no OpenCL GPU nor CPU" << " supported by ComputeCPP/triSYCL was found";
LOG(INFO) << "Found following OpenCL devices:";
LOG(INFO) << GetShortDeviceDescription(i);
LOG(ERROR) << "Device name cannot be given after Eigen QueueInterface destroyed";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/common_runtime/sycl/sycl_util.h
LOG(FATAL) << "Unknown data type " << src_tensor.dtype();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/common_runtime/eager/tensor_handle.h
DVLOG(3) << "Deleting TensorHandle " << this; }

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/common_runtime/eager/execute_node.h
VLOG(1) << "Unable to unprotect tensor: " << s;
VLOG(1) << "Unable to unprotect tensor: " << s;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/grappler/optimizers/graph_optimizer_stage.h
VLOG(2) << "Failed to run optimizer " << stage->optimizer_name() << ", stage " << stage->stage_name() << " node " << node->name() << ". Error: " << stage_status.error_message();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/lib/monitoring/collection_registry.h
LOG(FATAL) << "Expected collection for: " << allowed_metric_def_->name() << " but instead got: " << metric_def->name();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/distributed_runtime/rpc/grpc_call.h
VLOG(3) << "Creating ServerBidirectionalStreamingCall " << this; }
VLOG(3) << "Destroying ServerBidirectionalStreamingCall " << this; }

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/distributed_runtime/rpc/grpc_state.h
LOG(ERROR) << "GrpcMaybeUnparseProto returned with non-ok status: " << s.error_message();
VLOG(2) << "Completed call: " << method_;
VLOG(1) << method_ << " returned with non-ok status: " << s << " Retries: " << num_retries_ << " Max: " << max_retries_ << "" << context_->debug_error_string();
VLOG(1) << "Retrying call for " << method_ << "Retry: " << num_retries_ << " of " << max_retries_;
VLOG(3) << "Created new StreamingRPCState " << this;
VLOG(3) << "StreamingRPCState(" << this << ") calling grpc::StartCall";
VLOG(3) << "Destructing StreamingRPCState " << this; }
LOG(ERROR) << "GrpcMaybeUnparseProto returned with non-ok status: " << status.ToString();
VLOG(3) << "StreamingRPCState(" << this << ")::CallStarted(ok=" << ok << ")";
VLOG(3) << "StreamingRPCState(" << this << ")::RequestWriteCompleted(ok=" << ok << ")";
VLOG(3) << "StreamingRPCState(" << this << ")::ResponseReadCompleted(ok=" << ok << ")";
VLOG(3) << "StreamingRPCState(" << this << ")::CallFinished(ok=" << ok << ")";
VLOG(2) << "Ending gRPC stremaing call on the client side due to " << status.ToString();
VLOG(3) << "StreamingRPCState(" << this << ") calling grpc::Write";
VLOG(3) << "StreamingRPCState(" << this << ") calling grpc::Read";
VLOG(3) << "StreamingRPCState(" << this << ") calling grpc::Finish";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/distributed_runtime/rpc/grpc_util.h
LOG(ERROR) << "Truncated error message: " << s;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/distributed_runtime/rpc/grpc_worker_service_impl.h
LOG(FATAL) << "TODO(sanjay,jeff): Implement";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/distributed_runtime/rpc/eager/grpc_eager_service_impl.h
VLOG(1) << "local_impl_.Enqueue completed successfully";
VLOG(1) << "local_impl_.Enqueue failed with " << status.ToString() << " on request " << call->request().DebugString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/distributed_runtime/eager/eager_service_impl.h
VLOG(3) << "ServerContext: Deleting tensor handle " << handle_to_delete_->op_id << ":" << handle_to_delete_->output_num;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/core/distributed_runtime/eager/destroy_tensor_handle_node.h
LOG_EVERY_N_SEC(WARNING, 60) << "Ignoring an error encountered when deleting " "remote tensors handles: " << s.ToString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/tools/accuracy/csv_writer.h
LOG(ERROR) << "Could not write column names to file";
LOG(ERROR) << "Invalid size for row:" << values.size() << " expected: " << num_columns_;
LOG(ERROR) << "Writing to stream failed.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/tools/evaluation/stages/image_preprocessing_stage.h
LOG(ERROR) << "Type not supported";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/nnapi/NeuralNetworksShim.h
NNAPI_LOG("nnapi error: unable to open library %s", name);
NNAPI_LOG("nnapi error: unable to open function %s", name);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/toco/toco_tooling.h
LOG(QFATAL) << status.error_message();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/lite/toco/tflite/export.h
LOG(QFATAL) << status.error_message();
LOG(QFATAL) << status.error_message();
LOG(QFATAL) << status.error_message();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/examples/android/jni/object_tracking/image_data.h
LOGV("No uv data!");

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/examples/android/jni/object_tracking/utils.h
VLOG(2) << "Mean is " << mean;
VLOG(2) << "Std dev is " << std_dev;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/examples/android/jni/object_tracking/integral_image.h
LOGE("Mismatch! %d vs %d", curr_val, image_base[y][x]);
LOGE("Mismatch!");

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/examples/android/jni/object_tracking/object_detector.h
LOGV("Created %zu squares starting from size %.2f to min size %.2f " "using scale factor: %.2f", squares->size(), starting_square_size, smallest_square_size, scale_factor);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/examples/android/jni/object_tracking/flow_cache.h
LOGW("Tracking failure!");
LOGW("No points were valid!");
LOGV("Computation of cached value failed for level %d!", cache_level);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/examples/android/jni/object_tracking/time_log.h
LOGV("%s", str);
LOGE("Out of log entries!");
LOGE("Too many log entries!");
LOGD("%32s: %6.3fms %6.4fms", this_time->id, curr_time, avg_time);
LOGD("TOTAL TIME: %6.3fms %6.4fms", total_time, average_running_total);
LOGD(" ");
LOGV("%s", str);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/examples/android/jni/object_tracking/image_utils.h
VLOG(2) << "Copying from: " << area_to_copy << std::endl;
LOGV("corrected_mean: %1.2f std_dev: %1.2f", corrected_mean, std_dev);
LOG(ERROR) << "Bad image!" << std::endl;
LOG(ERROR) << *image << std::endl;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/examples/android/jni/object_tracking/image.h
LOGE("Couldn't allocate image data!");

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/examples/android/jni/object_tracking/geom.h
LOG(WARNING) << "This is not a square: " << box << std::endl;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/c/eager/tape.h
VLOG(1) << " " << t;
VLOG(1) << "Popped " << op;
VLOG(1) << "Got " << in_gradients.size() << " in_gradients for " << trace.input_tensor_id.size() << " sources";
VLOG(1) << "Tensor " << id << " not used";
VLOG(1) << "Tensor " << id << " usage count " << usage_count_it->second;
VLOG(1) << "Tensor " << id << " has no associated op. Deleting gradient";
VLOG(1) << "Tensor " << id << " is source";
VLOG(1) << "Op " << op_id << " missing " << missing_it->second << " output gradients";
VLOG(1) << "Final gradients size: " << gradients.size() - used_gradient_ids.size();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/stream_executor/dnn.h
LOG(FATAL) << "DoPoolForward not implemented for double.";
LOG(FATAL) << "DoPoolForward not implemented for float16.";
LOG(FATAL) << "DoPoolForward not implemented for int8.";
LOG(FATAL) << "DoPoolBackward not implemented.";
LOG(FATAL) << "DoPoolBackward not implemented.";
LOG(FATAL) << "DoPoolBackward not implemented.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/stream_executor/rng.h
LOG(ERROR) << "platform's random number generator does not support gaussian";
LOG(ERROR) << "platform's random number generator does not support gaussian";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/stream_executor/stream.h
LOG(WARNING) << "attempting to perform DNN operation using StreamExecutor " "without DNN support";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/stream_executor/stream_executor_pimpl.h
LOG(ERROR) << "requested sub-buffer allocation (offset + size) is greater " << "than parent allocation size: (" << element_offset << " + " << element_count << ") vs. (" << parent->ElementCount() << ")";
LOG(WARNING) << "parent failed to launch kernel: " << &kernel;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/stream_executor/host/host_gpu_executor.h
LOG(INFO) << "Shared memory configuration is unsupported for host " << "executors.";
LOG(INFO) << error_msg;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/stream_executor/cuda/cuda_fft.h
LOG(FATAL) << "Try to get cufftHandle value before initialization.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.h
LOG(ERROR) << "DoConvolveQuantized not supported by cuDNN";
LOG(ERROR) << "DoConvolveQuantized not supported by cuDNN";
LOG(ERROR) << "separable convolution not supported by CUDNN";
LOG(ERROR) << "DNN MatMulQuantized not supported by CUDNN";
LOG(ERROR) << "DNN MatMulQuantized not supported by CUDNN";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/stream_executor/rocm/rocm_dnn.h
LOG(ERROR) << "DoConvolveQuantized not supported by MIOpen";
LOG(ERROR) << "DoConvolveQuantized not supported by MIOpen";
LOG(ERROR) << "separable convolution not supported by MIOpen";
LOG(ERROR) << "DNN MatMulQuantized not supported by MIOpen";
LOG(ERROR) << "DNN MatMulQuantized not supported by MIOpen";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/stream_executor/rocm/rocm_fft.h
LOG(FATAL) << "Try to get hipfftHandle value before initialization.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/array4d.h
LOG(INFO) << "width: " << this->width();
LOG(INFO) << "height: " << this->height();
LOG(INFO) << "depth: " << this->depth();
LOG(INFO) << "planes: " << this->planes();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/overflow_util.h
LOG(FATAL) << "Invalid primitive type " << PrimitiveType_Name(ty);

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/debug_options_flags.h
LOG(ERROR) << "Out of fuel for "" << pass << "": " << ran_out_of_fuel_msg();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/python/tpu_driver/tpu_driver.h
LOG(FATAL) << "Unimplemented.";
LOG(FATAL) << "Unimplemented.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/collective_ops_utils.h
VLOG(3) << "Begin: " << desc_fn();
VLOG(3) << "Finished: " << desc_fn();
LOG(ERROR) << "This thread has been waiting for " << timeout.count() << "ms for and may be stuck: " << desc_fn();
LOG(ERROR) << "Thread is unstuck! Warning above was a false-positive. " "Perhaps the timeout is too short: " << desc_fn();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/dynamic_dimension_inference.h
VLOG(1) << "Set dimension inst " << inst->ToString() << " index " << index.ToString() << "@" << dim << " to " << size->ToShortString() << " constraint: " << constraint.multiple_of;

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/tuple_points_to_analysis.h
LOG(FATAL) << "Expected per-instruction information to already exist";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/hlo_pass_fix.h
VLOG(3) << "Running HloPassFix on " << Pass::name();
VLOG(3) << "changed_this_iteration: " << changed_this_iteration;
LOG(WARNING) << "Unexpectedly high number of iterations in HLO passes, " "exiting fixed point loop.";
VLOG(3) << "Running HloPassFix.";
VLOG(3) << "changed_this_iteration: " << changed_this_iteration;
LOG(WARNING) << "Unexpectedly high number of iterations in HLO passes, " "exiting fixed point loop.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/dfs_hlo_visitor_with_default.h
VLOG(3) << "Replacing instruction:";
VLOG(3) << " old: " << old_instruction->ToString();
VLOG(3) << " new: " << new_instruction->ToString();
VLOG(3) << "Replacing instruction:";
VLOG(3) << " old: " << old_instruction->ToString();
VLOG(3) << " new: " << new_instruction->ToString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/hlo_computation.h
VLOG(3) << "Traversing unreachable root: " << root->ToString();
VLOG(3) << "Accepting visitor with order.";
VLOG(3) << "Visiting ordered: " << instruction->ToString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/hlo_instruction.h
LOG(FATAL) << "Unimplemented method."; }
LOG(FATAL) << "Unimplemented method."; }
LOG(FATAL) << "Unimplemented method."; }
LOG(FATAL) << "Unimplemented method."; }
LOG(FATAL) << "Unimplemented method."; }
LOG(FATAL) << "Unimplemented method."; }

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/hlo_evaluator_typed_visitor.h
LOG(FATAL) << "Trying to get complex literal as double: " << literal.ToString(); }
LOG(FATAL) << "HandleDynamicSlice: unhandled primitive type for " "start_indices: " << PrimitiveType_Name(start_indices->shape().element_type());
LOG(FATAL) << "HandleDynamicUpdateSlice: unhandled primitive type for " "start_indices: " << PrimitiveType_Name(start_indices->shape().element_type());
LOG(FATAL) << "HandleMap: unhandled primitive type for " "input operand: " << PrimitiveType_Name( map->operand(0)->shape().element_type());
VLOG(3) << "HandleReduceWindow arg_literal: " << operand_literal.ToString();
VLOG(3) << "HandleReduceWindow init_literal: " << init_literal.ToString();

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/hlo_lexer.h
LOG(FATAL) << "This token does not have string value";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/gpu/cusolver_context.h
LOG(FATAL) << "Unimplemented";
LOG(FATAL) << "Unimplemented";
LOG(FATAL) << "Unimplemented";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/gpu/kernel_mapping_scheme.h
VLOG(10) << "dims_in_elems_ = " << absl::StrJoin(dims_in_elems_, ",");

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xla/service/cpu/target_machine_features_fake.h
LOG(FATAL) << "Unexpected call to " << __func__; }
LOG(FATAL) << "Unexpected call to " << __func__; }
LOG(FATAL) << "Unexpected call to " << __func__; }

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/tf2tensorrt/utils/trt_shape_optimization_profiles.h
VLOG(1) << "Collected shape(s) " << DebugString(shapes) << " for profiles.";

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/tf2tensorrt/utils/trt_allocator.h
VLOG(1) << "Destroying allocator attached to " << allocator_->Name(); }

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/tf2tensorrt/convert/trt_optimization_pass.h
VLOG(1) << "Constructing " << name_; }

/Users/holen/DegreeProject/VCS/log4mlf/tensorflow/tensorflow/compiler/xrt/kernels/xrt_state_ops.h
VLOG(1) << "XRTAllocateOp::Compute";
VLOG(1) << "XRTAllocateUninitializedOp::Compute";
VLOG(1) << "XRTAllocateFromTensorOp::Compute";
VLOG(1) << "XRTSubTupleOp::Compute";
VLOG(2) << "Releasing handle " << allocation_handle;
VLOG(1) << "XRTMakeTupleOp::Compute";
VLOG(1) << "XRTReadLiteralOp::Compute";
VLOG(2) << "Releasing handle " << allocation_handle;
VLOG(1) << "XRTReadToTensorOp::Compute";
VLOG(2) << "Releasing handle " << allocation_handle;
VLOG(1) << "XRTWriteLiteralOp::Compute";
VLOG(1) << "XRTReleaseAllocationOp::Compute";
VLOG(2) << "Released allocation handle " << key;
VLOG(1) << "XRTReleaseAllAllocationsOp::Compute";
VLOG(1) << "XRTCompactAllocationsOp::Compute";
